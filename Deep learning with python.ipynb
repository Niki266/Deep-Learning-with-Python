{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd28163",
   "metadata": {},
   "source": [
    "<center><h1>The Universal workflow of machine learning</h1></center>\n",
    "<center><h4>with IMDb Tensorflow dataset</h4></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44655381",
   "metadata": {},
   "source": [
    "## Table of contents -\n",
    "* [1. Introduction](#Introduction)\n",
    "* [2. Defining the problem and assembling a dataset](#Problem_definition)\n",
    "   * [2.1 What will our input data be?](#input_data)\n",
    "   * [2.2 What are we trying to predict?](#predict)\n",
    "   * [2.3 What type of problem are you facing?](#problem)\n",
    "   * [2.4 Hypothesis about Predictability](#predictability)\n",
    "   * [2.5 Hypothesis about Data Informativeness](#informativeness)\n",
    "* [3. Preparing your data](#preparing) \n",
    "   * [3.1 Feature Engineering - Vertorization, One-hot encoding,label encoding, Imputation](#Feature_Engineering)\n",
    "* [4. Choosing a measure of success](#measure_of_success)\n",
    "* [5. Baseline Model building](#Model_building)\n",
    "   * [5.1. Model Architecture](#Architecture)\n",
    "   * [5.2. Model Compilation](#Compilation)\n",
    "   * [5.3. Validating your approach](#approach)\n",
    "   * [5.4. Model Training](#Training)\n",
    "   * [5.5. Data Visualization](#Visualization)\n",
    "        * [5.5.1. Plotting the training and validation loss](#validation_loss)\n",
    "        * [5.5.2. Plotting the training and validation accuracy](#validation_accuracy)\n",
    "   * [5.6. Retraining a model from scratch](#Retraining)\n",
    "   * [5.7. Model Predictions](#Predictions)\n",
    "* [6. Deciding on an evaluation protocol](#evaluation_protocol) \n",
    "   * [6.1. Hold-out validation](#Hold-out)\n",
    "   * [6.2. K-fold validation](#K-fold)\n",
    "* [7. Fighting Overfitting ](#Fighting_Overfitting)\n",
    "   * [7.1. Version of the model with lower capacity](#lower_capacity)\n",
    "* [8. Scaling up: developing a model that overfits for baseline model](#Scaling_up_base)\n",
    "* [9. Regularizing your model and tuning your hyperparameters](#tuning)\n",
    "   * [9.1. Adding L2 weight regularization to the model](#L2) \n",
    "   * [9.2. Adding dropout](#dropout)\n",
    "* [10. Extensive Experimentation](#Experimentation) \n",
    "   * [10.1. Experiment with one and three hidden layers](#1_3)\n",
    "   * [10.2. Experiment with more or fewer hidden units - 32 units, 64 units etc](#32_64)\n",
    "   * [10.3. Investigate replacing the binary_crossentropy loss function with mse](#mse)\n",
    "   * [10.4. Experiment with replacing relu with tanh activations](#tanh)\n",
    "   * [10.5. Investigate the effect of different learning rates. Optimal configuration](#lr)\n",
    "   * [10.6. Take your best network and train on all the training data for the optimal epochs. Evaluate on the test set](#best)\n",
    "* [11. Developing a model that does better than a baseline](#Developing_model)\n",
    "* [12. Scaling up: developing a model that overfits for the new model that does better than baseline](#Scaling_Developing)\n",
    "* [13. Interpretation of results](#Results)\n",
    "* [14. Wrapping up and Conclusion](#Conclusion)\n",
    "* [15. References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5161e562",
   "metadata": {},
   "source": [
    "### <U>1. Introduction</U> <a class=\"anchor\" id=\"Introduction\"></a>\n",
    "\n",
    "For this project I will be following the the universal workflow of machine learning as demonstrated in Francois Chollet DLWP 4.5. Binary classification, which involves categorizing data into two classes, is a fundamental task in machine learning. In this project, we aim to classify movie reviews as either positive or negative based on their textual content. We will work with the IMDB dataset, which consists of 50,000 highly polarized reviews from the Internet Movie Database. The dataset is divided into 25,000 reviews for training and 25,000 for testing, with equal distribution between positive and negative reviews.\n",
    "\n",
    "The Universal workflow of machine learning involves-\n",
    "- Defining the problem and assembling a dataset\n",
    "- Preparing your data\n",
    "- Choosing a measure of success\n",
    "- Building a baseline model\n",
    "- Deciding on an evaluation protocol\n",
    "- Fighting Overfitting\n",
    "- Scaling up: developing the baseline model that overfits (not necessary only for more investigation)\n",
    "- Regularizing your model and tuning your hyperparameters\n",
    "- Developing a model that does better than a baseline\n",
    "- Scaling up: developing a model that overfits for the new model that does better than baseline\n",
    "- Interpretation of results\n",
    "- Wrapping up and Conclusion\n",
    "\n",
    "Additionally I will do -\n",
    "- Extensive Experimentation before developing a model that does better than a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fcd59c",
   "metadata": {},
   "source": [
    "### <U>2. Defining the problem and assembling a dataset</U> <a class=\"anchor\" id=\"Problem_definition\"></a>\n",
    "\n",
    "We are conducting sentiment analysis on the IMDb dataset, aiming to predict the sentiment associated with movie reviews. The input data for our analysis consists of movie reviews obtained from the IMDb dataset, while the prediction target is the sentiment label assigned to each review.\n",
    "\n",
    "#### 2.1 What will our input data be?<a class=\"anchor\" id=\"input_data\"></a>\n",
    "\n",
    "The input data for the IMDb dataset consists of sequences of words representing movie reviews. These reviews have been preprocessed and converted into sequences of integers, where each integer corresponds to a specific word in a predefined dictionary.In other words, each movie review is represented as a sequence of integers, with each integer mapping to a particular word in the dictionary. This preprocessing step allows the machine learning model to work with numerical data, which is essential for training and making predictions.\n",
    "\n",
    "When using the IMDb dataset in Keras, the input data will be in the form of these preprocessed sequences of integers. This format enables the model to learn patterns and relationships between words in the reviews and their associated sentiments (positive or negative). It's important to note that the IMDb dataset is split into separate training and test sets, each containing 25,000 reviews. This split ensures that the model is trained on one set of data and evaluated on a completely separate set, preventing overfitting and providing a reliable measure of its generalization performance.\n",
    "\n",
    "#### 2.2 What are we trying to predict?<a class=\"anchor\" id=\"predict\"></a>\n",
    "\n",
    "The prediction target is the sentiment associated with each review, which is binary: positive or negative. Each review is labeled as either 1 (positive sentiment) or 0 (negative sentiment).We utilize separate training and test sets, with 25,000 reviews each, to train and evaluate our machine learning model. Each set contains an equal distribution of positive and negative reviews, ensuring a balanced dataset for training and evaluation. This separation ensures that the model's performance is assessed on unseen data, preventing overfitting and providing a reliable measure of its generalization capabilities.\n",
    "\n",
    "The availability of both movie reviews and sentiment annotations (positive or negative labels) within the IMDb dataset enables the training of machine learning models for sentiment analysis tasks. Researchers and practitioners can utilize this dataset to develop and evaluate sentiment analysis algorithms effectively.Therefore, within the IMDb dataset, there is indeed data availability, which is crucial for training, predicting and evaluating sentiment analysis models accurately.\n",
    "\n",
    "The objective of our sentiment analysis task is to develop a machine learning model capable of accurately predicting the sentiment of movie reviews. By leveraging the available data, which includes both movie reviews and sentiment annotations, our goal is to train a model that can generalize well to unseen data and effectively classify the sentiment of new movie reviews.\n",
    "\n",
    "#### 2.3 What type of problem are you facing?<a class=\"anchor\" id=\"problem\"></a>\n",
    "\n",
    "The problem we are facing with the IMDb dataset is binary classification. In binary classification, the task involves categorizing input data into one of two classes or categories.\n",
    "\n",
    "In the context of IMDb dataset for sentiment analysis, the input data consists of movie reviews and the prediction target is the sentiment associated with each review, which is binary: either positive or negative.\n",
    "Therefore, the problem of classifying movie reviews as positive or negative sentiments falls under the category of binary classification. This understanding guides the choice of appropriate model architecture, loss function (e.g., binary cross-entropy), evaluation metrics, and other aspects of the machine learning pipeline tailored specifically for binary classification tasks.\n",
    "\n",
    "#### 2.4 Hypothesis about Predictability -<a class=\"anchor\" id=\"predictability\"></a>\n",
    "I hypothesize that the outputs (sentiment labels) can be predicted given the inputs (movie reviews). The training data, testing data and restriction to the top 10,000 most frequent words should be sufficient enough to get the right predictability. This assumption forms the basis of our task, we believe that there exists a relationship between the textual content of movie reviews and their associated sentiment labels, which our deep learning model can learn.\n",
    "\n",
    "#### 2.5 Hypothesis about Data Informativeness -<a class=\"anchor\" id=\"informativeness\"></a>\n",
    "We hypothesize that the available data (movie reviews with sentiment annotations and restriction to the top 10,000 most frequent words) is sufficiently informative to learn the relationship between inputs (reviews) and outputs (sentiments). This assumption implies that the dataset contains representative examples that capture the variability and complexity of real-world movie reviews, enabling the model to generalize well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810bee5b",
   "metadata": {},
   "source": [
    "<U>Dataset Selection -</U>\n",
    "\n",
    "I chose the IMDb dataset beacuse i was interested in doing a sentiment analysis using binary classification.. it is also preferable due to its relevance, availability, size, simplicity, benchmarking status, and potential real-world applications.The dataset's binary classification task and established benchmarks facilitate model development and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e6c5e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1  2.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "from keras import models, layers\n",
    "from tensorflow import keras\n",
    "print(tf.__version__,'',tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0313c17f",
   "metadata": {},
   "source": [
    "We import TensorFlow and Keras libraries and prints their versions. We also import specific modules and layers necessary to build a neural network model for sentiment analysis on the IMDb dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34a6dc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6fa9a3",
   "metadata": {},
   "source": [
    "The IMDb dataset is divided into four variables: train_data, train_labels, test_data, and test_labels. The dataset is split into training and testing sets. The load_data() function loads the dataset, and the num_words parameter specifies the maximum number of words to include in the vocabulary. In this case, num_words=10000 will only keep the top 10,000 most frequently occurring words in the training data. Rare words will be discarded. This allows us to work with vector data of manageable size. \n",
    "\n",
    "This helps in reducing the dimensionality of the input data and speeds up training. Each review in the dataset is represented as a sequence of integers, where each integer represents a word in the review. The train_data and test_data variables contain lists of reviews, where each review is represented as a list of integers (word indices), and train_labels and test_labels contain the corresponding sentiment labels for the reviews in which: -\n",
    "\n",
    "<b><u> 0 indicates a negative review and 1 indicates a positive review.</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6695855f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 13, 244, 6, 87, 337, 7, 628, 2219, 5, 28, 285, 15, 240, 93, 23, 288, 549, 18, 1455, 673, 4, 241, 534, 3635, 8448, 20, 38, 54, 13, 258, 46, 44, 14, 13, 1241, 7258, 12, 5, 5, 51, 9, 14, 45, 6, 762, 7, 2, 1309, 328, 5, 428, 2473, 15, 26, 1292, 5, 3939, 6728, 5, 1960, 279, 13, 92, 124, 803, 52, 21, 279, 14, 9, 43, 6, 762, 7, 595, 15, 16, 2, 23, 4, 1071, 467, 4, 403, 7, 628, 2219, 8, 97, 6, 171, 3596, 99, 387, 72, 97, 12, 788, 15, 13, 161, 459, 44, 4, 3939, 1101, 173, 21, 69, 8, 401, 2, 4, 481, 88, 61, 4731, 238, 28, 32, 11, 32, 14, 9, 6, 545, 1332, 766, 5, 203, 73, 28, 43, 77, 317, 11, 4, 2, 953, 270, 17, 6, 3616, 13, 545, 386, 25, 92, 1142, 129, 278, 23, 14, 241, 46, 7, 158]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e381a",
   "metadata": {},
   "source": [
    "The output is a sequence of word indices representing 101st review from the IMDb dataset.\n",
    "\n",
    "Each number in the sequence corresponds to an index of a specific word in the IMDb dataset's vocabulary. For instance, the first number 1 might correspond to the word \"the\", 13 to \"of\", 244 to \"and\", and so forth. These indices are essentially pointers to words in the IMDb dataset's vocabulary, with each index uniquely mapping to a specific word.\n",
    "\n",
    "This sequence of word indices serves as the input data for the neural network model. Each review is transformed into a sequence of these indices, where each index points to a particular word in the vocabulary. This preprocessing step enables the representation of textual data in a numerical format suitable for neural network processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3758b4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da419a8b",
   "metadata": {},
   "source": [
    "Restriction to the top 10,000 most frequent words, no word index will exceed 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec5d822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict(\n",
    "    [(value, key) for (key, value) in word_index.items()])\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b2a1e4",
   "metadata": {},
   "source": [
    "- It retrieves a dictionary (word_index) containing mappings of words to their corresponding indices in the IMDb dataset.\n",
    "\n",
    "- It creates a reverse mapping (reverse_word_index) where indices are mapped back to words.\n",
    "\n",
    "- It decodes the first review in the training data by iterating through each index in the review, looking up the corresponding word in the reverse_word_index dictionary, and joining them together to form the decoded review.\n",
    "\n",
    "- Finally, it prints out the decoded review.\n",
    "\n",
    "Overall, this code snippet allows us to understand the textual content of a review from the IMDb dataset by converting it from a sequence of word indices back into its original text form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b5b3a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea36273f",
   "metadata": {},
   "source": [
    "<b>The output 0 tells us that the sentiment of the 101st review is negative.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4b68fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? corbin bernsen gives a ? intense and riveting performance as dr alan feinstone a wealthy and successful beverly hills dentist who's obsessed with perfection when he discovers that his lovely blonde babe ? wife has been cheating on him and the irs start ? him about tax problems feinstone cracks under the pressure and goes violently around the bend director brian ? working from a suitably dark witty and demented script by stuart gordon dennis ? and charles ? exposes the ? ? and ? ? underneath the ? clean well ? surface of respectable ? rich america with deliciously ? glee moreover ? further ? up the grisly goings on with a ? twisted sense of pitch black ? humor bernsen positively shines as dr feinstone he expertly projects a truly unnerving underlying creepiness that's right beneath ? ? calm and assured ? the supporting cast are likewise excellent linda hoffman as ? bitchy ? wife brooke earl ? as smarmy ? irs agent marvin goldblum molly ? as ? assistant jessica patty toy as ? assistant karen jan ? as jolly office manager candy ? ? as sweet ? teenager sarah ken ? as ? no nonsense detective ? tony ? as ? equally ? partner detective sunshine michael ? as ? stud ? pool ? matt and mark ? as on the make ? steve ? the first rate make up f x are every bit as gory gross and ? as they ought to be the polished cinematography by ? ? boasts lots of great crazy ? camera angles and a few tasty zoom in close ups alan ? spirited ? score also hits the flesh ? spot an ? ? treat \n",
      "\n",
      " label:  1\n"
     ]
    }
   ],
   "source": [
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[104]])\n",
    "print(decoded_review,'\\n\\n', 'label: ',train_labels[104])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40bb11d",
   "metadata": {},
   "source": [
    "This code decodes the 105th review from the IMDb dataset and prints it alongside its corresponding label. It first decodes the review by mapping the sequence of word indices back to their respective words using a reverse dictionary. Then, it prints the decoded review followed by its label, which indicates whether the sentiment of the review is positive (1) or negative (0). \n",
    "\n",
    "<b>In this case the label is 1 which is a positive review.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5132a8",
   "metadata": {},
   "source": [
    "### <u>3. Preparing your data</u><a class=\"anchor\" id=\"preparing\"></a>\n",
    "\n",
    "The network requires tensor inputs, not lists for that we can implement. This ensures compatibility with the neural network model.\n",
    "\n",
    "There are two common approaches for this task:\n",
    "\n",
    "Padding and Embedding Layer: One approach is to pad the sequences to ensure they all have the same length and then convert them into an integer tensor of shape (samples, word_indices). This can be done using an Embedding layer, which is capable of handling such integer tensors.\n",
    "\n",
    "One-Hot Encoding: Another approach is to one-hot encode the sequences, which involves turning each sequence into a binary matrix where each word is represented by a vector of 0s and 1s. In this encoding scheme, each word corresponds to a specific index in the vector, and the vector is all 0s except for indices corresponding to the words in the sequence, which are set to 1.\n",
    "\n",
    "I will be using the latter.\n",
    "\n",
    "<u>One-hot encode</u> your lists to turn them into vectors of 0s and 1s.This would mean, for instance, turning the sequence [3, 5] into a 10,000-dimensional vector that would be all 0s except for indices 3 and 5, which would be 1s. Then you could use as the first layer in your network a Dense layer, capable of handling floating-point vector data.\n",
    "\n",
    "This process is implemented in the vectorize_sequences function, which takes sequences of integers and converts them into binary matrices. After applying this function to the training and test data, the resulting matrices represent the vectorized training and test data, respectively.\n",
    "\n",
    "Additionally, the labels are also vectorized, where positive and negative sentiments are encoded as 1s and 0s, respectively, and converted into floating-point value tensors.  \n",
    "\n",
    "<u>So if the the output is closer to 1 then it is a good review if the output is closer to 0 it is a bad review.</u>\n",
    "\n",
    "Once the data is vectorized, it is ready to be fed into a neural network for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c50cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54218d52",
   "metadata": {},
   "source": [
    "vectorize_sequences converts sequences of word indices into binary feature vectors using one-hot encoding. The function takes a list of sequences representing reviews in the IMDb dataset and an optional parameter specifying the dimensionality of the feature vectors. It initializes a NumPy array with zeros, where each row corresponds to a sequence, and each column represents a word in the predefined vocabulary. It then iterates over each sequence, setting the elements in the array corresponding to the indices in the sequence to 1. Finally, it returns the array containing the binary feature vectors for all input sequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8e0834",
   "metadata": {},
   "source": [
    "#### 3.1 Feature Engineering - Vertorization, One-hot encoding, label encoding, Imputation/handling missing values <a class=\"anchor\" id=\"Feature_Engineering\"></a>\n",
    "\n",
    "<u>Vectorization helps in-</u>\n",
    "\n",
    "- Numerical Conversion: It converts sequences of word indices from the IMDb dataset into binary feature vectors, facilitating numerical processing by neural network models.\n",
    "\n",
    "- Standardization: Ensures uniformity by converting each review into a fixed-length binary feature vector, which is crucial for consistent model training and evaluation.\n",
    "\n",
    "- Efficient Representation: The resulting sparse binary feature vectors are computationally efficient, reducing memory usage and enabling faster processing of large datasets like IMDb.\n",
    "\n",
    "- Alignment with Deep Learning Workflow: This preprocessing step aligns with the deep learning workflow, providing a standardized and efficient method for preparing textual data for input into neural network models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7ec0b0",
   "metadata": {},
   "source": [
    "<U>Train-test split</U>\n",
    "\n",
    "We split the data into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8a8b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a99080",
   "metadata": {},
   "source": [
    "vectorize_sequences is applied to both the training and testing data of the IMDb dataset. The resulting binary feature vectors are stored in x_train and x_test, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0f57167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416aef35",
   "metadata": {},
   "source": [
    "The output of x_train[0] shows that the first review has been converted into a binary feature vector where the second and third elements are set to 1, indicating the presence of corresponding words from the vocabulary in the review. All other elements are set to 0, indicating the absence of those words. This binary feature vector serves as the input data for the neural network model during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb6a182",
   "metadata": {},
   "source": [
    "<U>Value normalization and label encoding -</U>\n",
    "\n",
    "Value normalization and label encoding are essential preprocessing steps in machine learning. Value normalization ensures that features are on a consistent scale, preventing biases and promoting model stability and convergence during training. Label encoding converts categorical data into a numerical format, facilitating model understanding and compatibility with machine learning algorithms. These preprocessing techniques are crucial for optimizing model performance and ensuring effective training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da9fa79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d69f67",
   "metadata": {},
   "source": [
    "Now we convert the sentiment labels from the IMDb dataset into a numerical format suitable for training a neural network model. By utilizing NumPy arrays and setting the data type to 'float32', both the training and testing labels are transformed into a standardized numerical representation. This conversion ensures compatibility with deep learning frameworks, facilitating the subsequent training and evaluation processes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c04d66c",
   "metadata": {},
   "source": [
    "<u>Sorted set is the elements of training data 104 in ascending order with duplicates removed </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2e9ba89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 4, 5, 6, 7, 8]\n",
      "\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[0. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "sorted_set = sorted(set(train_data[104]))\n",
    "print([x for x in sorted_set if x < 11])\n",
    "numbs = [i for i in range(11)]\n",
    "print()\n",
    "print(numbs)\n",
    "print(x_train[104][:11])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4790754",
   "metadata": {},
   "source": [
    "We analyze the content of the 105th review from the IMDb training dataset. We first extract the unique word indices from the review, sort them, and then print those indices that are less than 11. This shows a subset of words from the review with indices up to 10. We then generate a list of numbers from 0 to 10, representing the indices of words in the vocabulary. Finally, we print the first 11 elements of the binary feature vector corresponding to the review, indicating the presence or absence of words from the vocabulary up to index 10 in the review. \n",
    "\n",
    "Handling missing values -\n",
    "\n",
    "we see that 3 and 9 are missing. The one-hot encoding has zeros in positions 3 and 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f7d902",
   "metadata": {},
   "source": [
    "### <u>4. Choosing a measure of success</u><a class=\"anchor\" id=\"measure_of_success\"></a>\n",
    "\n",
    "Before moving on to the model building lets first choose a measure of success for our model.\n",
    "\n",
    "For this dataset i chose accuracy.\n",
    "\n",
    "Accuracy is a suitable metric for evaluating sentiment analysis models on the IMDb dataset due to several reasons. Firstly, the dataset contains an equal distribution of positive and negative sentiment reviews, making it balanced. Secondly, as a binary classification task, accuracy provides a clear measure of the proportion of correctly classified instances out of the total instances. Thirdly, accuracy is easy to interpret and communicate, allowing stakeholders to understand model performance intuitively. Additionally, accuracy offers a comprehensive view of model effectiveness across all classes and serves as a baseline for comparing different models. Therefore, accuracy is a logical choice for assessing sentiment analysis models on the IMDb dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1b3ded",
   "metadata": {},
   "source": [
    "### <u>5. Baseline Model building</u><a class=\"anchor\" id=\"Model_building\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a406a5aa",
   "metadata": {},
   "source": [
    "#### 5.1. Model Architecture<a class=\"anchor\" id=\"Architecture\"></a>\n",
    "\n",
    "The architecture, featuring a simple stack of dense layers with ReLU activation and a sigmoid output, is suitable for sentiment analysis on the IMDb dataset due to its simplicity, efficiency in representation learning, scalability, interpretability, and effectiveness in capturing non-linear relationships in text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9da7e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 10:29:53.479315: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8df872",
   "metadata": {},
   "source": [
    "<u>Understanding the baseline model -</u>\n",
    "\n",
    "The baseline model is built as a sequential stack of layers. It begins with an input layer consisting of 10,000 units, corresponding to the number of features in the input data. Two hidden layers follow, each containing 16 neurons with Rectified Linear Unit (ReLU) activation functions, enabling the model to learn complex patterns in the data. \n",
    "\n",
    "The final output layer comprises a single neuron with a sigmoid activation function, producing a probability score between 0 and 1 indicating the likelihood of a review being positive. This architecture facilitates binary classification, distinguishing between positive and negative sentiment in movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8074282",
   "metadata": {},
   "source": [
    "#### 5.2. Model Compilation<a class=\"anchor\" id=\"Compilation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13e5c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8dd9ec",
   "metadata": {},
   "source": [
    "The compiled neural network model is configured for training with specific optimization, loss calculation, and evaluation settings. The optimizer parameter is set to 'rmsprop', indicating the use of the RMSprop optimization algorithm during training, which dynamically adjusts learning rates for different model parameters. The loss parameter is assigned 'binary_crossentropy', a standard choice for binary classification tasks like sentiment analysis, to measure the discrepancy between predicted probabilities and true binary labels. Additionally, the accuracy metric is specified to assess the model's performance, measuring the proportion of correctly classified instances during training and validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0b50a9",
   "metadata": {},
   "source": [
    "<u>Configuring the optimizer</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3678bb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikhitha/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf4232b",
   "metadata": {},
   "source": [
    "The model compilation step is refined by creating the RMSprop optimizer with a specified learning rate of 0.001 using optimizers.RMSprop(lr=0.001). By importing the optimizers module from Keras, a precise adjustment over optimizer parameters is achieved. The compiled neural network model retains its configuration for binary classification tasks with binary crossentropy as the loss function and accuracy as the evaluation metric. This modification allows for more precise adjustment of the optimizer's learning rate, enhancing the model's training dynamics and potential performance on sentiment analysis tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a68831",
   "metadata": {},
   "source": [
    "<u>Using custom losses and metrics</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c13ee7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import losses\n",
    "from keras import metrics\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "                      loss=losses.binary_crossentropy,\n",
    "                      metrics=[metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55bfaca",
   "metadata": {},
   "source": [
    "The model compilation process remains consistent with the previous iterations, maintaining the use of the RMSprop optimizer with a specified learning rate. However, there is a shift in how the loss function and evaluation metrics are specified. Instead of using strings to denote the loss function and metrics, the code directly assigns objects from the losses and metrics modules imported from Keras. This approach provides greater clarity, precision, and flexibility in defining these components, enhancing code readability and allowing for more fine-grained customization if necessary.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0d5666",
   "metadata": {},
   "source": [
    "#### 5.3. Validating your approach<a class=\"anchor\" id=\"approach\"></a>\n",
    "\n",
    "Validating your approach in machine learning is important for ensuring the reliability and effectiveness of your models. It serves multiple important purposes, including assessing how well your model generalizes to unseen data, detecting and addressing overfitting issues to enhance model robustness, optimizing hyperparameters for improved performance, comparing different model architectures or techniques to identify the most suitable approach, and building confidence in the model's capabilities for real-world deployment. By validating your approach, you can make informed decisions, improve model quality, and achieve impactful results in various machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ffba713",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a17363c",
   "metadata": {},
   "source": [
    "Two subsets of the training data are created: x_val, containing the first 10,000 samples from x_train, and partial_x_train, containing the remaining samples. x_val serves as a validation set for evaluating the model's performance during training, while partial_x_train is used as the training set for the model. This separation allows for effective model evaluation and parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f4dec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f382f0",
   "metadata": {},
   "source": [
    "two subsets of the training labels are created: y_val, comprising the first 10,000 labels from y_train, and partial_y_train, consisting of the remaining labels. y_val is utilized as a validation set during training to assess the model's performance, while partial_y_train serves as the training label set. This division facilitates model evaluation and parameter adjustment for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9860e",
   "metadata": {},
   "source": [
    "#### 5.4. Model Training<a class=\"anchor\" id=\"Training\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7119c51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 3s 72ms/step - loss: 0.5245 - acc: 0.7817 - val_loss: 0.3964 - val_acc: 0.8529\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 0.3060 - acc: 0.9001 - val_loss: 0.3055 - val_acc: 0.8877\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.2242 - acc: 0.9272 - val_loss: 0.2827 - val_acc: 0.8895\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 1s 22ms/step - loss: 0.1786 - acc: 0.9422 - val_loss: 0.2786 - val_acc: 0.8894\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 1s 22ms/step - loss: 0.1452 - acc: 0.9543 - val_loss: 0.2803 - val_acc: 0.8883\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.1207 - acc: 0.9620 - val_loss: 0.3022 - val_acc: 0.8841\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 1s 19ms/step - loss: 0.0992 - acc: 0.9711 - val_loss: 0.3103 - val_acc: 0.8836\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.0799 - acc: 0.9780 - val_loss: 0.3296 - val_acc: 0.8810\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 1s 23ms/step - loss: 0.0667 - acc: 0.9824 - val_loss: 0.3840 - val_acc: 0.8765\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 0.0556 - acc: 0.9864 - val_loss: 0.3839 - val_acc: 0.8778\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.0440 - acc: 0.9899 - val_loss: 0.4052 - val_acc: 0.8767\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 1s 23ms/step - loss: 0.0348 - acc: 0.9928 - val_loss: 0.4344 - val_acc: 0.8734\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 0.0275 - acc: 0.9951 - val_loss: 0.4708 - val_acc: 0.8746\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 0.0225 - acc: 0.9957 - val_loss: 0.5016 - val_acc: 0.8695\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 1s 22ms/step - loss: 0.0194 - acc: 0.9969 - val_loss: 0.5312 - val_acc: 0.8689\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.0151 - acc: 0.9976 - val_loss: 0.5611 - val_acc: 0.8705\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 0.0086 - acc: 0.9993 - val_loss: 0.5944 - val_acc: 0.8699\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 1s 25ms/step - loss: 0.0098 - acc: 0.9983 - val_loss: 0.6271 - val_acc: 0.8666\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 1s 21ms/step - loss: 0.0095 - acc: 0.9982 - val_loss: 0.6590 - val_acc: 0.8657\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 1s 23ms/step - loss: 0.0037 - acc: 0.9998 - val_loss: 0.6846 - val_acc: 0.8661\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e7ed6e",
   "metadata": {},
   "source": [
    "The compiled neural network model is trained using the fit method. The optimizer is set to 'rmsprop', the loss function to 'binary_crossentropy', and the evaluation metric to 'acc', indicating accuracy. The model is trained for 20 epochs with a batch size of 512 samples. The training data (partial_x_train and partial_y_train) are used for model training, while the validation data (x_val and y_val) are used to evaluate the model's performance after each epoch. The training progress and performance metrics are recorded in the history object for further analysis and visualization. This process allows for iterative model training and evaluation, aiming to optimize the model's performance on the task of sentiment analysis on the IMDb reviews.\n",
    "\n",
    "Model training output results -\n",
    "\n",
    "- The training accuracy (acc) steadily increases over epochs and reaches a very high value close to 1.0 (100%). This indicates that the model is able to learn from the training data very well and can classify the training samples with high accuracy.\n",
    "\n",
    "- The training loss (loss) steadily decreases over epochs, which indicates that the model is minimizing its error on the training data as training progresses.\n",
    "\n",
    "- The validation accuracy (val_acc) also increases initially but starts to slightly decrease towards the later epochs. This suggests that the model may be overfitting the training data, as it performs well on the training data but does not generalize as well to unseen validation data.\n",
    "\n",
    "- The validation loss (val_loss) follows a similar trend as the validation accuracy, initially decreasing but then stabilizing or slightly increasing towards the end of training. This further supports the observation of potential overfitting.\n",
    "\n",
    "- Overfitting occurs when a model learns to memorize the training data instead of generalizing from it. This often leads to high training accuracy but poor performance on unseen data. The discrepancy between training and validation performance is a common indicator of overfitting. The model's training accuracy reaches very high levels (close to 1.0), while the validation accuracy  slightly decreases. This suggests that the model is likely overfitting the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a42e7475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f62f2ad",
   "metadata": {},
   "source": [
    "The history_dict variable stores the training history of the neural network model, which includes metrics such as loss and accuracy recorded during the training process. By calling the .keys() method on the history_dict, we obtain a list of keys representing the available metrics tracked during training. These keys typically include 'loss', 'accuracy', 'val_loss', and 'val_accuracy', corresponding to the training and validation metrics recorded during each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d0b8a710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5224974155426025,\n",
       " 0.3121812641620636,\n",
       " 0.22672544419765472,\n",
       " 0.17763064801692963,\n",
       " 0.14408229291439056,\n",
       " 0.11608482897281647,\n",
       " 0.09605712443590164,\n",
       " 0.07958149909973145,\n",
       " 0.06418953090906143,\n",
       " 0.05583503469824791,\n",
       " 0.04098912701010704,\n",
       " 0.03526778146624565,\n",
       " 0.02817384898662567,\n",
       " 0.022429652512073517,\n",
       " 0.017448944970965385,\n",
       " 0.014543234370648861,\n",
       " 0.009498635306954384,\n",
       " 0.007188604678958654,\n",
       " 0.008315914310514927,\n",
       " 0.003287909086793661]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a6ceb",
   "metadata": {},
   "source": [
    " Retrieves the recorded values of the training loss metric from the training history of the neural network model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab885c9",
   "metadata": {},
   "source": [
    "#### 5.5. Data Visualization <a class=\"anchor\" id=\"Visualization\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8cdcd0",
   "metadata": {},
   "source": [
    "#### 5.5.1. Plotting the training and validation loss<a class=\"anchor\" id=\"validation_loss\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "838a4160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "acc = history.history['acc']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6165fa9b",
   "metadata": {},
   "source": [
    "We now effectively train the neural network model to classify movie reviews as positive or negative based on their text content. The systematic investigation begins with the compilation of the model, where the optimizer, loss function, and evaluation metrics are carefully selected. Subsequently, the model is trained over 20 epochs, with training and validation loss values recorded for analysis. Utilizing Matplotlib, the code generates a plot showcasing the training and validation loss trends over epochs. This visualization allows for the assessment of the model's learning dynamics, revealing insights into potential overfitting or underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a7ea88",
   "metadata": {},
   "source": [
    "#### 5.5.2. Plotting the training and validation accuracy<a class=\"anchor\" id=\"validation_accuracy\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e1feb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf() \n",
    "acc_values = history_dict['acc'] \n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a873344",
   "metadata": {},
   "source": [
    "The second plot starts by clearing the current figure (plt.clf()) to ensure a fresh plot. The accuracy values for both training and validation sets are extracted from the training history and stored in acc_values and val_acc_values variables, respectively. These values are then plotted against the number of epochs using plt.plot(), with blue dots representing training accuracy and a solid blue line representing validation accuracy.\n",
    "\n",
    "<u>Observations of plots -</u>\n",
    "\n",
    "The training loss decreases with every epoch, and the training accuracy increases with every epoch. It is what we would expect when running gradient descent optimization, the quantity we are trying to minimize should be less with every iteration. But that is not the case for the validation loss and accuracy, they seem to peak at the fourth epoch. A model that performs better on the training data is not necessarily a model that will do better on data it has never seen before. This means the model is <b><U>overfitting</U></b>, after the second epoch, we are overoptimizing on the training data, and you end up learning representations that are specific to the training data and dont generalize to data outside of the training set.<b><U>We will fight this overfitting later in the code using some techniques.</U></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d22370",
   "metadata": {},
   "source": [
    "#### 5.6. Retraining a model from scratch<a class=\"anchor\" id=\"Retraining\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b8c4074d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "49/49 [==============================] - 1s 8ms/step - loss: 0.4773 - accuracy: 0.8172\n",
      "Epoch 2/4\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.2740 - accuracy: 0.9064\n",
      "Epoch 3/4\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2080 - accuracy: 0.9267\n",
      "Epoch 4/4\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1722 - accuracy: 0.9394\n",
      "782/782 [==============================] - 1s 914us/step - loss: 0.2942 - accuracy: 0.8837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.29419681429862976, 0.883679986000061]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "\n",
    "results = model.evaluate(x_test, y_test)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eae420",
   "metadata": {},
   "source": [
    "A new neural network model is constructed and trained from scratch. The model architecture consists of a sequential stack of layers, starting with an input layer of 10,000 units corresponding to the feature dimensions of the IMDb dataset. Two hidden layers with 16 units each and ReLU activation functions are added, followed by a single output neuron with a sigmoid activation function for binary classification. The model is compiled with the RMSprop optimizer, binary crossentropy loss function, and accuracy metric.\n",
    "\n",
    "Subsequently, the model is trained using the fit() method on the training data (x_train and y_train) for 4 epochs with a batch size of 512. This process involves iteratively updating the model parameters to minimize the loss and improve accuracy.\n",
    "\n",
    "After training, the model's performance is evaluated on the test data (x_test and y_test) using the evaluate() method. The evaluation results, including the loss value and accuracy metric, are stored in the results variable for further analysis or reporting. This comprehensive workflow demonstrates the process of training and evaluating a neural network model for sentiment analysis on the IMDb dataset, adhering to best practices in deep learning.\n",
    "\n",
    "Retraining model results -\n",
    "\n",
    "- The training accuracy steadily increases over epochs, indicating that the model is learning to classify the training data with increasing accuracy.\n",
    "\n",
    "- The training loss steadily decreases over epochs, suggesting that the model is effectively minimizing its error on the training data.\n",
    "\n",
    "- The test accuracy is approximately 88.37%, indicating that the model achieves an accuracy of around 88.37% on unseen test data.\n",
    "\n",
    "- The test loss is approximately 0.2942, which represents the average loss over all test samples. Lower values of loss indicate better performance.\n",
    "\n",
    "- The fact that the test accuracy is close to the training accuracy suggests that the model generalizes well to unseen data. This indicates that the model has not significantly overfit the training data.\n",
    "\n",
    "- The test loss is also reasonably low, indicating that the model's predictions are relatively confident and accurate on the test set.\n",
    "\n",
    "<U>Understanding why retraining a model from scratch is needed -</U>\n",
    "\n",
    "- Retraining a model from scratch may be necessary for several reasons. Over time, the distribution of data may change and the original model may become outdated or less effective at capturing patterns in the new data. Retraining allows the model to adapt to these changes and maintain its performance.\n",
    "\n",
    "- It provides an opportunity to incorporate any new labeled data that may have become available since the initial training. By leveraging this additional data, the model can potentially improve its predictive power and generalization capabilities. \n",
    "\n",
    "- It allows for the exploration of different model architectures, hyperparameters, and optimization techniques. This iterative process enables the discovery of more effective models that better suit the problem at hand. \n",
    "\n",
    "- It can help address issues such as model degradation or overfitting that may have arisen during previous training iterations. By starting fresh, it's possible to implement corrective measures and optimize the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "361f170f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 16)                160016    \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,305\n",
      "Trainable params: 160,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80086595",
   "metadata": {},
   "source": [
    "\n",
    "The model.summary() method displays a succinct overview of the neural network model's architecture and parameters. The model, designated as \"sequential_3,\" comprises three dense layers: two hidden layers with 16 units each and a single output layer with one neuron for binary classification. The output shapes of the layers are presented along with the number of trainable parameters. In total, the model contains 160,305 trainable parameters, encompassing weights and biases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3168a348",
   "metadata": {},
   "source": [
    "#### 5.7. Model Predictions<a class=\"anchor\" id=\"Predictions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad26986",
   "metadata": {},
   "source": [
    "<U>Using a trained network to generate predictions on new data</U>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "26f9332c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 1s 803us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.23205994],\n",
       "       [0.9999499 ],\n",
       "       [0.94278145],\n",
       "       ...,\n",
       "       [0.11667339],\n",
       "       [0.10511211],\n",
       "       [0.71955585]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743fc883",
   "metadata": {},
   "source": [
    "The output of model.predict(x_test) provides an array of predicted probabilities for each sample in the test dataset (x_test). Each row in the array corresponds to a single sample, and the column contains the predicted probability of the sample belonging to the positive class (i.e., having positive sentiment).\n",
    "\n",
    "For example:\n",
    "\n",
    "- The first sample has a predicted probability of approximately 0.23 for positive sentiment.\n",
    "\n",
    "- The second sample has a very high predicted probability close to 1.0, indicating strong confidence in positive sentiment.\n",
    "\n",
    "- The third sample also has a high predicted probability of around 0.94 for positive sentiment.\n",
    "\n",
    "- The probabilities for the remaining samples follow a similar pattern, providing confidence scores for each sentiment prediction.\n",
    "\n",
    "These predicted probabilities can be further processed or compared with the ground truth labels to assess the model's performance and make predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0f2d255e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 1 1 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b230356",
   "metadata": {},
   "source": [
    "This provides insight into the sentiment distribution within the IMDb dataset by displaying the sentiment labels of the first ten reviews. These labels are indicative of whether each review is perceived positively or negatively. The array [0 1 1 0 1 1 1 0 0 1] reveals that the first review is negative (0), followed by three positive reviews (1). Subsequently, a negative review (0) appears, succeeded by three more positive reviews (1). The last three reviews are negative (0, 0, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c5aa0cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review \t prediction \t \t label\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "0 \t 0.2320599 \t 0 \t 0\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1 \t 0.9999499 \t 1 \t 1\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "2 \t 0.9427814 \t 1 \t 1\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "3 \t 0.88875777 \t 1 \t 0\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "4 \t 0.9746892 \t 1 \t 1\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "5 \t 0.88785976 \t 1 \t 1\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "6 \t 0.9998388 \t 1 \t 1\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "7 \t 0.018441014 \t 0 \t 0\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "8 \t 0.9718882 \t 1 \t 0\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "9 \t 0.9955728 \t 1 \t 1\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "10 \t 0.9535576 \t 1 \t 1\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "11 \t 0.024809223 \t 0 \t 0\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "12 \t 0.00061615097 \t 0 \t 0\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "13 \t 0.037146352 \t 0 \t 0\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "14 \t 0.99837416 \t 1 \t 1\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "15 \t 0.0007898159 \t 0 \t 0\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "16 \t 0.95160556 \t 1 \t 1\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "17 \t 0.75333285 \t 1 \t 0\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "18 \t 0.009799332 \t 0 \t 0\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "19 \t 0.060662128 \t 0 \t 0\n"
     ]
    }
   ],
   "source": [
    "print('review', '\\t', 'prediction','\\t','\\t','label')\n",
    "for i in range(20):\n",
    "    y_pred = model.predict(x_test[i:i+1])[0][0]\n",
    "    \n",
    "    print(i, '\\t', y_pred, '\\t', round(y_pred), '\\t', test_labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde21221",
   "metadata": {},
   "source": [
    "A table is printed representing the prediction results of the model alongside the actual labels for the first 20 reviews in the IMDb dataset. Each row corresponds to a review, displaying its index (review), the model's prediction probability (prediction), the rounded prediction (prediction rounded to the nearest integer), and the actual label (label). The prediction probability indicates the likelihood of the review being positive, while the rounded prediction is a binary classification based on whether the probability is greater than or equal to 0.5. The 1s and 0s indicate positive and negative reviews respectively which we got through binary classification.\n",
    "\n",
    "For example, it is seen in the output that -\n",
    "\n",
    "- For review index 0, the model predicts a probability of approximately 0.23 for a positive sentiment. As this probability is below 0.5, the model predicts a negative sentiment (rounding to 0), which matches the true label (0).\n",
    "\n",
    "- For review index 1, the model predicts a probability of approximately 0.999 for a positive sentiment. As this probability is close to 1, the model predicts a positive sentiment (rounding to 1), which matches the true label (1).\n",
    "\n",
    "- Similarly, the predictions are made for subsequent reviews, and it can be observed that the model's predictions often match the true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c88fb0b",
   "metadata": {},
   "source": [
    "### <u>6. Deciding on an evaluation protocol</u><a class=\"anchor\" id=\"evaluation_protocol\"></a>\n",
    "\n",
    "I have chosen two validation protocols <b>Hold-out validation and K-fold validation.</b> \n",
    "\n",
    "Hold-out validation involves splitting the dataset into training and validation sets, allowing for the assessment of model performance on unseen data. This method is efficient when ample data is available but may lead to variability in performance evaluation due to random partitioning. \n",
    "\n",
    "On the other hand, k-fold validation mitigates this variability by partitioning the dataset into k subsets and iteratively training the model on k-1 subsets while validating on the remaining subset. By averaging the performance across multiple folds, k-fold validation provides a more robust estimation of model performance and ensures better generalization. \n",
    "\n",
    "Utilizing both hold-out validation and k-fold validation offers complementary benefits: hold-out validation provides a quick initial assessment, while k-fold validation offers a more thorough evaluation, enhancing the overall reliability and rigor of the model evaluation process. This dual approach helps mitigate potential biases and uncertainties, leading to more confident model selection and deployment in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd7e1bc",
   "metadata": {},
   "source": [
    "#### 6.1.  Hold-out validation<a class=\"anchor\" id=\"Hold-out\"></a>\n",
    "\n",
    "A hold-out validation technique is used during the model training process. This approach involves splitting the available data into training and validation sets. The model is trained on the training data and its performance is monitored on the validation data. By doing so, it ensures that the model's performance can be evaluated on test data it hasn't seen during training, helping to detect overfitting and assess generalization ability.\n",
    "\n",
    "<b>The portion of the data being tested is the x_test dataset, and its corresponding labels y_test</b>. The x_test dataset typically contains a subset of the original data that was not used during training or validation, ensuring an unbiased evaluation of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "28120949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.5099 - accuracy: 0.7815 - val_loss: 0.4250 - val_accuracy: 0.8237\n",
      "Epoch 2/4\n",
      "30/30 [==============================] - 0s 15ms/step - loss: 0.3073 - accuracy: 0.9001 - val_loss: 0.3137 - val_accuracy: 0.8798\n",
      "Epoch 3/4\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.2240 - accuracy: 0.9269 - val_loss: 0.2823 - val_accuracy: 0.8878\n",
      "Epoch 4/4\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.1789 - accuracy: 0.9403 - val_loss: 0.3003 - val_accuracy: 0.8784\n",
      "782/782 [==============================] - 1s 898us/step - loss: 0.3258 - accuracy: 0.8668\n",
      "Test loss: 0.3258041739463806\n",
      "Test accuracy: 0.8668000102043152\n"
     ]
    }
   ],
   "source": [
    "#Original Code \n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(partial_x_train, partial_y_train,\n",
    "                    epochs=4,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Test loss:\", results[0])\n",
    "print(\"Test accuracy:\", results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308eaecd",
   "metadata": {},
   "source": [
    "The training loss decreases from approximately 0.5099 to 0.1789, while the training accuracy increases from around 0.7815 to 0.9403. This indicates that the model is improving as it is trained on more epochs.\n",
    "\n",
    "The validation loss decreases from approximately 0.4250 to 0.3003, while the validation accuracy increases from around 0.8237 to 0.8784. This shows that the model is generalizing well to unseen data, as indicated by the improvement in both loss and accuracy on the validation set.\n",
    "\n",
    "After training is complete, the model is evaluated on the test set to assess its generalization performance on completely unseen data.\n",
    "\n",
    "The test loss is approximately 0.3258, and the test accuracy is around 0.8668. These metrics indicate how well the trained model generalizes to unseen data. In this case, the model achieves an accuracy of around 86.68% on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d5245b",
   "metadata": {},
   "source": [
    "#### 6.2. K-fold Validation<a class=\"anchor\" id=\"K-fold\"></a>\n",
    "\n",
    "K-fold cross-validation is used for evaluating the performance of a neural network model on the IMDb dataset. It splits the data into K folds, trains K models on different combinations of training and validation sets, and evaluates each model's performance on the validation set. This process helps to obtain more reliable estimates of the model's performance by averaging the results across multiple folds. The model architecture remains consistent across all folds, consisting of an input layer, two hidden layers with ReLU activation, and an output layer with sigmoid activation. The model is compiled with the RMSprop optimizer and binary cross-entropy loss function. Finally, the average validation loss and accuracy are computed across all folds, providing a comprehensive assessment of the model's performance. \n",
    "\n",
    "<b>During each iteration of the K-fold cross-validation loop, the model is trained on a subset of the training data (x_train_fold, y_train_fold) and then evaluated on the corresponding validation set (x_val_fold, y_val_fold)</b>. This evaluation allows us to assess the model's performance on unseen data from each fold of the training dataset, providing insights into its generalization capabilities across different subsets of the data. The average validation loss and accuracy computed at the end represent the aggregated performance of the model across all folds of the cross-validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9edd2eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 966us/step - loss: 0.3054 - accuracy: 0.8778\n",
      "157/157 [==============================] - 0s 1ms/step - loss: 0.2886 - accuracy: 0.8852\n",
      "157/157 [==============================] - 0s 966us/step - loss: 0.2757 - accuracy: 0.8892\n",
      "157/157 [==============================] - 0s 1ms/step - loss: 0.2644 - accuracy: 0.8958\n",
      "157/157 [==============================] - 0s 1ms/step - loss: 0.3123 - accuracy: 0.8792\n",
      "Average validation loss: 0.2892756164073944\n",
      "Average validation accuracy: 0.8854399919509888\n"
     ]
    }
   ],
   "source": [
    "#Original Code\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "for train_index, val_index in kfold.split(x_train):\n",
    "    x_train_fold, x_val_fold = x_train[train_index], x_train[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, activation='relu', input_shape=(10000,)))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(x_train_fold, y_train_fold, epochs=4, batch_size=512, verbose=0)\n",
    "    \n",
    "    loss, accuracy = model.evaluate(x_val_fold, y_val_fold)\n",
    "    \n",
    "    losses.append(loss)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "average_loss = np.mean(losses)\n",
    "average_accuracy = np.mean(accuracies)\n",
    "\n",
    "print(\"Average validation loss:\", average_loss)\n",
    "print(\"Average validation accuracy:\", average_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a27040d",
   "metadata": {},
   "source": [
    "Validation loss ranges from approximately 0.2644 to 0.3123, while validation accuracy ranges from around 0.8778 to 0.8958 across different folds.\n",
    "\n",
    "The average validation loss is approximately 0.2893, and the average validation accuracy is around 0.8854 for k-fold cross-validation.\n",
    "\n",
    "when comparing the results of hold-out validation and k-fold validation, we observe that the average performance metrics from k-fold cross-validation are slightly better.\n",
    "\n",
    "K-fold cross-validation provides a more robust estimate of the model's performance by averaging the results across multiple folds, whereas hold-out validation relies on a single validation split, which might not be representative of the entire dataset. But k-fold cross-validation requires more computational resources and training time compared to hold-out validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3e45fc",
   "metadata": {},
   "source": [
    "#### <u>7. Fighting Overfitting</u><a class=\"anchor\" id=\"Fighting_Overfitting\"></a>\n",
    "\n",
    "Fighting overfitting refers to the process of implementing strategies or techniques to prevent a machine learning model from fitting too closely to the training data and instead encouraging it to generalize well to unseen data. Overfitting occurs when a model learns to capture noise or idiosyncrasies in the training data, resulting in poor performance on new, unseen data.\n",
    "\n",
    "These are the most common ways to prevent overfitting in neural networks:\n",
    "- Get more training data.\n",
    "- Reduce the capacity of the network. \n",
    "- Add weight regularization.\n",
    "- Add dropout\n",
    "\n",
    "The simplest way to prevent overfitting is to reduce the size of the models size. At the same time, keep in mind that you should use models that have enough parameters that they dont underfit. Lets replace the original baseline network with a smaller network.\n",
    "\n",
    "#### 7.1. Version of the model with lower capacity<a class=\"anchor\" id=\"lower_capacity\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f10362ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(4, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e45925",
   "metadata": {},
   "source": [
    "This model comprises a sequential stack of layers. The first layer is a densely connected layer with 4 units, utilizing the ReLU activation function. It expects input data of shape (batch_size, 10000), where each sample represents a review from the IMDb dataset encoded as a binary vector of length 10000, with each element indicating the presence or absence of a particular word in the review. The subsequent hidden layer is also a densely connected layer with 4 units and ReLU activation. The final layer is a single unit densely connected layer with a sigmoid activation function, which outputs a probability score between 0 and 1, representing the likelihood of the review being positive. This model architecture aims to capture complex relationships within the data and make accurate predictions regarding the sentiment of IMDb reviews.\n",
    "\n",
    "The  smaller network model, is similar to the baseline model but with smaller hidden layers, each containing only 4 units. The rest of the architecture and training configuration remain the same as the baseline model. Both models validation losses over the epochs are plotted for comparison. \n",
    "\n",
    "These models are designed to investigate the impact of network size on model performance and identify the optimal architecture for sentiment analysis on the IMDb dataset.\n",
    "\n",
    "Now lets compare the smaller network to the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a930d583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 0.4590 - accuracy: 0.8222 - val_loss: 0.3470 - val_accuracy: 0.8747\n",
      "Epoch 2/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.2617 - accuracy: 0.9081 - val_loss: 0.2877 - val_accuracy: 0.8876\n",
      "Epoch 3/20\n",
      "49/49 [==============================] - 1s 22ms/step - loss: 0.2013 - accuracy: 0.9276 - val_loss: 0.2850 - val_accuracy: 0.8856\n",
      "Epoch 4/20\n",
      "49/49 [==============================] - 1s 20ms/step - loss: 0.1666 - accuracy: 0.9415 - val_loss: 0.3192 - val_accuracy: 0.8750\n",
      "Epoch 5/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.1470 - accuracy: 0.9487 - val_loss: 0.3105 - val_accuracy: 0.8794\n",
      "Epoch 6/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.1266 - accuracy: 0.9569 - val_loss: 0.3526 - val_accuracy: 0.8702\n",
      "Epoch 7/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.1135 - accuracy: 0.9618 - val_loss: 0.3579 - val_accuracy: 0.8733\n",
      "Epoch 8/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.1031 - accuracy: 0.9650 - val_loss: 0.4262 - val_accuracy: 0.8624\n",
      "Epoch 9/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0925 - accuracy: 0.9678 - val_loss: 0.4168 - val_accuracy: 0.8642\n",
      "Epoch 10/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.0831 - accuracy: 0.9724 - val_loss: 0.4445 - val_accuracy: 0.8616\n",
      "Epoch 11/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.0730 - accuracy: 0.9758 - val_loss: 0.4744 - val_accuracy: 0.8598\n",
      "Epoch 12/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0676 - accuracy: 0.9773 - val_loss: 0.5000 - val_accuracy: 0.8596\n",
      "Epoch 13/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0585 - accuracy: 0.9814 - val_loss: 0.5369 - val_accuracy: 0.8560\n",
      "Epoch 14/20\n",
      "49/49 [==============================] - 1s 17ms/step - loss: 0.0527 - accuracy: 0.9834 - val_loss: 0.5648 - val_accuracy: 0.8572\n",
      "Epoch 15/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0473 - accuracy: 0.9856 - val_loss: 0.5957 - val_accuracy: 0.8549\n",
      "Epoch 16/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0406 - accuracy: 0.9886 - val_loss: 0.6347 - val_accuracy: 0.8520\n",
      "Epoch 17/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.0371 - accuracy: 0.9894 - val_loss: 0.6679 - val_accuracy: 0.8518\n",
      "Epoch 18/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0308 - accuracy: 0.9919 - val_loss: 0.7434 - val_accuracy: 0.8472\n",
      "Epoch 19/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.0298 - accuracy: 0.9915 - val_loss: 0.7356 - val_accuracy: 0.8494\n",
      "Epoch 20/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0235 - accuracy: 0.9938 - val_loss: 0.7789 - val_accuracy: 0.8489\n",
      "Epoch 1/20\n",
      "49/49 [==============================] - 2s 41ms/step - loss: 0.6315 - accuracy: 0.6777 - val_loss: 0.5887 - val_accuracy: 0.7432\n",
      "Epoch 2/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.5474 - accuracy: 0.7936 - val_loss: 0.5335 - val_accuracy: 0.8134\n",
      "Epoch 3/20\n",
      "49/49 [==============================] - 1s 16ms/step - loss: 0.4941 - accuracy: 0.8470 - val_loss: 0.4973 - val_accuracy: 0.8251\n",
      "Epoch 4/20\n",
      "49/49 [==============================] - 1s 17ms/step - loss: 0.4548 - accuracy: 0.8813 - val_loss: 0.4735 - val_accuracy: 0.8353\n",
      "Epoch 5/20\n",
      "49/49 [==============================] - 1s 16ms/step - loss: 0.4245 - accuracy: 0.9002 - val_loss: 0.4539 - val_accuracy: 0.8577\n",
      "Epoch 6/20\n",
      "49/49 [==============================] - 1s 16ms/step - loss: 0.3998 - accuracy: 0.9151 - val_loss: 0.4409 - val_accuracy: 0.8649\n",
      "Epoch 7/20\n",
      "49/49 [==============================] - 1s 16ms/step - loss: 0.3788 - accuracy: 0.9267 - val_loss: 0.4310 - val_accuracy: 0.8704\n",
      "Epoch 8/20\n",
      "49/49 [==============================] - 1s 17ms/step - loss: 0.3599 - accuracy: 0.9354 - val_loss: 0.4262 - val_accuracy: 0.8693\n",
      "Epoch 9/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.3426 - accuracy: 0.9418 - val_loss: 0.4181 - val_accuracy: 0.8740\n",
      "Epoch 10/20\n",
      "49/49 [==============================] - 1s 16ms/step - loss: 0.3270 - accuracy: 0.9480 - val_loss: 0.4180 - val_accuracy: 0.8716\n",
      "Epoch 11/20\n",
      "49/49 [==============================] - 1s 17ms/step - loss: 0.3127 - accuracy: 0.9523 - val_loss: 0.4121 - val_accuracy: 0.8746\n",
      "Epoch 12/20\n",
      "49/49 [==============================] - 1s 16ms/step - loss: 0.2989 - accuracy: 0.9579 - val_loss: 0.4178 - val_accuracy: 0.8697\n",
      "Epoch 13/20\n",
      "49/49 [==============================] - 1s 17ms/step - loss: 0.2862 - accuracy: 0.9602 - val_loss: 0.4227 - val_accuracy: 0.8681\n",
      "Epoch 14/20\n",
      "49/49 [==============================] - 1s 17ms/step - loss: 0.2741 - accuracy: 0.9628 - val_loss: 0.4144 - val_accuracy: 0.8720\n",
      "Epoch 15/20\n",
      "49/49 [==============================] - 2s 38ms/step - loss: 0.2627 - accuracy: 0.9656 - val_loss: 0.4293 - val_accuracy: 0.8672\n",
      "Epoch 16/20\n",
      "49/49 [==============================] - 1s 16ms/step - loss: 0.2514 - accuracy: 0.9676 - val_loss: 0.4355 - val_accuracy: 0.8653\n",
      "Epoch 17/20\n",
      "49/49 [==============================] - 1s 16ms/step - loss: 0.2414 - accuracy: 0.9699 - val_loss: 0.4487 - val_accuracy: 0.8626\n",
      "Epoch 18/20\n",
      "49/49 [==============================] - 1s 17ms/step - loss: 0.2315 - accuracy: 0.9719 - val_loss: 0.4281 - val_accuracy: 0.8671\n",
      "Epoch 19/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.2221 - accuracy: 0.9736 - val_loss: 0.4301 - val_accuracy: 0.8668\n",
      "Epoch 20/20\n",
      "49/49 [==============================] - 1s 17ms/step - loss: 0.2128 - accuracy: 0.9749 - val_loss: 0.4421 - val_accuracy: 0.8653\n"
     ]
    }
   ],
   "source": [
    "#Baseline Model\n",
    "model1 = models.Sequential()\n",
    "model1.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model1.add(layers.Dense(16, activation='relu'))\n",
    "model1.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model1.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history1 = model1.fit(x_train, y_train, epochs=20, batch_size=512, validation_data=(x_test, y_test))\n",
    "\n",
    "#Smaller network model\n",
    "model2 = models.Sequential()\n",
    "model2.add(layers.Dense(4, activation='relu', input_shape=(10000,)))\n",
    "model2.add(layers.Dense(4, activation='relu'))\n",
    "model2.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history2 = model2.fit(x_train, y_train, epochs=20, batch_size=512, validation_data=(x_test, y_test))\n",
    "\n",
    "validation_loss1 = history1.history['val_loss']\n",
    "validation_loss2 = history2.history['val_loss']\n",
    "epochs = range(1, len(validation_loss1) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c01e1bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, validation_loss1, 'bo', label='Baseline Model Validation Loss')\n",
    "plt.plot(epochs, validation_loss2, 'r+', label='Smaller Network Model Validation Loss')\n",
    "plt.title('Validation Loss Comparison')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1f0e71",
   "metadata": {},
   "source": [
    "The validation loss comparison between the baseline model and the smaller network model over the training epochs is plotted. The validation loss values for both models are represented on the y-axis, while the x-axis denotes the epochs during training. Each model's validation loss values are plotted with distinct markers and colors for clarity. This visualization allows for a direct comparison of how the two models perform in terms of minimizing loss during validation, helping to assess their relative effectiveness in learning patterns from the IMDb dataset.\n",
    "\n",
    "<u>Plot Observation -</u>\n",
    "\n",
    "Compared to the reference network, the smaller network begins overfitting later after six epochs as opposed to four and its performance deteriorates more gradually after overfitting.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa212be",
   "metadata": {},
   "source": [
    "### <u>8. Scaling up: developing a model that overfits for baseline model</u><a class=\"anchor\" id=\"Scaling_up_base\"></a>\n",
    "\n",
    "<u>There is another scaling up model that overfits for developing a model that does better than a baseline.</u>\n",
    "\n",
    "Scaling up and developing a model that overfits is crucial in the deep learning workflow to assess the model's capacity, detect signs of overfitting, optimize regularization and hyperparameters, and gain insights into the model's behavior and learning dynamics. By pushing the model to its limits, we can identify its maximum potential and refine it through regularization techniques and hyperparameter tuning to strike the right balance between complexity and generalization, ultimately leading to the development of more robust and effective deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee0b81a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9015beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 0.4656 - accuracy: 0.8209 - val_loss: 0.3354 - val_accuracy: 0.8741\n",
      "Epoch 2/20\n",
      "49/49 [==============================] - 1s 20ms/step - loss: 0.2485 - accuracy: 0.9119 - val_loss: 0.2842 - val_accuracy: 0.8885\n",
      "Epoch 3/20\n",
      "49/49 [==============================] - 1s 20ms/step - loss: 0.1850 - accuracy: 0.9360 - val_loss: 0.2874 - val_accuracy: 0.8860\n",
      "Epoch 4/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.1490 - accuracy: 0.9486 - val_loss: 0.3100 - val_accuracy: 0.8794\n",
      "Epoch 5/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.1243 - accuracy: 0.9598 - val_loss: 0.3394 - val_accuracy: 0.8740\n",
      "Epoch 6/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.1052 - accuracy: 0.9652 - val_loss: 0.3651 - val_accuracy: 0.8712\n",
      "Epoch 7/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.0878 - accuracy: 0.9730 - val_loss: 0.4022 - val_accuracy: 0.8669\n",
      "Epoch 8/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0737 - accuracy: 0.9786 - val_loss: 0.4393 - val_accuracy: 0.8636\n",
      "Epoch 9/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.0617 - accuracy: 0.9846 - val_loss: 0.4903 - val_accuracy: 0.8582\n",
      "Epoch 10/20\n",
      "49/49 [==============================] - 1s 21ms/step - loss: 0.0525 - accuracy: 0.9880 - val_loss: 0.5256 - val_accuracy: 0.8588\n",
      "Epoch 11/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.0432 - accuracy: 0.9912 - val_loss: 0.5670 - val_accuracy: 0.8570\n",
      "Epoch 12/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.0352 - accuracy: 0.9945 - val_loss: 0.6237 - val_accuracy: 0.8536\n",
      "Epoch 13/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.0281 - accuracy: 0.9957 - val_loss: 0.6559 - val_accuracy: 0.8527\n",
      "Epoch 14/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.0222 - accuracy: 0.9973 - val_loss: 0.7002 - val_accuracy: 0.8508\n",
      "Epoch 15/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.0170 - accuracy: 0.9983 - val_loss: 0.7463 - val_accuracy: 0.8492\n",
      "Epoch 16/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.0125 - accuracy: 0.9991 - val_loss: 0.7837 - val_accuracy: 0.8488\n",
      "Epoch 17/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.0094 - accuracy: 0.9998 - val_loss: 0.8255 - val_accuracy: 0.8480\n",
      "Epoch 18/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.0070 - accuracy: 0.9999 - val_loss: 0.8544 - val_accuracy: 0.8486\n",
      "Epoch 19/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.0054 - accuracy: 0.9999 - val_loss: 0.8831 - val_accuracy: 0.8484\n",
      "Epoch 20/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.0043 - accuracy: 0.9999 - val_loss: 0.9138 - val_accuracy: 0.8484\n",
      "Epoch 1/20\n",
      "49/49 [==============================] - 8s 155ms/step - loss: 0.3457 - accuracy: 0.8497 - val_loss: 0.3028 - val_accuracy: 0.8754\n",
      "Epoch 2/20\n",
      "49/49 [==============================] - 6s 127ms/step - loss: 0.1410 - accuracy: 0.9501 - val_loss: 0.3302 - val_accuracy: 0.8745\n",
      "Epoch 3/20\n",
      "49/49 [==============================] - 6s 132ms/step - loss: 0.0498 - accuracy: 0.9853 - val_loss: 0.4321 - val_accuracy: 0.8704\n",
      "Epoch 4/20\n",
      "49/49 [==============================] - 7s 136ms/step - loss: 0.0090 - accuracy: 0.9980 - val_loss: 0.5926 - val_accuracy: 0.8664\n",
      "Epoch 5/20\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6940 - val_accuracy: 0.8664\n",
      "Epoch 6/20\n",
      "49/49 [==============================] - 7s 142ms/step - loss: 9.6942e-04 - accuracy: 1.0000 - val_loss: 0.7235 - val_accuracy: 0.8701\n",
      "Epoch 7/20\n",
      "49/49 [==============================] - 7s 140ms/step - loss: 1.6944e-04 - accuracy: 1.0000 - val_loss: 0.7587 - val_accuracy: 0.8703\n",
      "Epoch 8/20\n",
      "49/49 [==============================] - 7s 143ms/step - loss: 1.1492e-04 - accuracy: 1.0000 - val_loss: 0.7844 - val_accuracy: 0.8708\n",
      "Epoch 9/20\n",
      "49/49 [==============================] - 7s 142ms/step - loss: 8.6698e-05 - accuracy: 1.0000 - val_loss: 0.8039 - val_accuracy: 0.8710\n",
      "Epoch 10/20\n",
      "49/49 [==============================] - 7s 147ms/step - loss: 6.8511e-05 - accuracy: 1.0000 - val_loss: 0.8207 - val_accuracy: 0.8714\n",
      "Epoch 11/20\n",
      "49/49 [==============================] - 7s 146ms/step - loss: 5.5714e-05 - accuracy: 1.0000 - val_loss: 0.8363 - val_accuracy: 0.8717\n",
      "Epoch 12/20\n",
      "49/49 [==============================] - 7s 147ms/step - loss: 4.6354e-05 - accuracy: 1.0000 - val_loss: 0.8486 - val_accuracy: 0.8716\n",
      "Epoch 13/20\n",
      "49/49 [==============================] - 8s 156ms/step - loss: 3.9301e-05 - accuracy: 1.0000 - val_loss: 0.8617 - val_accuracy: 0.8719\n",
      "Epoch 14/20\n",
      "49/49 [==============================] - 7s 149ms/step - loss: 3.3691e-05 - accuracy: 1.0000 - val_loss: 0.8722 - val_accuracy: 0.8718\n",
      "Epoch 15/20\n",
      "49/49 [==============================] - 7s 151ms/step - loss: 2.9230e-05 - accuracy: 1.0000 - val_loss: 0.8823 - val_accuracy: 0.8719\n",
      "Epoch 16/20\n",
      "49/49 [==============================] - 7s 150ms/step - loss: 2.5581e-05 - accuracy: 1.0000 - val_loss: 0.8922 - val_accuracy: 0.8716\n",
      "Epoch 17/20\n",
      "49/49 [==============================] - 7s 154ms/step - loss: 2.2601e-05 - accuracy: 1.0000 - val_loss: 0.9009 - val_accuracy: 0.8718\n",
      "Epoch 18/20\n",
      "49/49 [==============================] - 8s 156ms/step - loss: 2.0087e-05 - accuracy: 1.0000 - val_loss: 0.9094 - val_accuracy: 0.8720\n",
      "Epoch 19/20\n",
      "49/49 [==============================] - 8s 155ms/step - loss: 1.7953e-05 - accuracy: 1.0000 - val_loss: 0.9176 - val_accuracy: 0.8718\n",
      "Epoch 20/20\n",
      "49/49 [==============================] - 7s 152ms/step - loss: 1.6140e-05 - accuracy: 1.0000 - val_loss: 0.9252 - val_accuracy: 0.8717\n"
     ]
    }
   ],
   "source": [
    "model1 = models.Sequential()\n",
    "model1.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model1.add(layers.Dense(16, activation='relu'))\n",
    "model1.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model1.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history1 = model1.fit(x_train, y_train, epochs=20, batch_size=512, validation_data=(x_test, y_test))\n",
    "\n",
    "model2 = models.Sequential()\n",
    "model2.add(layers.Dense(512, activation='relu', input_shape=(10000,)))\n",
    "model2.add(layers.Dense(512, activation='relu'))\n",
    "model2.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history2 = model2.fit(x_train, y_train, epochs=20, batch_size=512, validation_data=(x_test, y_test))\n",
    "\n",
    "validation_loss1 = history1.history['val_loss']\n",
    "validation_loss2 = history2.history['val_loss']\n",
    "epochs = range(1, len(validation_loss1) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d77b7f4",
   "metadata": {},
   "source": [
    "Two models with different hyperparameters are being trained and validated.After training, the validation loss for each model (validation_loss1 and validation_loss2) is extracted from the training history for further analysis. The loss values are stored for each epoch to visualize the training progress and compare the performance of the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "58f5a563",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAwa0lEQVR4nO3deZhU5Zn38e8NQrDZXMCJCnSDERXoZgcxKhgNKqiIJgq2C6IyRE3ijDouvJFW4zjGLIo4MUQFDUSiGNEY44YLuI00DogLGFSQxdGGyGZDpOn7/eOcaqqLqt6qa+mu3+e66qo6+1Onq899nuc55z7m7oiISO5qkekCiIhIZikQiIjkOAUCEZEcp0AgIpLjFAhERHKcAoGISI5TIJCkmZmb2XfCz/eZ2c/qMm8DtlNsZs83tJzSuMzsRjO7P9PlkOSZ7iMQM3sO+B93vylm/Bjgd0AXd6+oYXkHDnf3VXXYVp3mNbMC4FOgVU3bbgxmNgKY7e5dUrmdBNs24MfAJKA78BXwJnCLuy9Pd3kkN6lGIACzgAvCg1K0C4A5qT4Q57i7gZ8CPwEOAHoC84HRGSxTrcxsn0yXQRqPAoFAcOA5ADguMsLM9gdOAx42syFm9qaZbTazz81supm1jrciM5tlZj+PGr42XGaDmU2MmXe0mf2vmW01s7VmVhI1eWH4vtnMtpvZMDObYGavRS1/jJktNrMt4fsxUdNeMbNbzex1M9tmZs+bWaf67hgzOypc12Yze9/MzoiaNsrMPgjXv97MrgnHdzKzp8Nl/mFmi8xsr/81MzscuAIY7+4vufs/3b3c3ee4+3+F83Q0s4fNrMzM1pjZ/4usK9wfr5vZb8JtfRLukwnh/vzSzC6K+dvcZ2YvhGV+1czyo6bfHS631cyWmFn076HEzOaZ2Wwz2wpMCMfNDqe3CadtCsuy2Mz+JZx2iJk9Fe6LVWZ2Wcx6Hw2/47ZwHw+q799JkqNAILj7DuBR4MKo0ecAK9x9GbAb+DegEzAMOBG4vLb1mtkpwDXA94HDgZNiZvk63OZ+BGfAPzKzM8Npx4fv+7l7O3d/M2bdBwB/BaYBBwK/Bv5qZgdGzXYecDFwENA6LEudmVkr4C/A8+E6fgzMMbMjwlkeAP7V3dsDfYCXwvFXA+uAzsC/ADcC8dpgTwTWufvbNRTjHqAj0AMYTrC/Lo6aPhR4l2Af/BGYCwwGvgOcD0w3s3ZR8xcDtxL8LZcCc6KmLQb6EZwU/BF4zMzaRE0fA8wj+HtFLwdwUVjOrmFZJgM7wmmPEOyPQ4AfAP9pZidGLXtGWO79gKeA6Yl3h6SCAoFEPAT80Mz2DYcvDMfh7kvc/S13r3D31QT9BsPrsM5zgJnu/p67fw2URE9091fcfbm7V7r7uwQHjLqsF4LA8Xd3/0NYrkeAFcDpUfPMdPePogJdvzquO+JooB3wX+7+jbu/BDwNjA+n7wJ6mVkHd//K3d+JGn8wkO/uu9x9kcfvjDsQ+DzRxs2sJXAucIO7bwv3/a8ImuwiPnX3me6+G/gTwYH4lrB28TzwDUFQiPiruy90938CU4BhZtYVwN1nu/umcH/+CvgWcETUsm+6+/zw77WD6naF3+c77r47/M1sDdd9LHCdu+9096XA/THf4TV3fyb8Dn8A+ibaJ5IaCgQCgLu/BpQBY8ysB8FZ5R8BzKxn2NTxf2GzwH8SnFHW5hBgbdTwmuiJZjbUzF4Omz22EJxF1rX55pDY9YXDh0YN/1/U53KCg3p9HAKsdffKBNs4GxgFrAmbWYaF4+8EVgHPh8011ydY/yaCgJFIJ4KaTPT3jP2OX0R93gHg7rHjor931d/D3bcD/yD4npjZ1Wb2YdjUtpngDL9TvGXj+APwHDA3bAb8RVijOgT4h7tvq+E7xP6d2pj6INJKgUCiPUxQE7gAeD7qgPJbgrPtw929A0FTR2zHcjyfE5yhRnSLmf5HgqaAru7eEbgvar21Xc62AciPGdcNWF+HctXVBqBrTPt+1TbcfbG7jyFoNppPUOsgPHu/2t17ENRQ/j2mKSRiAdClhjbxjQRn2tHfM9nvWPX3CJuMDgA2hP0B1xHU4vZ39/2ALVT/Oyf8m4Q1n5vdvRdwDEH/0oUE+/AAM2vfiN9BGpkCgUR7mKAd/zLCZqFQe2ArsN3MjgR+VMf1PUrQqdjLzPKAqTHT2xOcLe40syEEbfoRZUAlQdt4PM8APc3sPDPbx8zOBXoRNN00SNjhWfUC3ibox/gPM2tlwWWmpxOc9ba24L6Gju6+i2D/7A7Xc5qZfcfMLGr87tjtufvfgf8GHjGzEeE625jZODO7PmwqeRS4zczahx27/w7Mbuh3BEaZ2bEWdPbfSnDZ8FqCv0UFwX7fx8xuAjrUdaVmdoKZFYbNWVsJAtjucN1vALeH360IuIS9+xgkgxQIpErYBv0G0JbgTD3iGoKD9Dbg9wRt0XVZ39+Auwg6UVexpzM14nLgFjPbBtxEeEYdLlsO3Aa8Hl6FcnTMujcRnHVeTdDE8h/Aae6+sS5li+NQgmaU6FdXgo7MUwnOzv8buNDdV4TLXACsDpvLJhN0zkLQMf4isJ3gnoD/dvdXEmz3JwSdo/cCm4GPgbEEndQQdFB/DXwCvEZQi3qwgd+RcPmpBE1CAwk6jyFo1vkb8BFB081Oam4KivVtgo7krcCHwKvsCVjjgQKC2sETwFR3fyGJ7yCNTDeUieQIM5tFcJXS/8t0WSS7qEYgIpLjFAhERHKcmoZERHJcSmsEZnaKma0Mbyvf61pqC26f/4uZLQtvLb843npERCR1UlYjCC8j+4ggvcA6gtvXx7v7B1Hz3Ah0dPfrzKwzsBL4trt/k2i9nTp18oKCgpSUWUSkuVqyZMlGd+8cb1oq794bAqxy908AzGwuQa6SD6LmcaB9eL11O4JL2mrMdFlQUEBpaWlqSiwi0kyZWeyd+FVS2TR0KNWvQ15H9dvKIbh++iiC64uXAz+NuZ0fADObZGalZlZaVlaWqvKKiOSkVAaCeCkIYtuhTibIgHgIQUKw6Wa2192M7j7D3Qe5+6DOnePWbEREpIFSGQjWUT3PTBeCM/9oFwN/9sAqgidSHZnCMomISIxU9hEsBg43s+4ECabGUT2XDMBnBDnZF4UPsTiC4Fb6etm1axfr1q1j586dSRZZpGlo06YNXbp0oVWrVpkuijQDKQsE7l5hZlcS5DBpCTzo7u+b2eRw+n0ESa9mmdlygqak6xqSK2bdunW0b9+egoICbK+nLYo0L+7Opk2bWLduHd27d890caQZSOl9BOHDJnq6+2Hufls47r4wCODuG9x9pLsXunsfd29QVsWdO3dy4IEHKghITjAzDjzwQNWAc8icOVBQAC1aBO9zGjl3a7N5+IOCgOQS/d5zx5w5MGkSlJcHw2vWBMMAxcWJl6sP5RoSEcliU6bsCQIR5eXB+MaiQNBIWrZsSb9+/ejbty8DBgzgjTfeaNT1T5gwgXnz5gFw6aWX8sEHH9SyRO1eeeUVzIwHHnigatz//u//Ymb88pe/rPN6Vq9eTZ8+fRo0T/fu3Vm5cmW1cVdddRW/+MUvEq6roKCAjRuDrqRjjjkm7jzR+yuRWbNmsWHDngvZGmu/zpo1iyuvvDLp9YgAfPZZ/cY3RE4GglS0t+27774sXbqUZcuWcfvtt3PDDTckv9IE7r//fnr16tUo6yosLORPf9rznJm5c+fSt2/6nh0+btw45s6dWzVcWVnJvHnzOPfcc+u0fDIBNzYQNOZ+FWks3WIf8FrL+IbIuUAQaW9bswbc97S3NWbny9atW9l///0B2L59OyeeeCIDBgygsLCQJ598EoCvv/6a0aNH07dvX/r06VN1MF6yZAnDhw9n4MCBnHzyyXz++ed7rX/EiBFVaTbatWvHlClT6Nu3L0cffTRffBE8ZrisrIyzzz6bwYMHM3jwYF5//fW4Ze3WrRs7d+7kiy++wN159tlnOfXUU6umL126lKOPPpqioiLGjh3LV199VVXOvn37MmzYMO69996q+Xfv3s21117L4MGDKSoq4ne/+12N+2r8+PHVAsHChQspKCggPz+fM888k4EDB9K7d29mzJgRd/l27YLnsrs7V155Jb169WL06NF8+eWXVfPccsstDB48mD59+jBp0iTcnXnz5lFaWkpxcTH9+vVjx44d1fbrI488QmFhIX369OG6666rtr14+7sufv3rX9OnTx/69OnDXXfdBST+HVx//fX06tWLoqIirrnmmjpvQ5qf226DvLzq4/LygvGNxt2b1GvgwIEe64MPPthrXCL5+e5BCKj+ys+v8yriatGihfft29ePOOII79Chg5eWlrq7+65du3zLli3u7l5WVuaHHXaYV1ZW+rx58/zSSy+tWn7z5s3+zTff+LBhw/zLL790d/e5c+f6xRdf7O7uF110kT/22GPu7j58+HBfvHixu7sD/tRTT7m7+7XXXuu33nqru7uPHz/eFy1a5O7ua9as8SOPPHKvMr/88ss+evRov/vuu/2ee+7x1157zSdMmOBTp071O++8093dCwsL/ZVXXnF395/97Gf+05/+dK/x11xzjffu3dvd3X/3u99VlWHnzp0+cOBA/+STT/zTTz+tmidWr169fOnSpe7u/q//+q8+ffp0d3fftGmTu7uXl5d77969fePGje7unp+f72VlZe7u3rZtW3d3f/zxx/2kk07yiooKX79+vXfs2LFqf0XW4+5+/vnnV+2v6P0YPbx+/Xrv2rWrf/nll75r1y4/4YQT/Iknnqhxf0ebOXOmX3HFFdXGlZaWep8+fXz79u2+bds279Wrl7/zzjtxfwebNm3ynj17emVlpbu7f/XVV3H3W31+99K0zZ4dHKPMgvfZs+u/DqDUExxXc65GkKr2tkjT0IoVK3j22We58MILq3byjTfeSFFRESeddBLr16/niy++oLCwkBdffJHrrruORYsW0bFjR1auXMl7773H97//ffr168fPf/5z1q1bV+N2W7duzWmnnQbAwIEDWb16NQAvvvgiV155Jf369eOMM85g69atbNu2Le46zjnnHB577DEeeeQRxo8fXzV+y5YtbN68meHDhwNw0UUXsXDhwr3GX3DBBVXLPP/88zz88MP069ePoUOHsmnTJv7+97/X+B0itYKKigqefPJJfvjDHwIwbdq0qjPvtWvX1riehQsXMn78eFq2bMkhhxzC9773vappL7/8MkOHDqWwsJCXXnqJ999/v8byLF68mBEjRtC5c2f22WcfiouLWbhwIZB4f9fmtddeY+zYsbRt25Z27dpx1llnsWjRori/gw4dOtCmTRsuvfRS/vznP5MXezooOae4GFavhsrK4L2xrhaKaDaXj9ZVt25Bc1C88Y1l2LBhbNy4kbKyMp555hnKyspYsmQJrVq1oqCggJ07d9KzZ0+WLFnCM888ww033MDIkSMZO3YsvXv35s0336zztlq1alV1KWHLli2pqAiSt1ZWVvLmm2+y77771rqOb3/727Rq1YoXXniBu+++u9Z2d3dPePmiu3PPPfdw8sknVxtf0wFz/PjxjBw5kuHDh1NUVMRBBx3EK6+8wosvvsibb75JXl4eI0aMqPW6+Xhl2rlzJ5dffjmlpaV07dqVkpKSWtfjNaRmT7S/a5NonfF+BzfddBNvv/02CxYsYO7cuUyfPp2XXnqpTtsRaYicqxGko71txYoV7N69mwMPPJAtW7Zw0EEH0apVK15++WXWhFFow4YN5OXlcf7553PNNdfwzjvvcMQRR1BWVlYVCHbt2lXr2WsiI0eOZPr06VXDS5curXH+W265hTvuuIOWLVtWjevYsSP7778/ixYtAuAPf/gDw4cPZ7/99qNjx4689tprAMyJ6mA5+eST+e1vf8uuXbsA+Oijj/j6669r3PZhhx3GgQceyPXXX19VI9myZQv7778/eXl5rFixgrfeeqvGdRx//PHMnTuX3bt38/nnn/Pyyy8DVB30O3XqxPbt26tdSdS+ffu4taShQ4fy6quvsnHjRnbv3s0jjzxSVftpqOOPP5758+dTXl7O119/zRNPPMFxxx0X93ewfft2tmzZwqhRo7jrrrtq/duJJCvnagSRKtWUKUFzULduQRBItqq1Y8cO+vXrBwRnfw899BAtW7akuLiY008/nUGDBtGvXz+OPDLIqbd8+XKuvfZaWrRoQatWrfjtb39L69atmTdvHj/5yU/YsmULFRUVXHXVVfTu3bve5Zk2bRpXXHEFRUVFVFRUcPzxx3PfffclnD/RZZgPPfQQkydPpry8nB49ejBz5kwAZs6cycSJE8nLy6t29n/ppZeyevVqBgwYgLvTuXNn5s+fX2t5x48fzw033MDYsWMBOOWUU7jvvvsoKiriiCOO4Oijj65x+bFjx/LSSy9RWFhIz549qw7c++23H5dddhmFhYUUFBQwePDgqmUmTJjA5MmT2XfffavVwg4++GBuv/12TjjhBNydUaNGMWbMmFq/Q7RZs2ZV+95vvfUWEyZMYMiQIUCwn/r3789zzz231+9g27ZtjBkzhp07d+Lu/OY3v6nXtkXqq8k9s3jQoEEe+2CaDz/8kKOOOipDJRLJDP3upT7MbIm7D4o3LeeahkREpDoFAhGRHKdAICJSi1Rn/8y0nOssFhGpj3Rk/8w01QhERGqQjuyfmaZAICJSg3Rk/8w0BYJGkigN9YYNG/jBD36Q1rKsXr0aM+NnP/tZ1biNGzfSqlWreqdHjiR1q+88I0aM4Lnnnqs27q677uLyyy9PuJ7opG+jRo1i8+bNe81TUlJSa4rs+fPnV0snfdNNN/Hiiy/WuExdvPLKK1XpJSR3NEb2z2zvY8jtQFBS0mirSpSG+pBDDqk1L36y4qU56NGjB08//XTV8GOPPdagG9MaKjarKAQprqNzGdXkmWeeYb/99mvQtmMDwS233MJJJ53UoHWJJJuNIB0Zj5OV24Hg5ptTstroNNTRD2QpLy/nnHPOoaioiHPPPZehQ4dWnQE/8MAD9OzZkxEjRnDZZZdVnbknSiddUlLCpEmTGDlyJBdeeOFeZdh333056qijqtb/pz/9iXPOOadq+po1azjxxBMpKirixBNP5LOwnvvpp58ybNgwBg8eXK1GAXDnnXdWpZeeOnVqjfvgBz/4AU8//TT//Oc/q/bDhg0bOPbYY/nRj37EoEGD6N27d8L1RD985rbbbuOII47gpJNOqvYQm9///vcMHjyYvn37cvbZZ1NeXs4bb7zBU089xbXXXku/fv34+OOPqz2kZsGCBfTv35/CwkImTpxYVb6CggKmTp1alS58xYoVNX6/aPFSVu/evZsJEybQp08fCgsLq+4OnjZtWlV66XHjxtV5G5I5xcUwYwbk54NZ8D5jRt07iptEH0OitKTZ+ko2DXU10LDl4kiUhjo6/fKdd97pkyZNcnf35cuXe8uWLavSHufn5/umTZv8m2++8WOPPbYqjXGidNJTp071AQMGeHl5+V5liWzzySef9KuvvtrXrl3r3/ve96qlRz7ttNN81qxZ7u7+wAMP+JgxY9zd/fTTT/eHHnrI3d2nT59eleb5ueee88suu8wrKyt99+7dPnr0aH/11VfdfU8q6FijRo3y+fPnu7v77bff7tdcc42770kLXVFR4cOHD/dly5a5e/W00JFU05H0zV9//bVv2bLFDzvssKoU2ZG01O7uU6ZM8WnTprl79ZTd0cM7duzwLl26+MqVK93d/YILLvDf/OY3VduLLH/vvff6JZdcstf3iaTtjpYoZXVpaamfdNJJVfNFUkkffPDBvnPnzmrjGkppqJsGs/ip783SWw6UhjpKSUkQ1iOZKiOfk2wmSpSGOtprr71WdRbYp08fioqKAHj77bcZPnw4BxxwAK1atapKwww1p5M+44wzaswuesopp/DCCy/wyCOP7PXErzfffJPzzjsPCNJIRxLIvf7661XNN7HppZ9//nn69+/PgAEDWLFiRZ3TS0P1ZqFHH32UAQMG0L9/f95///0aHw+5aNEixo4dS15eHh06dOCMM86omvbee+9x3HHHUVhYyJw5c2pN0Ldy5Uq6d+9Oz549gT1ptSPOOussoH7ppROlrO7RoweffPIJP/7xj3n22Wfp0KEDAEVFRRQXFzN79mz22UdXb+eCdDxhLFm5GQgiQRn2fG7E/oLoNNTRYgNDbeNhTzrppUuXsnTpUtavX0/79u0BaNu2bY3laN26NQMHDuRXv/oVZ599do3zRqdwjpfO2d254YYbqsqxatUqLrnkkhrXeeaZZ7JgwQLeeecdduzYwYABA/j000/55S9/yYIFC3j33XcZPXp0g9JLQ5A0bvr06SxfvpypU6cmlV4a4Fvf+hbQOOml999/f5YtW8aIESO49957ufTSSwH461//yhVXXMGSJUsYOHBgnbcjTVdanjCWpNwLBGkQnYY62rHHHsujjz4KwAcffMDy5csBGDJkCK+++ipfffUVFRUVPP7441XL1DeddKyrr76aO+64Y6+yHHPMMVVn63PmzOHYY48F4Lvf/W618REnn3wyDz74INu3bwdg/fr11R4HGU+7du0YMWIEEydOrKoNbN26lbZt29KxY0e++OIL/va3v9W4juOPP54nnniCHTt2sG3bNv7yl79UTdu2bRsHH3wwu3btqlbWROmljzzySFavXs2qVauAPWm1k5EoZfXGjRuprKzk7LPP5tZbb+Wdd96hsrKStWvXcsIJJ/CLX/yCzZs3V+1Pab6S7WNIh9yum9bS4VkfidJQR7v88su56KKLKCoqon///hQVFdGxY0cOPfRQbrzxRoYOHcohhxxCr1696NixI1D/dNKxevfuHfdqoWnTpjFx4kTuvPNOOnfuXJVe+u677+a8887j7rvvrlaLGDlyJB9++CHDhg0DgoP87NmzOeigg2rc/vjx4znrrLOqgkvfvn3p378/vXv3pkePHnz3u9+tcfkBAwZw7rnn0q9fP/Lz8znuuOOqpt16660MHTqU/Px8CgsLqw7+48aN47LLLmPatGnVrthq06YNM2fO5Ic//CEVFRUMHjyYyZMn17j9WAsWLKBLly5Vw4899ljclNXLli3j4osvprKyEoDbb7+d3bt3c/7557NlyxbcnX/7t39r8JVR0rQUF2fXgT+W0lCn0e7du9m1axdt2rTh448/5sQTT+Sjjz6idevWbN++nXbt2lFRUcHYsWOZOHFiVW5+kXiayu9eskNNaahzu0aQZuXl5Zxwwgns2rULd696GA0El4O++OKL7Ny5k5EjR3LmmWdmtrAikjMUCNKoffv2xNZmImq7W1ZEJFWaTWdxU2viEkmGfu/SmJpFIGjTpg2bNm3SP4fkBHdn06ZNtGnTJtNFkWaiWTQNdenShXXr1u113b5Ic9WmTZtqVy+JJCOlgcDMTgHuBloC97v7f8VMvxaIXFS1D3AU0Nnd/1Gf7bRq1Yru3bs3QolFRHJPypqGzKwlcC9wKtALGG9mvaLncfc73b2fu/cDbgBerW8QEBGR5KSyj2AIsMrdP3H3b4C5wJga5h8PPJLC8oiISBypDASHAmujhteF4/ZiZnnAKcDjCaZPMrNSMytVP4CISONKZSCIlyks0WU9pwOvJ2oWcvcZ7j7I3Qd17ty50QooIiKpDQTrgK5Rw12ADQnmHYeahUREMiKVgWAxcLiZdTez1gQH+6diZzKzjsBw4MkUlkVERBJI2eWj7l5hZlcCzxFcPvqgu79vZpPD6ZEUmmOB593961SVRUREEmsW2UdFRKRmNWUfbRYpJkREpOEUCEREcpwCgYhIjlMgEBHJcQoEItLszZkDBQXQokXwPmdOpkuUXZpFGmoRkUTmzIFJk6C8PBhesyYYhux+oHw6qUYgIs3alCl7gkBEeXkwXgIKBCLSrH32Wf3G5yIFAhFp1rp1q9/4XKRAICLN2m23QV5e9XF5ecF4CSgQiEizVlwMM2ZAfj6YBe8zZqijOJquGhKRZq+4WAf+mqhGICKS4xQIRERynAKBiEiOUyAQEclxCgQiIjlOgUBEsp6SxqWWLh8VkaympHGppxqBiGQ1JY1LPQUCEclqShqXegoEIpLVlDQu9RQIRCSrKWlc6ikQiEhWU9K41NNVQyKS9ZQ0LrVUIxARyXEKBCIiOU6BQEQkxykQiIjkOAUCEZEcp0AgIpLjUhoIzOwUM1tpZqvM7PoE84wws6Vm9r6ZvZrK8oiIyN5Sdh+BmbUE7gW+D6wDFpvZU+7+QdQ8+wH/DZzi7p+Z2UGpKo+IiMSXyhrBEGCVu3/i7t8Ac4ExMfOcB/zZ3T8DcPcvU1geEckQPU8gu6UyEBwKrI0aXheOi9YT2N/MXjGzJWZ2YbwVmdkkMys1s9KysrIUFVdEUiHyPIE1a8B9z/MEFAyyRyoDgcUZ5zHD+wADgdHAycDPzKznXgu5z3D3Qe4+qHPnzg0vUUlJw5cVkQbR8wSyXyoDwTqga9RwF2BDnHmedfev3X0jsBDom7IS3XxzylYtIvHpeQLZL5WBYDFwuJl1N7PWwDjgqZh5ngSOM7N9zCwPGAp8mMIyiUia6XkC2S9lgcDdK4ArgecIDu6Puvv7ZjbZzCaH83wIPAu8C7wN3O/u7zVqQUpKgty1FrZURT6rmUgkLfQ8gexn7rHN9tlt0KBBXlpa2rCFzYLeKhFJqzlzgj6Bzz4LagK33aa00ulmZkvcfVC8aXoegYiknJ4nkN1yK8XE1KmZLoGISNbJrUCgfgERkb3kViAQEZG9KBCIiOQ4BQIRqZVyBTVvumpIRGoUyRUUSRMRyRUEuhKouVCNQERqpFxBzZ8CgYjUSLmCmj8FAhGpkXIFNX8KBCJSI+UKav4UCESkRsXFMGMG5OcH6bry84NhdRQ3H7pqSERqpVxBzVudagRm1tbMWoSfe5rZGWbWKrVFExGRdKhr09BCoI2ZHQosAC4GZqWqUCIikj51DQTm7uXAWcA97j4W6JW6YomISLrUORCY2TCgGPhrOE79CyJNhFJESE3qGgiuAm4AnggfN9kDeDllpcpWSmMtTVAkRcSaNcED+iIpIhQMJKLej6oMO43bufvW1BSpZkk9qjJZetSlNEEFBcHBP1Z+Pqxene7SSKbU9KjKul419Ecz62BmbYEPgJVmdm1jFlJEUkMpIqQ2dW0a6hXWAM4EngG6ARekqlBZpaQkqAmYBcORz2omkiZCKSKkNnUNBK3C+wbOBJ50911AbrSRlJQEzUGRJqHIZwUCaSKUIkJqU9dA8DtgNdAWWGhm+UBG+ghEpH6UIkJqU6dLQN19GjAtatQaMzshNUXKYlOnZroEIg2iFBFSk7p2Fnc0s1+bWWn4+hVB7SC3qDlIRJqhujYNPQhsA84JX1uBmakqlIiIpE9d7w4+zN3Pjhq+2cyWpqA8IiKSZnWtEewws2MjA2b2XWBHaookIiLpVNcawWTgYTPrGA5/BVyUmiKJiEg61fWqoWVAXzPrEA5vNbOrgHdTWDYREUmDej2q0t23RuUY+vcUlEdERNIsmWcWW60zmJ1iZivNbJWZXR9n+ggz22JmS8PXTUmUR6TZUhppSaVkAkGNKSbMrCVwL3AqwUNsxptZvIfZLHL3fuHrliTKk/10H4I0gNJIS6rVGAjMbJuZbY3z2gYcUsu6hwCr3P0Td/8GmAuMaaRyN00335zpEkgTNGUKlJdXH1deHowXaQw1BgJ3b+/uHeK82rt7bR3NhwJro4bXheNiDTOzZWb2NzPrHW9FZjYpcldzWVlZLZsVaV6URlpSLZmmodrE60OIbU56B8h3977APcD8eCty9xnuPsjdB3Xu3LlxS5lqSmMtSVIaaUm1VAaCdUDXqOEuwIboGcKrkLaHn58hSHfdKYVlSj+lsZYkKY20pFoqA8Fi4HAz625mrYFxwFPRM5jZt82CU2UzGxKWZ1MKyyTS5CiNtKRaXe8srjd3rzCzK4HngJbAg+GD7yeH0+8DfgD8yMwqCFJWjPP6PkS5KVEaa2kgpZGWVKr3w+szLaMPrxcRaaKSfni9iIg0XwoETYk6mJss3Rks2UyBoCnRDWlNku4MlmynQCCSYrozWLKdAkG20w1pTZ7uDJZsp0CQ7XRDWpOnO4Ml2ykQiKSY7gyWbKdA0JTohrQmSXcGS7bTDWUiIjlAN5RJQP0KIhKHAkEu0X0IIhKHAoGI5A7ViuNSIGjudB+CyB6qFcelQNDc6T6ERqFcQdKcKRCI1EK5gpq45lQrTlGZdfloLikpSf6H1BjraGIKCoKDf6z8fFi9Ot2lkaSY7akdN0VJlL+my0cVCKR+mvo/UgO0aBH/K5tBZWX6yyNJaOq/3xQFAjUNidRCuYKySLK10aZ4d34amrYUCKR2jflDbILNSsoVlEWSveqnCf7+0nHBhwKB1K4xf4hN8PI95QpqRE3xQNyYsvT7KxBIkxC5fPNmK8nI5ZvFxUHHcGVl8K4g0EANORHIpqt+kt1msidCKWraUmex1E9DrhoqKYn/DzB1ap3WFbl8s7wcHMNw8vJ0Vt4kJdtZm+nO3iZcfnUWS+NpaL9AEk1LetRjI8rUWXS2nNFnQhP4/goEkvUuXlOCYzjBP1Lk88VrSjJbsKYoE300jdnHlImrfpI9kDeBu/sVCCS9GvCPPDO/JDz0B/9Ikc8z80vqvI5G62PI9D9vprefaZmq0WT5gTxp7t6kXgMHDnTJLbNnu+flBf99Dg7B8OzZ6Vm+GmjAQo2oIdufOjVy6Kr+mjq1YetKRrLLZ1qyf/8Mfn+g1BMcVzN+YK/vS4EgN82e7Z6f717CVM/Pr99BPD9/z7EvEgggGF9vmT4QJLv9TC/f1DXhQFZTIFDTkDQJkcs3p3pJvS/fTLqPoTE7+5r65ZO5rpnuc10+Ks1edNK4yOWn0MCkcZm+fDDZ5TNw+a9kB10+Kjkt4ykisumMPgOX/0r2UyCQZi86RcTNTE0uRURDLl9s6pdPSrOX0qYhMzsFuBtoCdzv7v+VYL7BwFvAue4+r6Z1qmlImrRM3xmbrBx8HkVzkZGmITNrCdwLnAr0AsabWa8E890BPJeqsohkjaZ+Rq8g0CylsmloCLDK3T9x92+AucCYOPP9GHgc+DKFZRHJDjqQShZKZSA4FFgbNbwuHFfFzA4FxgL31bQiM5tkZqVmVlpWVtboBRURyWWpDAQWZ1xs4+hdwHXuvrumFbn7DHcf5O6DOnfu3FjlExERYJ8Urnsd0DVquAuwIWaeQcBcCy6r6wSMMrMKd5+fwnKJiEiUVAaCxcDhZtYdWA+MA86LnsHdu0c+m9ks4GkFARGR9EpZIHD3CjO7kuBqoJbAg+7+vplNDqfX2C8gIiLpkcoaAe7+DPBMzLi4AcDdJ6SyLCIiEp/uLBYRyXEKBCIiOU6BQEQkxykQiIjkOAUCEZEcp0AgIpLjFAhERHKcAoGISI5TIJC0mDMneHZwixbB+5w5mS6RiESk9M5iEQgO+pMmQXl5MLxmTTAMDXxcpIg0KtUIpE6SOaOfMmVPEIgoLw/Gi0jmqUYgtUr2jP6zz+o3XkTSKydqBGqfTk6yZ/TdutVvvIikV7MPBJGz2TVrwH3P2ayCQd0le0Z/222Ql1d9XF5eMF5EMq/ZBwK1Tycv2TP64mKYMQPy88EseJ8xQx3FItmi2QcCtU8nrzHO6IuLYfVqqKwM3hUERLJHsw8Eap9Ons7oRZq3Zh8I1D7dOHRGL9J8NftAoLNZEZGa5cR9BMXFOvCLiCTS7GsEIiJSMwUCEZEcp0AgIpLjFAiaAKXIEJFUyonO4qZMKZxFJNVUI8hySpEhIqmmQJDllCJDRFJNgSDLNUaKDPUxiEhNFAiyXLIpMpSGW0Rqo0CQ5ZJNkaE+BhGpjbl7pstQL4MGDfLS0tJMF6PJaNEiqAnEMgsSyIlIbjCzJe4+KN60lNYIzOwUM1tpZqvM7Po408eY2btmttTMSs3s2FSWJxcpDbeI1CZlgcDMWgL3AqcCvYDxZtYrZrYFQF937wdMBO5PVXlyldJwi0htUlkjGAKscvdP3P0bYC4wJnoGd9/ue9qm2gJNq52qCVAabhGpTSrvLD4UWBs1vA4YGjuTmY0FbgcOAkbHW5GZTQImAXRTm0a9KQ23iNQklTUCizNurzN+d3/C3Y8EzgRujbcid5/h7oPcfVDnzp0bt5QiIjkulYFgHdA1argLsCHRzO6+EDjMzDqlsEwNohuyRKQ5S2UgWAwcbmbdzaw1MA54KnoGM/uOmVn4eQDQGtiUwjLVW2PckKVAIiLZLGWBwN0rgCuB54APgUfd/X0zm2xmk8PZzgbeM7OlBFcYnetZdmNDsjdk6c5eEcl2uqGsFsnekFVQEBz8Y+Xnw+rVyZZORKRuMnZDWXOQ7A1Zyh4qItlOgaAWyd6QpTt7RSTbKRDUItkbsnRnr4hkOz2qsg6SuSErstyUKUFzULduQRDQDV4iki0UCNJAd/aKSDZT05CISI5TIBARyXEKBCIiOU6BQEQkxykQiIjkuCaXYsLMyoA4SRuyQidgY6YLUYNsLx9kfxlVvuSofMlJpnz57h43j3+TCwTZzMxKE+XyyAbZXj7I/jKqfMlR+ZKTqvKpaUhEJMcpEIiI5DgFgsY1I9MFqEW2lw+yv4wqX3JUvuSkpHzqIxARyXGqEYiI5DgFAhGRHKdAUE9m1tXMXjazD83sfTP7aZx5RpjZFjNbGr5uSnMZV5vZ8nDbez3X0wLTzGyVmb1rZgPSWLYjovbLUjPbamZXxcyT9v1nZg+a2Zdm9l7UuAPM7AUz+3v4vn+CZU8xs5Xh/rw+jeW708xWhH/DJ8xsvwTL1vh7SGH5SsxsfdTfcVSCZTO1//4UVbbV4bPT4y2b0v2X6JiS1t+fu+tVjxdwMDAg/Nwe+AjoFTPPCODpDJZxNdCphumjgL8BBhwN/E+GytkS+D+CG10yuv+A44EBwHtR434BXB9+vh64I8F3+BjoAbQGlsX+HlJYvpHAPuHnO+KVry6/hxSWrwS4pg6/gYzsv5jpvwJuysT+S3RMSefvTzWCenL3z939nfDzNuBD4NDMlqrexgAPe+AtYD8zOzgD5TgR+NjdM36nuLsvBP4RM3oM8FD4+SHgzDiLDgFWufsn7v4NMDdcLuXlc/fn3b0iHHwL6NLY262rBPuvLjK2/yLMzIBzgEcae7t1UcMxJW2/PwWCJJhZAdAf+J84k4eZ2TIz+5uZ9U5vyXDgeTNbYmaT4kw/FFgbNbyOzASzcST+58vk/ov4F3f/HIJ/VuCgOPNky76cSFDLi6e230MqXRk2XT2YoGkjG/bfccAX7v73BNPTtv9ijilp+/0pEDSQmbUDHgeucvetMZPfIWju6AvcA8xPc/G+6+4DgFOBK8zs+JjpFmeZtF5HbGatgTOAx+JMzvT+q49s2JdTgApgToJZavs9pMpvgcOAfsDnBM0vsTK+/4Dx1FwbSMv+q+WYknCxOOPqvf8UCBrAzFoR/MHmuPufY6e7+1Z33x5+fgZoZWad0lU+d98Qvn8JPEFQfYy2DugaNdwF2JCe0lU5FXjH3b+InZDp/Rfli0iTWfj+ZZx5Mrovzewi4DSg2MNG41h1+D2khLt/4e673b0S+H2C7WZ6/+0DnAX8KdE86dh/CY4pafv9KRDUU9ie+ADwobv/OsE83w7nw8yGEOznTWkqX1szax/5TNCh+F7MbE8BF1rgaGBLpAqaRgnPwjK5/2I8BVwUfr4IeDLOPIuBw82se1jLGRcul3JmdgpwHXCGu5cnmKcuv4dUlS+632lsgu1mbP+FTgJWuPu6eBPTsf9qOKak7/eXqp7w5voCjiWoer0LLA1fo4DJwORwniuB9wl68N8Cjklj+XqE210WlmFKOD66fAbcS3C1wXJgUJr3YR7Bgb1j1LiM7j+CoPQ5sIvgLOsS4EBgAfD38P2AcN5DgGeilh1FcKXHx5H9nabyrSJoH478Du+LLV+i30OayveH8Pf1LsHB6eBs2n/h+FmR313UvGndfzUcU9L2+1OKCRGRHKemIRGRHKdAICKS4xQIRERynAKBiEiOUyAQEclxCgQiITPbbdUzozZaJkwzK4jOfCmSTfbJdAFEssgOd++X6UKIpJtqBCK1CPPR32Fmb4ev74Tj881sQZhUbYGZdQvH/4sFzwdYFr6OCVfV0sx+H+acf97M9g3n/4mZfRCuZ26GvqbkMAUCkT32jWkaOjdq2lZ3HwJMB+4Kx00nSOddRJDwbVo4fhrwqgdJ8wYQ3JEKcDhwr7v3BjYDZ4fjrwf6h+uZnJqvJpKY7iwWCZnZdndvF2f8auB77v5JmBzs/9z9QDPbSJA2YVc4/nN372RmZUAXd/9n1DoKgBfc/fBw+Dqglbv/3MyeBbYTZFmd72HCPZF0UY1ApG48wedE88Tzz6jPu9nTRzeaIPfTQGBJmBFTJG0UCETq5tyo9zfDz28QZHsEKAZeCz8vAH4EYGYtzaxDopWaWQugq7u/DPwHsB+wV61EJJV05iGyx75W/QHmz7p75BLSb5nZ/xCcPI0Px/0EeNDMrgXKgIvD8T8FZpjZJQRn/j8iyHwZT0tgtpl1JMgK+xt339xI30ekTtRHIFKLsI9gkLtvzHRZRFJBTUMiIjlONQIRkRynGoGISI5TIBARyXEKBCIiOU6BQEQkxykQiIjkuP8PWySDoSl/ff8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, validation_loss1, 'bo', label='Baseline Model Validation Loss')\n",
    "plt.plot(epochs, validation_loss2, 'r+', label='Bigger Model Validation Loss')\n",
    "plt.title('Validation Loss Comparison')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf08afe",
   "metadata": {},
   "source": [
    "<u>Plot Observation - </u>\n",
    "\n",
    "Shows how the bigger network fares compared to the reference network.\n",
    " \n",
    "The bigger network starts overfitting almost immediately, after just one epoch, and it overfits much more severely. Its validation loss is also noisier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "570187e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_loss1 = history1.history['loss']\n",
    "training_loss2 = history2.history['loss']\n",
    "epochs = range(1, len(training_loss1) + 1)\n",
    "\n",
    "plt.plot(epochs, training_loss1, 'bo', label='Baseline Training Loss')\n",
    "plt.plot(epochs, training_loss2, 'r+', label='Bigger Model Training Loss')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef62dc7",
   "metadata": {},
   "source": [
    "<u>Plot Obeservation -</u>\n",
    "\n",
    "This plot shows the training losses for the two networks. As you can see, the bigger network gets its training loss near zero very quickly. The more capacity the network has, the more quickly it can model the training data which results in a low training loss, but it is then more susceptible to overfitting resulting in a large difference between the training and validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4facab9",
   "metadata": {},
   "source": [
    "### <u>9. Regularizing your model and tuning your hyperparameters</u><a class=\"anchor\" id=\"tuning\"></a>\n",
    "\n",
    "Regularization in deep learning refers to techniques used to prevent overfitting, which occurs when a model learns to memorize the training data rather than capturing generalizable patterns. Overfitting can lead to poor performance on unseen data.\n",
    "\n",
    "Tuning hyperparameters involves selecting the optimal values for these settings to maximize the model's effectiveness. In deep learning, common hyperparameters include the learning rate, batch size, number of layers, number of neurons per layer, activation functions, dropout rates, and regularization strengths. \n",
    "\n",
    "<u>Hyperparameter tuning is typically conducted through experimentation, where different combinations of hyperparameters are tested on a validation set, and the best-performing configuration is selected based on predefined evaluation metrics</u>.Which i will do in my extensive experimentation and deploying a model that does better than baseline.\n",
    "\n",
    "For now, we will do these -\n",
    "- L1 and/or L2 regularization.\n",
    "- Add dropout.\n",
    "- Try different architectures: add or remove layers. Done with Dense Layers \n",
    "- Try different hyperparameters (such as the number of units per layer or the learning rate of the optimizer) to find the optimal configuration. Done as part of developing a model better than baseline and extensive experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5777379f",
   "metadata": {},
   "source": [
    "#### 9.1. Adding L2 weight regularization to the model <a class=\"anchor\" id=\"L2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7bb650fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "activation='relu', input_shape=(10000,))) \n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                       activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d141a6f6",
   "metadata": {},
   "source": [
    "The above model is a neural network model with two hidden layers, each consisting of 16 units. Regularization is applied to the kernel weights of these layers using L2 regularization with a penalty factor of 0.001. l2(0.001) means every coefficient in the weight matrix of the layer will add 0.001 * weight_coefficient_value to the total loss of the network. Note that because this penalty is only added at training time, the loss for this network will be much higher at training than at test time.The activation function used in the hidden layers is ReLU (Rectified Linear Unit). The input shape of the first layer is specified as (10000,), indicating that the input data has 10,000 features. Finally, a single-unit output layer with a sigmoid activation function is added to produce binary classification predictions. This regularization technique helps prevent overfitting by adding a penalty term to the loss function based on the magnitude of the weights, encouraging smaller weights and reducing model complexity.\n",
    "\n",
    "The second model that will be used below incorporates L2 regularization with a penalty factor of 0.001 on the kernel weights of the hidden layers. Both models are compiled using the RMSprop optimizer and binary cross-entropy loss function. They are trained for 20 epochs with a batch size of 512 and validated using the test data. Finally, the validation losses of both models are plotted to compare their performance. L2 regularization is expected to reduce overfitting by penalizing large weights in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0a0284a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "49/49 [==============================] - 2s 42ms/step - loss: 0.4595 - accuracy: 0.8216 - val_loss: 0.3432 - val_accuracy: 0.8813\n",
      "Epoch 2/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.2632 - accuracy: 0.9106 - val_loss: 0.2852 - val_accuracy: 0.8898\n",
      "Epoch 3/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.2029 - accuracy: 0.9283 - val_loss: 0.2800 - val_accuracy: 0.8894\n",
      "Epoch 4/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.1701 - accuracy: 0.9392 - val_loss: 0.3062 - val_accuracy: 0.8786\n",
      "Epoch 5/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.1480 - accuracy: 0.9490 - val_loss: 0.3096 - val_accuracy: 0.8797\n",
      "Epoch 6/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.1276 - accuracy: 0.9571 - val_loss: 0.3278 - val_accuracy: 0.8777\n",
      "Epoch 7/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.1146 - accuracy: 0.9619 - val_loss: 0.3481 - val_accuracy: 0.8745\n",
      "Epoch 8/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.1007 - accuracy: 0.9654 - val_loss: 0.3726 - val_accuracy: 0.8714\n",
      "Epoch 9/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0914 - accuracy: 0.9718 - val_loss: 0.4005 - val_accuracy: 0.8694\n",
      "Epoch 10/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.0809 - accuracy: 0.9738 - val_loss: 0.4802 - val_accuracy: 0.8518\n",
      "Epoch 11/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.0726 - accuracy: 0.9778 - val_loss: 0.4560 - val_accuracy: 0.8638\n",
      "Epoch 12/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.0636 - accuracy: 0.9810 - val_loss: 0.4855 - val_accuracy: 0.8632\n",
      "Epoch 13/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0565 - accuracy: 0.9834 - val_loss: 0.5142 - val_accuracy: 0.8604\n",
      "Epoch 14/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0500 - accuracy: 0.9856 - val_loss: 0.5603 - val_accuracy: 0.8520\n",
      "Epoch 15/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0418 - accuracy: 0.9886 - val_loss: 0.5819 - val_accuracy: 0.8578\n",
      "Epoch 16/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.0375 - accuracy: 0.9893 - val_loss: 0.6126 - val_accuracy: 0.8561\n",
      "Epoch 17/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0313 - accuracy: 0.9915 - val_loss: 0.6575 - val_accuracy: 0.8506\n",
      "Epoch 18/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0283 - accuracy: 0.9925 - val_loss: 0.6900 - val_accuracy: 0.8535\n",
      "Epoch 19/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0229 - accuracy: 0.9943 - val_loss: 0.7678 - val_accuracy: 0.8500\n",
      "Epoch 20/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0221 - accuracy: 0.9944 - val_loss: 0.7658 - val_accuracy: 0.8512\n",
      "Epoch 1/20\n",
      "49/49 [==============================] - 3s 55ms/step - loss: 0.4903 - accuracy: 0.8138 - val_loss: 0.3766 - val_accuracy: 0.8804\n",
      "Epoch 2/20\n",
      "49/49 [==============================] - 1s 20ms/step - loss: 0.3116 - accuracy: 0.9040 - val_loss: 0.3340 - val_accuracy: 0.8880\n",
      "Epoch 3/20\n",
      "49/49 [==============================] - 1s 17ms/step - loss: 0.2704 - accuracy: 0.9191 - val_loss: 0.3285 - val_accuracy: 0.8883\n",
      "Epoch 4/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.2470 - accuracy: 0.9295 - val_loss: 0.3388 - val_accuracy: 0.8841\n",
      "Epoch 5/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.2366 - accuracy: 0.9324 - val_loss: 0.3408 - val_accuracy: 0.8840\n",
      "Epoch 6/20\n",
      "49/49 [==============================] - 1s 17ms/step - loss: 0.2275 - accuracy: 0.9377 - val_loss: 0.3528 - val_accuracy: 0.8795\n",
      "Epoch 7/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.2202 - accuracy: 0.9397 - val_loss: 0.3574 - val_accuracy: 0.8792\n",
      "Epoch 8/20\n",
      "49/49 [==============================] - 1s 17ms/step - loss: 0.2190 - accuracy: 0.9400 - val_loss: 0.3786 - val_accuracy: 0.8729\n",
      "Epoch 9/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.2148 - accuracy: 0.9419 - val_loss: 0.3691 - val_accuracy: 0.8768\n",
      "Epoch 10/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.2085 - accuracy: 0.9439 - val_loss: 0.3881 - val_accuracy: 0.8733\n",
      "Epoch 11/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.2027 - accuracy: 0.9454 - val_loss: 0.4204 - val_accuracy: 0.8629\n",
      "Epoch 12/20\n",
      "49/49 [==============================] - 1s 20ms/step - loss: 0.2020 - accuracy: 0.9463 - val_loss: 0.5037 - val_accuracy: 0.8414\n",
      "Epoch 13/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.1998 - accuracy: 0.9467 - val_loss: 0.4191 - val_accuracy: 0.8650\n",
      "Epoch 14/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.1975 - accuracy: 0.9488 - val_loss: 0.4213 - val_accuracy: 0.8651\n",
      "Epoch 15/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.1927 - accuracy: 0.9486 - val_loss: 0.3978 - val_accuracy: 0.8711\n",
      "Epoch 16/20\n",
      "49/49 [==============================] - 1s 20ms/step - loss: 0.1924 - accuracy: 0.9495 - val_loss: 0.4460 - val_accuracy: 0.8564\n",
      "Epoch 17/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.1889 - accuracy: 0.9516 - val_loss: 0.4041 - val_accuracy: 0.8698\n",
      "Epoch 18/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.1883 - accuracy: 0.9512 - val_loss: 0.4087 - val_accuracy: 0.8691\n",
      "Epoch 19/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.1853 - accuracy: 0.9520 - val_loss: 0.4094 - val_accuracy: 0.8694\n",
      "Epoch 20/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.1812 - accuracy: 0.9550 - val_loss: 0.4503 - val_accuracy: 0.8606\n"
     ]
    }
   ],
   "source": [
    "#First model without regularization\n",
    "model1 = models.Sequential()\n",
    "model1.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model1.add(layers.Dense(16, activation='relu'))\n",
    "model1.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model1.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history1 = model1.fit(x_train, y_train, epochs=20, batch_size=512, validation_data=(x_test, y_test))\n",
    "\n",
    "#Second model with L2 regularization\n",
    "model2 = models.Sequential()\n",
    "model2.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                         activation='relu', input_shape=(10000,)))\n",
    "model2.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
    "                         activation='relu'))\n",
    "model2.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history2 = model2.fit(x_train, y_train, epochs=20, batch_size=512, validation_data=(x_test, y_test))\n",
    "\n",
    "validation_loss1 = history1.history['val_loss']\n",
    "validation_loss2 = history2.history['val_loss']\n",
    "epochs = range(1, len(validation_loss1) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "7a55f8d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, validation_loss1, 'r+', label='Baseline Model Without Regularization Validation Loss')\n",
    "plt.plot(epochs, validation_loss2, 'bo', label='Model With L2 Regularization Validation Loss')\n",
    "plt.title('Validation Loss Comparison')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0970dd7",
   "metadata": {},
   "source": [
    "<u>Plot Observation -</u>\n",
    "\n",
    "This validation loss comparison plot shows the impact of the L2 regularization penalty. As you can see, the model with L2 regularization (dots) has become much more resistant to overfitting than the reference model (crosses), even though both models have the same number of parameters. Initially, both models start with relatively high validation losses. However, as training progresses, the validation loss decreases for both models. The model with L2 regularization consistently maintains a lower validation loss compared to the baseline model without regularization, indicating that L2 regularization helps prevent overfitting by constraining the weights of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033dacc8",
   "metadata": {},
   "source": [
    "#### 9.2. Adding dropout<a class=\"anchor\" id=\"dropout\"></a>\n",
    "\n",
    "To prevent overfitting, dropout layers are inserted after each hidden layer in the IMDb dataset model. When a model works well on training data but is unable to generalise to fresh, untested data, this is known as overfitting. Randomly removing a different subset of neurons on each example would prevent conspiracies and thus reduce overfitting.Dropout layers assist the model in avoiding over-reliance on particular characteristics or patterns in the training data by arbitrarily removing a portion of the input units during the training process. In the end, this enhances the model's performance on test data that hasn't been seen and lowers the chance of overfitting by encouraging the model to acquire more reliable and generalizable representations of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eb222a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2a1a9bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "49/49 [==============================] - 2s 44ms/step - loss: 0.5897 - accuracy: 0.6858 - val_loss: 0.4493 - val_accuracy: 0.8618\n",
      "Epoch 2/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.4407 - accuracy: 0.8156 - val_loss: 0.3344 - val_accuracy: 0.8829\n",
      "Epoch 3/20\n",
      "49/49 [==============================] - 1s 17ms/step - loss: 0.3556 - accuracy: 0.8657 - val_loss: 0.2915 - val_accuracy: 0.8870\n",
      "Epoch 4/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.3002 - accuracy: 0.8940 - val_loss: 0.2916 - val_accuracy: 0.8832\n",
      "Epoch 5/20\n",
      "49/49 [==============================] - 1s 17ms/step - loss: 0.2593 - accuracy: 0.9129 - val_loss: 0.2761 - val_accuracy: 0.8886\n",
      "Epoch 6/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.2290 - accuracy: 0.9253 - val_loss: 0.2898 - val_accuracy: 0.8888\n",
      "Epoch 7/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.2131 - accuracy: 0.9294 - val_loss: 0.3008 - val_accuracy: 0.8875\n",
      "Epoch 8/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.1909 - accuracy: 0.9393 - val_loss: 0.3193 - val_accuracy: 0.8849\n",
      "Epoch 9/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.1793 - accuracy: 0.9431 - val_loss: 0.3422 - val_accuracy: 0.8822\n",
      "Epoch 10/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.1652 - accuracy: 0.9474 - val_loss: 0.3506 - val_accuracy: 0.8820\n",
      "Epoch 11/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.1513 - accuracy: 0.9523 - val_loss: 0.3931 - val_accuracy: 0.8825\n",
      "Epoch 12/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.1435 - accuracy: 0.9561 - val_loss: 0.4003 - val_accuracy: 0.8808\n",
      "Epoch 13/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.1370 - accuracy: 0.9586 - val_loss: 0.4446 - val_accuracy: 0.8766\n",
      "Epoch 14/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.1302 - accuracy: 0.9604 - val_loss: 0.4689 - val_accuracy: 0.8756\n",
      "Epoch 15/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.1201 - accuracy: 0.9639 - val_loss: 0.5076 - val_accuracy: 0.8765\n",
      "Epoch 16/20\n",
      "49/49 [==============================] - 1s 20ms/step - loss: 0.1166 - accuracy: 0.9630 - val_loss: 0.4916 - val_accuracy: 0.8758\n",
      "Epoch 17/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.1098 - accuracy: 0.9662 - val_loss: 0.5235 - val_accuracy: 0.8756\n",
      "Epoch 18/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.1085 - accuracy: 0.9664 - val_loss: 0.5457 - val_accuracy: 0.8731\n",
      "Epoch 19/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.1064 - accuracy: 0.9687 - val_loss: 0.5711 - val_accuracy: 0.8740\n",
      "Epoch 20/20\n",
      "49/49 [==============================] - 1s 17ms/step - loss: 0.1071 - accuracy: 0.9678 - val_loss: 0.5624 - val_accuracy: 0.8703\n",
      "Epoch 1/20\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 0.4460 - accuracy: 0.8236 - val_loss: 0.3393 - val_accuracy: 0.8795\n",
      "Epoch 2/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.2598 - accuracy: 0.9091 - val_loss: 0.2832 - val_accuracy: 0.8896\n",
      "Epoch 3/20\n",
      "49/49 [==============================] - 1s 17ms/step - loss: 0.1987 - accuracy: 0.9300 - val_loss: 0.2855 - val_accuracy: 0.8862\n",
      "Epoch 4/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.1678 - accuracy: 0.9406 - val_loss: 0.2930 - val_accuracy: 0.8838\n",
      "Epoch 5/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.1447 - accuracy: 0.9486 - val_loss: 0.3399 - val_accuracy: 0.8705\n",
      "Epoch 6/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.1233 - accuracy: 0.9571 - val_loss: 0.3413 - val_accuracy: 0.8747\n",
      "Epoch 7/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.1088 - accuracy: 0.9637 - val_loss: 0.3686 - val_accuracy: 0.8702\n",
      "Epoch 8/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0961 - accuracy: 0.9674 - val_loss: 0.4042 - val_accuracy: 0.8658\n",
      "Epoch 9/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0818 - accuracy: 0.9746 - val_loss: 0.4129 - val_accuracy: 0.8687\n",
      "Epoch 10/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0705 - accuracy: 0.9775 - val_loss: 0.4498 - val_accuracy: 0.8639\n",
      "Epoch 11/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.0622 - accuracy: 0.9810 - val_loss: 0.4787 - val_accuracy: 0.8618\n",
      "Epoch 12/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0493 - accuracy: 0.9860 - val_loss: 0.5204 - val_accuracy: 0.8577\n",
      "Epoch 13/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0424 - accuracy: 0.9884 - val_loss: 0.6078 - val_accuracy: 0.8494\n",
      "Epoch 14/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0354 - accuracy: 0.9896 - val_loss: 0.5851 - val_accuracy: 0.8560\n",
      "Epoch 15/20\n",
      "49/49 [==============================] - 1s 19ms/step - loss: 0.0290 - accuracy: 0.9921 - val_loss: 0.6290 - val_accuracy: 0.8546\n",
      "Epoch 16/20\n",
      "49/49 [==============================] - 1s 17ms/step - loss: 0.0252 - accuracy: 0.9930 - val_loss: 0.6553 - val_accuracy: 0.8544\n",
      "Epoch 17/20\n",
      "49/49 [==============================] - 1s 17ms/step - loss: 0.0189 - accuracy: 0.9952 - val_loss: 0.6938 - val_accuracy: 0.8530\n",
      "Epoch 18/20\n",
      "49/49 [==============================] - 1s 17ms/step - loss: 0.0150 - accuracy: 0.9967 - val_loss: 0.8265 - val_accuracy: 0.8420\n",
      "Epoch 19/20\n",
      "49/49 [==============================] - 1s 17ms/step - loss: 0.0141 - accuracy: 0.9966 - val_loss: 0.7851 - val_accuracy: 0.8515\n",
      "Epoch 20/20\n",
      "49/49 [==============================] - 1s 18ms/step - loss: 0.0099 - accuracy: 0.9981 - val_loss: 0.8143 - val_accuracy: 0.8486\n"
     ]
    }
   ],
   "source": [
    "#Adding dropout Model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=20, batch_size=512, validation_data=(x_test, y_test))\n",
    "\n",
    "validation_loss = history.history['val_loss']\n",
    "epochs = range(1, len(validation_loss) + 1)\n",
    "\n",
    "#Baseline model\n",
    "model1 = models.Sequential()\n",
    "model1.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model1.add(layers.Dense(16, activation='relu'))\n",
    "model1.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model1.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history1 = model1.fit(x_train, y_train, epochs=20, batch_size=512, validation_data=(x_test, y_test))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1fdc7a",
   "metadata": {},
   "source": [
    "The dropout rate is set to 0.5, meaning that half of the input units will be randomly dropped out during each training epoch. The baseline model without dropout layers is also trained for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8fe27385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, validation_loss1, 'r+', label='Model With dropout Validation Loss')\n",
    "plt.plot(epochs, validation_loss2, 'bo', label='Baseline Model Without dropout Validation Loss')\n",
    "plt.title('Validation Loss Comparison')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26abf95f",
   "metadata": {},
   "source": [
    "<u>Plot Observation -</u>\n",
    "\n",
    "this is a clear improvement over the refer- ence network.The red crosses represent the validation loss of the model with dropout layers, while the blue circles represent the validation loss of the baseline model without dropout layers. The plot helps visualize how the dropout layers affect the training process and the generalization capability of the model. Typically, we expect to see a reduction in validation loss when dropout layers are added, indicating improved performance and better generalization to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f883864b",
   "metadata": {},
   "source": [
    "### <u>10.Extensive Experimentation</u><a class=\"anchor\" id=\"Experimentation\"></a>\n",
    "\n",
    "By doing extensive experimentation we can improve the model and find out what implementations work.We can then use hyperparameter tuning to deploy a model thats beats the baseline model. I have a total of 6 experiments.\n",
    "\n",
    "Please note that i have listed these as original code as i did not refer to anything while building them, however this experimentation is to improve the baseline model(from Chollet). The way i am building the code is different.\n",
    "\n",
    "#### 10.1. Experiment with one and three hidden layers.<a class=\"anchor\" id=\"1_3\"></a>\n",
    "\n",
    "The objective is to investigate the performance of neural networks with one and three hidden layers on the IMDb dataset.By varying the number of hidden layers, we aim to understand how increasing the depth of the network affects its ability to learn and generalize from the data.The models are evaluated on the test set to assess their performance.\n",
    "\n",
    "When comparing the effects of using one hidden layer versus three hidden layers on validation and test accuracy, several factors can come into play. However, it's important to note that the impact can vary depending on the specific dataset and problem at hand. Here are some general justifications for how the number of hidden layers can affect model performance:\n",
    "\n",
    "One Hidden Layer:\n",
    "\n",
    "- Simplicity: Using only one hidden layer simplifies the model's architecture, making it easier to understand and interpret. With a simpler model, there is a lower risk of overfitting, especially when dealing with smaller datasets.\n",
    "\n",
    "- Generalization: One hidden layer can be sufficient for capturing linear relationships and simpler patterns in the data. If the dataset exhibits a linearly separable structure, a single hidden layer may be able to learn and generalize well, resulting in good validation and test accuracy.\n",
    "\n",
    "- Reduced Complexity: Fewer hidden layers mean fewer parameters to learn and optimize during training. This can result in faster training times and less computational overhead.\n",
    "\n",
    "Three Hidden Layers:\n",
    "\n",
    "- Increased Capacity: Additional hidden layers provide the neural network with more capacity to learn complex representations of the data. Deep architectures can capture hierarchical features and abstract patterns, potentially improving the model's ability to handle intricate relationships within the dataset.\n",
    "\n",
    "- Representation Learning: Each hidden layer can learn and extract different levels of features and abstractions. As information passes through multiple layers, the network can progressively learn more meaningful representations of the data. This can be advantageous when dealing with complex datasets that require multiple layers of abstraction.\n",
    "\n",
    "- Potential for Overfitting: The deeper the network, the higher the risk of overfitting, particularly when the dataset is small or noisy. Additional hidden layers introduce more parameters to learn, increasing the model's capacity to memorize training samples. Regularization techniques like dropout or weight decay can help mitigate this issue.\n",
    "\n",
    "Code Explanation -\n",
    "\n",
    "- We define a list containing the number of hidden layers to experiment with (one and three).\n",
    "\n",
    "- For each number of hidden layers in the list, a neural network model is built using Sequential API in TensorFlow.\n",
    "\n",
    "- The model loops through each number of hidden layers\n",
    "\n",
    "- The model architecture includes an embedding layer, followed by one or three dense hidden layers with 16 units and ReLU activation functions, and an output layer with one unit and sigmoid activation.\n",
    "\n",
    "- The models are compiled with the rmsprop optimizer, binary cross-entropy loss function, and accuracy as the metric.\n",
    "\n",
    "- Each model is trained for 5 epochs with a batch size of 512 and a validation split of 0.2.\n",
    "\n",
    "- After training, each model is evaluated on the test set, and the test accuracy is printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "979ed0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "40/40 [==============================] - 14s 348ms/step - loss: 0.8833 - accuracy: 0.4996 - val_loss: 0.6932 - val_accuracy: 0.4938\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 15s 369ms/step - loss: 0.6932 - accuracy: 0.4970 - val_loss: 0.6932 - val_accuracy: 0.4938\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 14s 361ms/step - loss: 0.6932 - accuracy: 0.4970 - val_loss: 0.6932 - val_accuracy: 0.4938\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 14s 355ms/step - loss: 0.6932 - accuracy: 0.5016 - val_loss: 0.6932 - val_accuracy: 0.4938\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 14s 341ms/step - loss: 0.6932 - accuracy: 0.5016 - val_loss: 0.6932 - val_accuracy: 0.4938\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6932 - accuracy: 0.5000\n",
      "Configuration: Hidden Layers=1\n",
      "Test Accuracy: 0.5\n",
      "Epoch 1/5\n",
      "40/40 [==============================] - 17s 400ms/step - loss: 0.7425 - accuracy: 0.5084 - val_loss: 0.7617 - val_accuracy: 0.4938\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 14s 356ms/step - loss: 0.6711 - accuracy: 0.6421 - val_loss: 0.6209 - val_accuracy: 0.7038\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 15s 375ms/step - loss: 0.5398 - accuracy: 0.7568 - val_loss: 0.4514 - val_accuracy: 0.8212\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 18s 438ms/step - loss: 0.3865 - accuracy: 0.8461 - val_loss: 0.3620 - val_accuracy: 0.8490\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 18s 446ms/step - loss: 0.2836 - accuracy: 0.8908 - val_loss: 0.2889 - val_accuracy: 0.8824\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.3012 - accuracy: 0.8760\n",
      "Configuration: Hidden Layers=3\n",
      "Test Accuracy: 0.876039981842041\n"
     ]
    }
   ],
   "source": [
    "#Original Code\n",
    "hidden_layers_list = [1, 3]\n",
    "\n",
    "vocabulary_size = 10000 \n",
    "\n",
    "max_sequence_length = x_train.shape[1]\n",
    "\n",
    "for num_layers in hidden_layers_list:\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, 16, input_length=max_sequence_length))\n",
    "    model.add(Flatten())\n",
    "    for _ in range(num_layers):\n",
    "        model.add(Dense(16, activation='relu')) \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, train_labels, epochs=5, batch_size=512, validation_split=0.2)\n",
    "\n",
    "    loss, accuracy = model.evaluate(x_test, test_labels)\n",
    "    print(f\"Configuration: Hidden Layers={num_layers}\")\n",
    "    print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf01da74",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "- By comparing the test accuracies obtained for models with different numbers of hidden layers, we can observe how the depth of the neural network affects its performance.\n",
    "\n",
    "- Increasing the number of hidden layers (from one to three) allows the model to learn more complex representations of the data, potentially leading to better performance.\n",
    "\n",
    "- However, adding more layers also increases the risk of overfitting, especially if the dataset is not sufficiently large or if regularization techniques are not applied.\n",
    "\n",
    "- Observing the test accuracies helps in understanding the trade-offs between model complexity and performance on the IMDb dataset.\n",
    "\n",
    "This experiment provides insights into how the number of hidden layers affects the performance of neural networks on the IMDb dataset. It helps in understanding the impact of network depth on the model's ability to learn and generalize from the data. Adjustments can be made based on observed results to optimize the model architecture for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c654cca",
   "metadata": {},
   "source": [
    "#### 10.2. Experiment with more or fewer hidden units - 32 units, 64 units etc. <a class=\"anchor\" id=\"32_64\"></a>\n",
    "\n",
    "Experimenting with different numbers of hidden units involves varying the size of the hidden layers in the neural network. This can significantly impact the model's capacity to learn complex patterns from the data.\n",
    "\n",
    "The experiments objective is to observe how the number of hidden units in the hidden layers affects the performance of the neural network on the IMDb dataset.By altering the number of hidden units, we can control the model's complexity. Fewer units might lead to underfitting, while more units might lead to overfitting.\n",
    "\n",
    "- We define a list of different numbers of hidden units to experiment with.\n",
    "\n",
    "- For each number of hidden units in the list, we build a neural network model using Sequential API in TensorFlow.\n",
    "\n",
    "- The model loops through each number of hidden units.\n",
    "\n",
    "- The model architecture includes an embedding layer, a flatten layer, a dense layer with the specified number of units, and an output layer with one unit and sigmoid activation function.\n",
    "\n",
    "- We compile the model with the rmsprop optimizer, binary cross-entropy loss function, and accuracy as the metric.\n",
    "\n",
    "- The model is trained for 5 epochs with a batch size of 512 and a validation split of 0.2.\n",
    "\n",
    "- After training, the model is evaluated on the test data, and the test accuracy is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "d0dabdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "40/40 [==============================] - 18s 440ms/step - loss: 1.0743 - accuracy: 0.5063 - val_loss: 0.6938 - val_accuracy: 0.4938\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 17s 428ms/step - loss: 0.6968 - accuracy: 0.4984 - val_loss: 0.6932 - val_accuracy: 0.4938\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 17s 433ms/step - loss: 0.6932 - accuracy: 0.5016 - val_loss: 0.6932 - val_accuracy: 0.4938\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 17s 426ms/step - loss: 0.6931 - accuracy: 0.5016 - val_loss: 0.6932 - val_accuracy: 0.4938\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 17s 437ms/step - loss: 0.6932 - accuracy: 0.5016 - val_loss: 0.6932 - val_accuracy: 0.4938\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.6932 - accuracy: 0.5000\n",
      "Configuration: Hidden Units=32\n",
      "Test Accuracy: 0.5\n",
      "Epoch 1/5\n",
      "40/40 [==============================] - 27s 654ms/step - loss: 1.2326 - accuracy: 0.5055 - val_loss: 0.6932 - val_accuracy: 0.4938\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 24s 612ms/step - loss: 0.7289 - accuracy: 0.5005 - val_loss: 0.6932 - val_accuracy: 0.4938\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 25s 627ms/step - loss: 0.6984 - accuracy: 0.5020 - val_loss: 0.6932 - val_accuracy: 0.4938\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 25s 636ms/step - loss: 0.7393 - accuracy: 0.5564 - val_loss: 0.8548 - val_accuracy: 0.4938\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 25s 616ms/step - loss: 0.6867 - accuracy: 0.6226 - val_loss: 0.5185 - val_accuracy: 0.7492\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.5312 - accuracy: 0.7307\n",
      "Configuration: Hidden Units=64\n",
      "Test Accuracy: 0.7307199835777283\n",
      "Epoch 1/5\n",
      "40/40 [==============================] - 39s 973ms/step - loss: 1.7222 - accuracy: 0.4999 - val_loss: 0.7255 - val_accuracy: 0.5062\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 37s 935ms/step - loss: 0.6838 - accuracy: 0.6100 - val_loss: 0.5652 - val_accuracy: 0.7340\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 39s 964ms/step - loss: 0.5997 - accuracy: 0.7729 - val_loss: 0.4996 - val_accuracy: 0.7334\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 39s 971ms/step - loss: 0.3478 - accuracy: 0.8634 - val_loss: 0.2976 - val_accuracy: 0.8844\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 40s 1s/step - loss: 0.2643 - accuracy: 0.8961 - val_loss: 0.3322 - val_accuracy: 0.8588\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.3521 - accuracy: 0.8463\n",
      "Configuration: Hidden Units=128\n",
      "Test Accuracy: 0.8463199734687805\n",
      "Epoch 1/5\n",
      "40/40 [==============================] - 77s 2s/step - loss: 4.2270 - accuracy: 0.4981 - val_loss: 0.8215 - val_accuracy: 0.5062\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 76s 2s/step - loss: 0.5967 - accuracy: 0.6625 - val_loss: 0.4634 - val_accuracy: 0.7892\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 75s 2s/step - loss: 0.4653 - accuracy: 0.8221 - val_loss: 0.3220 - val_accuracy: 0.8744\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 76s 2s/step - loss: 0.3314 - accuracy: 0.8737 - val_loss: 0.2835 - val_accuracy: 0.8878\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 73s 2s/step - loss: 0.2576 - accuracy: 0.8971 - val_loss: 0.2849 - val_accuracy: 0.8872\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.2877 - accuracy: 0.8862\n",
      "Configuration: Hidden Units=256\n",
      "Test Accuracy: 0.8862400054931641\n"
     ]
    }
   ],
   "source": [
    "#Original Code \n",
    "hidden_units_list = [32, 64, 128, 256]\n",
    "\n",
    "max_sequence_length = x_train.shape[1]  \n",
    "\n",
    "for num_units in hidden_units_list:\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(10000, 16, input_length=max_sequence_length))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_units, activation='relu'))  \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, train_labels, epochs=5, batch_size=512, validation_split=0.2)\n",
    "\n",
    "    loss, accuracy = model.evaluate(x_test, test_labels)\n",
    "    print(f\"Configuration: Hidden Units={num_units}\")\n",
    "    print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee73ef",
   "metadata": {},
   "source": [
    "Observations -\n",
    "\n",
    "- By observing the test accuracies obtained for different numbers of hidden units, we can determine the impact of hidden unit size on the model's performance.\n",
    "\n",
    "- Generally, as the number of hidden units increases, the model's capacity to learn intricate patterns from the data also increases. However, this may lead to overfitting if the model becomes too complex.\n",
    "\n",
    "- Conversely, too few hidden units may result in the model being unable to capture the underlying patterns in the data, leading to underfitting.\n",
    "\n",
    "- By analyzing the test accuracies across different numbers of hidden units, we can identify the optimal balance between model complexity and performance on the IMDb dataset.\n",
    "\n",
    "This experiment helps in understanding how the size of the hidden layers influences the neural network's ability to learn from the data and make accurate predictions. Adjustments can be made based on the observed results to optimize the model architecture for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5367f0b9",
   "metadata": {},
   "source": [
    "#### 10.3. Investigate replacing the binary_crossentropy loss function with mse.<a class=\"anchor\" id=\"mse\"></a>\n",
    "\n",
    "The experiments objective is to observe how replacing the binary_crossentropy loss function with the MSE loss function affects the performance of the neural network on the IMDb dataset.Binary_crossentropy loss is commonly used for binary classification tasks, while MSE loss is often used for regression tasks. However, it's interesting to see how the model performs with different loss functions.\n",
    "\n",
    "- Two neural network models are built, one with the MSE loss function and the other with the binary_crossentropy loss function.\n",
    "\n",
    "- Both models have the same architecture, consisting of an embedding layer, a flatten layer, a dense layer with 16 units and ReLU activation, and an output layer with one unit and sigmoid activation.\n",
    "\n",
    "- The models are compiled with their respective loss functions, using the adam optimizer and accuracy as the metric.\n",
    "\n",
    "- Each model is trained for 5 epochs with a batch size of 512 and a validation split of 0.2. So that it can be trained properly\n",
    "\n",
    "- After training, each model is evaluated on the test data, and their test accuracies are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2ba09a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 84s 134ms/step - loss: 0.4996 - accuracy: 0.5013 - val_loss: 0.5000 - val_accuracy: 0.4938\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 94s 151ms/step - loss: 0.5000 - accuracy: 0.5016 - val_loss: 0.5000 - val_accuracy: 0.4938\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 92s 147ms/step - loss: 0.5000 - accuracy: 0.5016 - val_loss: 0.5000 - val_accuracy: 0.4938\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 93s 149ms/step - loss: 0.5000 - accuracy: 0.5016 - val_loss: 0.5000 - val_accuracy: 0.4938\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 98s 157ms/step - loss: 0.5000 - accuracy: 0.5016 - val_loss: 0.5000 - val_accuracy: 0.4938\n",
      "782/782 [==============================] - 38s 48ms/step - loss: 0.5000 - accuracy: 0.5000\n",
      "MSE model test accuracy: 0.5\n",
      "Epoch 1/5\n",
      "625/625 [==============================] - 86s 137ms/step - loss: 0.6891 - accuracy: 0.8268 - val_loss: 0.2926 - val_accuracy: 0.8848\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 86s 137ms/step - loss: 0.2338 - accuracy: 0.9076 - val_loss: 0.3145 - val_accuracy: 0.8822\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 90s 145ms/step - loss: 0.1954 - accuracy: 0.9291 - val_loss: 0.3110 - val_accuracy: 0.8890\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 89s 142ms/step - loss: 0.1724 - accuracy: 0.9356 - val_loss: 0.3224 - val_accuracy: 0.8872\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 87s 139ms/step - loss: 0.1545 - accuracy: 0.9438 - val_loss: 0.3580 - val_accuracy: 0.8794\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.3753 - accuracy: 0.8733\n",
      "Binary Crossentropy model test accuracy: 0.8732799887657166\n"
     ]
    }
   ],
   "source": [
    "#Original Code \n",
    "max_len = len(x_train[0])\n",
    "\n",
    "model_mse = Sequential()\n",
    "model_mse.add(Embedding(max_features, 128, input_length=max_len))\n",
    "model_mse.add(Flatten())\n",
    "model_mse.add(Dense(2, activation='softmax')) \n",
    "\n",
    "model_mse.compile(optimizer='rmsprop',\n",
    "                   loss='mse', \n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "history_mse = model_mse.fit(x_train, y_train,\n",
    "                             epochs=5,\n",
    "                             batch_size=32,\n",
    "                             validation_split=0.2)\n",
    "\n",
    "test_loss_mse, test_acc_mse = model_mse.evaluate(x_test, y_test)\n",
    "print('MSE model test accuracy:', test_acc_mse)\n",
    "\n",
    "model_binary_crossentropy = Sequential()\n",
    "model_binary_crossentropy.add(Embedding(max_features, 128, input_length=max_len))\n",
    "model_binary_crossentropy.add(Flatten())\n",
    "model_binary_crossentropy.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_binary_crossentropy.compile(optimizer='rmsprop',\n",
    "                                   loss='binary_crossentropy',  \n",
    "                                   metrics=['accuracy'])\n",
    "\n",
    "history_binary_crossentropy = model_binary_crossentropy.fit(x_train, y_train,\n",
    "                                                              epochs=5,\n",
    "                                                              batch_size=32,\n",
    "                                                              validation_split=0.2)\n",
    "\n",
    "test_loss_binary_crossentropy, test_acc_binary_crossentropy = model_binary_crossentropy.evaluate(x_test, y_test)\n",
    "print('Binary Crossentropy model test accuracy:', test_acc_binary_crossentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f335b1d9",
   "metadata": {},
   "source": [
    "Observations -\n",
    "\n",
    "- By comparing the test accuracies obtained for the two models (one with MSE loss and the other with binary_crossentropy loss), we can observe how the choice of loss function affects the model's performance.\n",
    "\n",
    "- Binary_crossentropy loss is well-suited for binary classification tasks, while MSE loss is typically used for regression tasks. However, in this experiment, we observe how the models perform with different loss functions for a binary classification task.\n",
    "\n",
    "- It's essential to consider not only the test accuracy but also other factors such as convergence speed and potential overfitting when evaluating the impact of different loss functions.\n",
    "\n",
    "This experiment helps in understanding the influence of the choice of loss function on the model's training and performance. Based on the observed results, appropriate loss functions can be selected to optimize the model for the specific task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38330379",
   "metadata": {},
   "source": [
    "#### 10.4. Experiment with replacing relu with tanh activations.<a class=\"anchor\" id=\"tanh\"></a>\n",
    "\n",
    "Experimenting with replacing Rectified Linear Unit (ReLU) activation with Hyperbolic Tangent (tanh) activation is an essential exploration in deep learning. The activation function plays a crucial role in introducing non-linearity to the neural network, affecting its ability to learn complex patterns from the data. \n",
    "\n",
    "The objective is to investigate how replacing ReLU activation with tanh activation affects the performance of the neural network on the IMDb dataset.\n",
    "\n",
    "- ReLU is a widely-used activation function that has become a default choice in many deep learning architectures due to its simplicity and effectiveness.\n",
    "\n",
    "- Tanh activation function ranges from -1 to 1, offering a different non-linear transformation compared to ReLU.\n",
    "\n",
    "- A neural network model is built using Sequential API with tanh activation function in the hidden layer instead of ReLU.\n",
    "\n",
    "- The model architecture includes an embedding layer, a flatten layer, a dense layer with 16 units and tanh activation, and an output layer with one unit and sigmoid activation.\n",
    "\n",
    "- The model is compiled with 'rmsprop' optimizer, binary cross-entropy loss function, and accuracy as the metric.\n",
    "\n",
    "- The model is trained for 5 epochs with a batch size of 512 and a validation split of 0.2.\n",
    "\n",
    "- After training, the model is evaluated on the test data, and the test accuracy is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "10315894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "40/40 [==============================] - 1s 22ms/step - loss: 0.4710 - accuracy: 0.8221 - val_loss: 0.3461 - val_accuracy: 0.8784\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 1s 14ms/step - loss: 0.2681 - accuracy: 0.9086 - val_loss: 0.2937 - val_accuracy: 0.8816\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1934 - accuracy: 0.9316 - val_loss: 0.2881 - val_accuracy: 0.8816\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1511 - accuracy: 0.9475 - val_loss: 0.2855 - val_accuracy: 0.8900\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1242 - accuracy: 0.9575 - val_loss: 0.3118 - val_accuracy: 0.8854\n",
      "782/782 [==============================] - 1s 989us/step - loss: 0.3383 - accuracy: 0.8774\n",
      "Model with tanh Activation:\n",
      "Test Accuracy: 0.8774399757385254\n"
     ]
    }
   ],
   "source": [
    "#Original Code\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='tanh', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='tanh'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, train_labels, epochs=5, batch_size=512, validation_split=0.2)\n",
    "\n",
    "loss_tanh, accuracy_tanh = model.evaluate(x_test, test_labels)\n",
    "print(\"Model with tanh Activation:\")\n",
    "print(f\"Test Accuracy: {accuracy_tanh}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4f70d8",
   "metadata": {},
   "source": [
    "Observations -\n",
    "\n",
    "- By comparing the test accuracy obtained with tanh activation to the accuracy obtained with ReLU activation, we can observe how the choice of activation function affects the model's performance.\n",
    "\n",
    "- Tanh activation function squashes the output to the range [-1, 1], which may introduce different characteristics to the model compared to ReLU. While ReLU outputs values in the range [0, +). This centered output range of tanh might be desirable for certain tasks or architectures, especially those that require outputs to be within a specific range.\n",
    "\n",
    "- Observing the test accuracy helps in understanding the impact of using tanh activation on the model's ability to learn and generalize from the data.\n",
    "\n",
    "This experiment provides insights into how the choice of activation function influences the neural network's performance on the IMDb dataset. It helps in understanding the behavior of different activation functions and their suitability for specific tasks. Adjustments can be made based on observed results to optimize the model architecture for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ae79cc",
   "metadata": {},
   "source": [
    "#### 10.5. Investigate the effect of different learning rates. Optimal configuration<a class=\"anchor\" id=\"lr\"></a>\n",
    "\n",
    "Investigating the effect of different learning rates is crucial in deep learning as it directly impacts the optimization process during training. A learning rate that is too high might lead to overshooting the minima, while a learning rate that is too low might result in slow convergence or getting stuck in local minima. \n",
    "\n",
    "This is also part of regularizing the model and tuning the hyperparameters by trying different hyperparameters (such as the number of units per layer or the learning rate of the optimizer) to find the optimal configuration.\n",
    "\n",
    "The objective is to observe how different learning rates affect the performance of the neural network on the IMDb dataset. Learning rate controls the step size during the optimization process (e.g., gradient descent), influencing how quickly or slowly the model parameters are updated.\n",
    "\n",
    "- A list of different learning rates is defined to experiment with.\n",
    "\n",
    "- For each learning rate in the list, a neural network model is built using Sequential API with ReLU activation in the hidden layer.\n",
    "\n",
    "- The model loops through each learning rate\n",
    "\n",
    "- The model architecture includes an embedding layer, a flatten layer, a dense layer with 16 units and ReLU activation, and an output layer with one unit and sigmoid activation.\n",
    "\n",
    "- The model is compiled with the rmsprop optimizer with the specified learning rate, binary cross-entropy loss function, and accuracy as the metric.\n",
    "\n",
    "- The model is trained for 5 epochs with a batch size of 512 and a validation split of 0.2 for each learning rate.\n",
    "\n",
    "- After training, the model is evaluated on the test data, and the test accuracy is printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cb7fc1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikhitha/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 3s 63ms/step - loss: 0.6844 - accuracy: 0.5564 - val_loss: 0.6444 - val_accuracy: 0.7252\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 2s 61ms/step - loss: 0.5373 - accuracy: 0.7800 - val_loss: 0.4322 - val_accuracy: 0.8244\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 2s 59ms/step - loss: 0.3276 - accuracy: 0.8809 - val_loss: 0.3211 - val_accuracy: 0.8716\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 2s 59ms/step - loss: 0.2288 - accuracy: 0.9188 - val_loss: 0.2907 - val_accuracy: 0.8802\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 2s 59ms/step - loss: 0.1747 - accuracy: 0.9403 - val_loss: 0.2794 - val_accuracy: 0.8884\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2822 - accuracy: 0.8826\n",
      "Configuration: Learning Rate=0.001\n",
      "Test Accuracy: 0.8825600147247314\n",
      "Epoch 1/5\n",
      "40/40 [==============================] - 3s 63ms/step - loss: 0.9853 - accuracy: 0.5475 - val_loss: 0.6604 - val_accuracy: 0.6980\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 2s 59ms/step - loss: 0.4880 - accuracy: 0.8407 - val_loss: 0.4395 - val_accuracy: 0.8674\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 2s 62ms/step - loss: 0.3302 - accuracy: 0.9326 - val_loss: 0.4200 - val_accuracy: 0.8658\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 2s 60ms/step - loss: 0.2356 - accuracy: 0.9624 - val_loss: 0.4429 - val_accuracy: 0.8670\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 3s 63ms/step - loss: 0.1758 - accuracy: 0.9768 - val_loss: 0.3718 - val_accuracy: 0.8786\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3918 - accuracy: 0.8714\n",
      "Configuration: Learning Rate=0.01\n",
      "Test Accuracy: 0.871399998664856\n",
      "Epoch 1/5\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 11.1210 - accuracy: 0.5016 - val_loss: 0.6932 - val_accuracy: 0.4938\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.6933 - accuracy: 0.5016 - val_loss: 0.6933 - val_accuracy: 0.4938\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 0.6933 - accuracy: 0.4954 - val_loss: 0.6934 - val_accuracy: 0.4938\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 0.6932 - accuracy: 0.5023 - val_loss: 0.6931 - val_accuracy: 0.5062\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 3s 64ms/step - loss: 0.6933 - accuracy: 0.4976 - val_loss: 0.6932 - val_accuracy: 0.4938\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.5000\n",
      "Configuration: Learning Rate=0.1\n",
      "Test Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "#Original Code\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(10000, 16, input_length=max_len))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    optimizer = RMSprop(lr=lr)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, train_labels, epochs=5, batch_size=512, validation_split=0.2)\n",
    "\n",
    "    loss, accuracy = model.evaluate(x_test, test_labels)\n",
    "    print(f\"Configuration: Learning Rate={lr}\")\n",
    "    print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46a169c",
   "metadata": {},
   "source": [
    "Observations -\n",
    "\n",
    "- By observing the test accuracy obtained for different learning rates, we can understand how the choice of learning rate affects the model's performance.\n",
    "\n",
    "- A learning rate that is too high might lead to overshooting the minima and unstable training, while a learning rate that is too low might result in slow convergence or getting stuck in local minima.\n",
    "\n",
    "- Experimenting with different learning rates helps in finding the optimal learning rate that results in faster convergence and better generalization on the IMDb dataset.\n",
    "\n",
    "This experiment provides insights into how different learning rates influence the training and performance of the neural network. It helps in understanding the impact of the learning rate on the optimization process and guides in selecting an appropriate learning rate for training deep learning models. Adjustments can be made based on observed results to optimize the learning process and improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9701f4",
   "metadata": {},
   "source": [
    "#### 10.6. Take your best network and train on all the training data for the optimal epochs. Evaluate on the test set.<a class=\"anchor\" id=\"best\"></a>\n",
    "\n",
    "In this experiment, we take the best-performing network obtained from the previous experiments and train it on all the available training data for the optimal number of epochs. The purpose is to utilize all available data for training and to determine the model's performance when trained with the entire dataset. \n",
    "\n",
    "- The objective is to train the best-performing network obtained from the previous experiments on all available training data for the optimal number of epochs.\n",
    "\n",
    "- By training on the entire dataset, we aim to utilize all available information for training and potentially improve the model's performance.\n",
    "\n",
    "- The model is evaluated on the test set to assess its generalization ability.\n",
    "\n",
    "- The best model obtained from the previous experiments is defined using the architecture that yielded the highest performance.\n",
    "\n",
    "- The model is compiled with the rmsprop optimizer, binary cross-entropy loss function, and accuracy as the metric.\n",
    "\n",
    "- The model is trained on all available training data for the optimal number of epochs determined from the previous experiments (in this case, 5 epochs).\n",
    "\n",
    "- After training, the model is evaluated on the test data to assess its performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1029c994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "49/49 [==============================] - 3s 63ms/step - loss: 0.6919 - accuracy: 0.5632\n",
      "Epoch 2/5\n",
      "49/49 [==============================] - 3s 65ms/step - loss: 0.4547 - accuracy: 0.8284\n",
      "Epoch 3/5\n",
      "49/49 [==============================] - 3s 65ms/step - loss: 0.2610 - accuracy: 0.9008\n",
      "Epoch 4/5\n",
      "49/49 [==============================] - 3s 70ms/step - loss: 0.1924 - accuracy: 0.9296\n",
      "Epoch 5/5\n",
      "49/49 [==============================] - 3s 66ms/step - loss: 0.1502 - accuracy: 0.9488\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.2803 - accuracy: 0.8856\n",
      "Best Model Evaluation on Test Set:\n",
      "Test Accuracy: 0.8855999708175659\n"
     ]
    }
   ],
   "source": [
    "#Original Code\n",
    "best_model = Sequential()\n",
    "best_model.add(Embedding(10000, 16, input_length=max_len))\n",
    "best_model.add(Flatten())\n",
    "best_model.add(Dense(16, activation='relu'))\n",
    "best_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "best_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "best_model.fit(x_train, train_labels, epochs=5, batch_size=512)\n",
    "\n",
    "loss, accuracy = best_model.evaluate(x_test, test_labels)\n",
    "print(\"Best Model Evaluation on Test Set:\")\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be43097",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "- By training the best model on all available training data, we aim to leverage all available information for training and potentially improve the model's performance.\n",
    "\n",
    "- Evaluating the model on the test set provides insights into its generalization ability and how well it performs on unseen data.\n",
    "\n",
    "- Comparing the test accuracy obtained with the best model trained on the entire dataset to the accuracies obtained from previous experiments helps in assessing the effectiveness of utilizing all available data for training.\n",
    "\n",
    "This experiment allows us to determine the performance of the best model when trained on the entire dataset for the optimal number of epochs. It provides insights into how well the model generalizes to unseen data and helps in assessing the effectiveness of training on all available data. Adjustments can be made based on observed results to further optimize the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5c4286",
   "metadata": {},
   "source": [
    "### <u>11. Developing a model that does better than a baseline</u><a class=\"anchor\" id=\"Developing_model\"></a>\n",
    "\n",
    "After testing and tuning with parameters and applying the experiments that i had done above i had created a model that shows higher accuracy perfectly without overfitting. \n",
    "\n",
    "Since it was difficult for me to find out exactly which combination of parameters would give me the best accuracy, I created a grid of units,dropout_rate and learning_rate examples and performed a gridsearch with cross validation using GridSearchCV to get the best parameters. It also shows the best corresponding accuracy that we can get using these hyperparater combinations.\n",
    "\n",
    "How it works is that a grid of hyperparameters is defined using the param_grid dictionary. It includes different combinations of units, dropout rates, and learning rates to be explored during the grid search. The fit() method of the GridSearchCV function is called with the training data to perform the grid search with cross-validation. It searches through the specified hyperparameter grid (param_grid) to find the combination with the best performance on the training data. After the search is complete, it prints the best hyperparameters and their corresponding accuracy obtained from the grid search. \n",
    "\n",
    "When creating the model i took the help of my experiments which showed my the best learning rate, hidden units and layers and tanh activations but after getting several combinations wrong i decided to make the gridsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "60cbaff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1x/lcz6lbkj70x5p6_2vnx0nh3h0000gn/T/ipykernel_12864/4012721411.py:25: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=create_model, epochs=3, batch_size=32, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'dropout_rate': 0.6, 'learning_rate': 0.0001, 'units': 256}\n",
      "Best Accuracy:  0.8938000202178955\n"
     ]
    }
   ],
   "source": [
    "#Original Code\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "def create_model(units=256, dropout_rate=0.5, learning_rate=0.0001):\n",
    "    model = Sequential([\n",
    "        Dense(units=units, activation='relu', input_shape=(max_features,)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "param_grid = {\n",
    "    'units': [128, 256, 512],\n",
    "    'dropout_rate': [0.4, 0.5, 0.6],\n",
    "    'learning_rate': [0.0001, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "model = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=create_model, epochs=3, batch_size=32, verbose=0)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(x_train_reshaped, y_train)\n",
    "\n",
    "print(\"Best Parameters: \", grid_result.best_params_)\n",
    "print(\"Best Accuracy: \", grid_result.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4073aefb",
   "metadata": {},
   "source": [
    "<u>Process of developing the new model -</u>\n",
    "\n",
    "After tuning the hyperparameters units and getting the best parameters for units = 256, dropout_rate = 0.6, and learning_rate = 0.0001. The input data is reshaped to fit the model's input shape. The model architecture consists of a dense layer with ReLU activation, followed by a dropout layer to prevent overfitting, and finally, an output layer with sigmoid activation for binary classification.\n",
    "\n",
    "The model is compiled with the Adam optimizer and binary crossentropy loss function. It is then trained on the training data (x_train_reshaped, y_train) for 3 epochs with a batch size of 32. Additionally, 20% of the training data is used as validation data.\n",
    "\n",
    "After training, the model's performance is evaluated on the test data (x_test_reshaped, y_test), and the test accuracy is printed.\n",
    "\n",
    "Three key choices to build the working model -\n",
    "\n",
    "- The Last-layer activation establishes useful constraints on the networks output. The IMDB classification  used sigmoid in the last layer, the regression example didnt use any last-layer activation and so on.\n",
    "\n",
    "- The Loss function matches the type of problem we are trying to solve. We used binary_crossentropy, the regression example used mse, and so on.\n",
    "\n",
    "- For Optimization configuration is did something differnet. Instead of going with rmsprop and its default learning rate, i went with adam optimizer which helped get the best output for the model i built.\n",
    "\n",
    "<u>For this model i have used -</u>\n",
    "\n",
    "- Hyperparameter tuning\n",
    "- Reference to my extensive experimentations(like knowing what values to use)\n",
    "- Used best learning rate from grid search and experiment\n",
    "- Used best units from grid search and experiment\n",
    "- Used best dropout_rate from grid search \n",
    "- Knowing to use the relu function instead of tanh due to experiments\n",
    "- Used binary cross entropy with sigmoid\n",
    "- Used Adam optimizer\n",
    "- Developing a small model with most statistical power\n",
    "- 3 epochs to prevent overfitting\n",
    "- A batch size of 32\n",
    "- Reshaped training data\n",
    "- Last-layer Activation: Sigmoid (since it's a binary classification problem)\n",
    "- Loss Function: Binary crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "83869eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.4731 - accuracy: 0.8019 - val_loss: 0.3280 - val_accuracy: 0.8782\n",
      "Epoch 2/3\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2839 - accuracy: 0.8933 - val_loss: 0.2814 - val_accuracy: 0.8880\n",
      "Epoch 3/3\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2256 - accuracy: 0.9170 - val_loss: 0.2678 - val_accuracy: 0.8938\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.2802 - accuracy: 0.8899\n",
      "Test Accuracy: 0.8899199962615967\n"
     ]
    }
   ],
   "source": [
    "#Original Code\n",
    "units = 256\n",
    "dropout_rate = 0.6\n",
    "learning_rate = 0.0001\n",
    "\n",
    "x_train_reshaped = np.reshape(x_train, (x_train.shape[0], -1))\n",
    "x_test_reshaped = np.reshape(x_test, (x_test.shape[0], -1))\n",
    "\n",
    "max_features = x_train_reshaped.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(units=units, activation='relu', input_shape=(max_features,)),\n",
    "    Dropout(dropout_rate),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train_reshaped, y_train, epochs= 3, batch_size= 32, validation_split=0.2)\n",
    "\n",
    "loss, accuracy = model.evaluate(x_test_reshaped, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facfe0e2",
   "metadata": {},
   "source": [
    "<u>Understanding the developed model and its output -</u>\n",
    "\n",
    "The model was trained for 3 epochs. During training, both training and validation accuracies steadily increased, indicating that the model was learning the features of the training data and generalizing well to unseen validation data. The training and validation losses also decreased over epochs, indicating the model's improvement in minimizing the loss function.\n",
    "\n",
    "After training, the model achieved a test accuracy of approximately 88.99%. This accuracy represents how well the model generalized to unseen test data. It indicates that the model can correctly classify sentiments in movie reviews with a high level of accuracy.\n",
    "\n",
    "<u>Now that we have obtained a model that has statistical power, the question becomes, is your model sufficiently powerful? </u>\n",
    "\n",
    "Yes, with a test accuracy of approximately 88.99%, the model seems to have achieved a satisfactory level of performance. The developed model has the best accuracy rate. For future further developemnt there is a possibility the statistical power being improved, by trying more architecture modifications, training modifications and feature enginnering. \n",
    "\n",
    "<u>Does it have enough layers and parameters to properly model the problem at hand?</u> \n",
    "\n",
    "Yes, it have enough layers and parameters to properly model the problem at hand as -\n",
    "\n",
    "- The model achieves increasing training accuracy over the epochs, indicating that it's learning from the training data. The final training accuracy is around 91.70%.\n",
    "\n",
    "- Validation accuracy also increases over epochs, reaching approximately 89.38% in the final epoch.\n",
    "\n",
    "- The test accuracy is 88.99%, which is close to the validation accuracy. This suggests that the model generalizes well to unseen data.\n",
    "\n",
    "Given the consistent improvement in accuracy over epochs and the high accuracy achieved on both the validation and test datasets, it appears that the model architecture with the provided number of layers and parameters is sufficiently powerful for the IMDb sentiment analysis problem. It effectively captures the underlying patterns in the data and generalizes well to unseen samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ecc4dc",
   "metadata": {},
   "source": [
    "### <U>12. Scaling up: developing a model that overfits for the new model that does better than baseline</U><a class=\"anchor\" id=\"Scaling_Developing\"></a>\n",
    "\n",
    "To figure out how big a model we will need, we must develop a model that overfits. However, overfitting may not produce the best accuracy rate. To make the new model overfit i used -\n",
    "\n",
    "- 512 units in each dense layer to increase model capacity\n",
    "- Two additional dense layers to increase the depth of the network\n",
    "- 10 epochs durinf training to allow the model to train for a longer duration\n",
    "- The same dropout rate\n",
    "- The same learning rate\n",
    "- The same batch size\n",
    "\n",
    "The model was then proven to overfit. It had a lower accuracy rate compared to the previous developed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a49a458d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 14s 21ms/step - loss: 0.5564 - accuracy: 0.6967 - val_loss: 0.3122 - val_accuracy: 0.8736\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.2920 - accuracy: 0.8826 - val_loss: 0.2722 - val_accuracy: 0.8920\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.1990 - accuracy: 0.9251 - val_loss: 0.2808 - val_accuracy: 0.8926\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 14s 23ms/step - loss: 0.1403 - accuracy: 0.9496 - val_loss: 0.3057 - val_accuracy: 0.8920\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.0885 - accuracy: 0.9686 - val_loss: 0.4077 - val_accuracy: 0.8790\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.0556 - accuracy: 0.9811 - val_loss: 0.4173 - val_accuracy: 0.8906\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 13s 20ms/step - loss: 0.0326 - accuracy: 0.9896 - val_loss: 0.5022 - val_accuracy: 0.8888\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 13s 22ms/step - loss: 0.0232 - accuracy: 0.9924 - val_loss: 0.5429 - val_accuracy: 0.8926\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.0150 - accuracy: 0.9954 - val_loss: 0.6084 - val_accuracy: 0.8890\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 13s 21ms/step - loss: 0.0091 - accuracy: 0.9971 - val_loss: 0.7489 - val_accuracy: 0.8818\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.7495 - accuracy: 0.8742\n",
      "Test Accuracy: 0.8741599917411804\n"
     ]
    }
   ],
   "source": [
    "#Original Code\n",
    "units = 512  \n",
    "dropout_rate = 0.6  \n",
    "learning_rate = 0.0001 \n",
    "\n",
    "model = Sequential([\n",
    "    Dense(units=units, activation='relu', input_shape=(max_features,)),\n",
    "    Dropout(dropout_rate),\n",
    "    Dense(units=units, activation='relu'),  \n",
    "    Dropout(dropout_rate),  \n",
    "    Dense(units=units, activation='relu'),  \n",
    "    Dropout(dropout_rate),  \n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train_reshaped, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "loss, accuracy = model.evaluate(x_test_reshaped, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93328dfe",
   "metadata": {},
   "source": [
    "The output indicates that the model is overfitting to the training data.\n",
    "\n",
    "- The accuracy on the training data increases steadily over epochs, reaching very high values (e.g., around 99.71% in the final epoch). This suggests that the model is fitting the training data very closely.\n",
    "\n",
    "- The accuracy on the validation data also increases initially, but then starts to decrease after a certain number of epochs like after the third epoch. This indicates that the model's performance on the validation data is degrading, a typical sign of overfitting.\n",
    "\n",
    "- The accuracy on the test data is 87.42%, which is lower than the training accuracy but still relatively high. However, since the model is overfitting, this test accuracy might not be a reliable estimate of the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2a5239da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 12s 18ms/step - loss: 0.5054 - accuracy: 0.7512 - val_loss: 0.3051 - val_accuracy: 0.8796\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 11s 18ms/step - loss: 0.2796 - accuracy: 0.8887 - val_loss: 0.2798 - val_accuracy: 0.8894\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 11s 18ms/step - loss: 0.2041 - accuracy: 0.9229 - val_loss: 0.2730 - val_accuracy: 0.8952\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 11s 18ms/step - loss: 0.1493 - accuracy: 0.9452 - val_loss: 0.2902 - val_accuracy: 0.8916\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 11s 18ms/step - loss: 0.1039 - accuracy: 0.9649 - val_loss: 0.3163 - val_accuracy: 0.8918\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 12s 19ms/step - loss: 0.0718 - accuracy: 0.9765 - val_loss: 0.3466 - val_accuracy: 0.8912\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 12s 19ms/step - loss: 0.0470 - accuracy: 0.9864 - val_loss: 0.3983 - val_accuracy: 0.8882\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 12s 19ms/step - loss: 0.0294 - accuracy: 0.9914 - val_loss: 0.4338 - val_accuracy: 0.8896\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 12s 19ms/step - loss: 0.0186 - accuracy: 0.9959 - val_loss: 0.4748 - val_accuracy: 0.8876\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 12s 19ms/step - loss: 0.0135 - accuracy: 0.9970 - val_loss: 0.5155 - val_accuracy: 0.8878\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABEM0lEQVR4nO3dd3gVZfbA8e8hBJIACRAgAqGJoYaeUKR3FEHFAoiwqCuWn8q6K4Idd9dddd21KzbEgoANBEUFpIkiJXQIvYZeQyAEUs7vj7mJIabcQG5ukns+z8OTe6e8c2YS5sy878z7iqpijDHGd5XydgDGGGO8yxKBMcb4OEsExhjj4ywRGGOMj7NEYIwxPs4SgTHG+DhLBOayiYiKyFWuzxNE5Cl3lr2E7QwTkTmXGqcpWCLyuIi87+04zOUTe4/AiMiPwDJVfTrL9OuBd4BwVU3JZX0FIlR1uxvbcmtZEakL7AL8c9t2QRCRbsCnqhruye3ksG0BHgRGAfWAk8BS4O+qur6w4zG+ye4IDMAkYLjrpJTZcGCyp0/EPu5VYDTwEFAZaADMAPp7MaY8iUhpb8dgCo4lAgPOiacy0Dl9gohUAq4DPhaRtiKyVEROichBEXlDRMpkV5CITBKRf2b6Psa1zgERuTPLsv1FZLWInBaRfSIyPtPsxa6fp0TkjIh0EJGRIrIk0/pXi8gKEYl3/bw607yFIvIPEflFRBJEZI6IVMnvgRGRxq6yTonIRhEZmGnetSKyyVX+fhF5xDW9ioh861rnhIj8LCJ/+L8mIhHA/wFDVXW+qp5X1URVnayqz7uWCRGRj0XkqIjsEZEn08tyHY9fRORl17Z2uo7JSNfxPCIif8ryu5kgInNdMS8SkTqZ5r/qWu+0iMSISOa/h/Ei8qWIfCoip4GRrmmfuuYHuOYdd8WyQkTCXPNqiMhM17HYLiJ3Zyn3c9c+JriOcVR+f0/m8lgiMKjqOeBzYESmybcCm1V1LZAKPAxUAToAPYH78ypXRPoBjwC9gQigV5ZFzrq2WRHnCvg+EbnBNa+L62dFVS2vqkuzlF0Z+A54DQgF/gd8JyKhmRa7DbgDqAaUccXiNhHxB2YBc1xlPAhMFpGGrkU+AO5R1QpAJDDfNf1vQBxQFQgDHgeyq4PtCcSp6vJcwngdCAGuBLriHK87Ms1vB6zDOQafAVOBaOAq4HbgDREpn2n5YcA/cH6Xa4DJmeatAFriXBR8BnwhIgGZ5l8PfInz+8q8HsCfXHHWcsVyL3DONW8KzvGoAdwM/EtEemZad6Ar7orATOCNnA+H8QRLBCbdR8AtIhLo+j7CNQ1VjVHV31Q1RVV347QbdHWjzFuBD1V1g6qeBcZnnqmqC1V1vaqmqeo6nBOGO+WCkzi2qeonrrimAJuBAZmW+VBVt2ZKdC3dLDtde6A88LyqXlDV+cC3wFDX/GSgiYgEq+pJVV2VaXp1oI6qJqvqz5p9Y1wocDCnjYuIHzAYeExVE1zH/r84VXbpdqnqh6qaCkzDORH/3XV3MQe4gJMU0n2nqotV9TzwBNBBRGoBqOqnqnrcdTz/C5QFGmZad6mqznD9vs5xsWTX/lylqqmuv5nTrrI7AWNVNUlV1wDvZ9mHJao627UPnwAtcjomxjMsERgAVHUJcBS4XkSuxLmq/AxARBq4qjoOuaoF/oVzRZmXGsC+TN/3ZJ4pIu1EZIGr2iMe5yrS3eqbGlnLc32vmen7oUyfE3FO6vlRA9inqmk5bOMm4Fpgj6uapYNr+n+A7cAcV3XNuBzKP46TMHJSBedOJvN+Zt3Hw5k+nwNQ1azTMu93xu9DVc8AJ3D2ExH5m4jEuqraTuFc4VfJbt1sfAL8CEx1VQO+6LqjqgGcUNWEXPYh6+8pQKwNolBZIjCZfYxzJzAcmJPphPI2ztV2hKoG41R1ZG1Yzs5BnCvUdLWzzP8MpyqglqqGABMylZvX42wHgDpZptUG9rsRl7sOALWy1O9nbENVV6jq9TjVRjNw7jpwXb3/TVWvxLlD+WuWqpB0PwHhudSJH8O50s68n5e7jxm/D1eVUWXggKs9YCzOXVwlVa0IxHPx7znH34nrzudZVW0CXI3TvjQC5xhWFpEKBbgPpoBZIjCZfYxTj383rmohlwrAaeCMiDQC7nOzvM9xGhWbiEgQ8EyW+RVwrhaTRKQtTp1+uqNAGk7deHZmAw1E5DYRKS0ig4EmOFU3l8TV4JnxD1iO047xqIj4i/OY6QCcq94y4rzXEKKqyTjHJ9VVznUicpWISKbpqVm3p6rbgLeAKSLSzVVmgIgMEZFxrqqSz4HnRKSCq2H3r8Cnl7qPwLUi0kmcxv5/4Dw2vA/nd5GCc9xLi8jTQLC7hYpIdxFp5qrOOo2TwFJdZf8K/Nu1b82Bu/hjG4PxIksEJoOrDvpXoBzOlXq6R3BO0gnAezh10e6U9z3wCk4j6nZ+b0xNdz/wdxFJAJ7GdUXtWjcReA74xfUUSvssZR/Huer8G04Vy6PAdap6zJ3YslETpxol879aOA2Z1+Bcnb8FjFDVza51hgO7XdVl9+I0zoLTMD4POIPzTsBbqrowh+0+hNM4+iZwCtgB3IjTSA1OA/VZYCewBOcuauIl7iOu9Z/BqRJqg9N4DE61zvfAVpyqmyRyrwrK6gqchuTTQCywiN8T1lCgLs7dwXTgGVWdexn7YAqYvVBmjI8QkUk4Tyk96e1YTNFidwTGGOPjLBEYY4yPs6ohY4zxcXZHYIwxPq7YvbRRpUoVrVu3rrfDMMaYYiUmJuaYqlbNbl6xSwR169Zl5cqV3g7DGGOKFRHJ+iZ+BqsaMsYYH2eJwBhjfJwlAmOM8XHFro0gO8nJycTFxZGUlOTtUIzxiICAAMLDw/H39/d2KKYEKhGJIC4ujgoVKlC3bl3kD6MtGlO8qSrHjx8nLi6OevXqeTscUwKViKqhpKQkQkNDLQmYEklECA0NtTte4zElIhEAlgRMiWZ/38aTSkwiMMaYkuzVedtYvfekR8q2RFBA4uLiuP7664mIiKB+/fqMHj2aCxcuZLvsgQMHuPnmm/Ms89prr+XUqVOXFM/48eN56aWXsp0uImzfvj1j2ssvv4yI5OtFvUmTJvHAAw/ke5ndu3cTHh5OWlraRdNbtmzJ8uXZj+G+e/duIiMjAVi5ciUPPfRQtsvVrVuXY8dyH47gX//610Xfr7766lyXd9fIkSP58ssvC6QsY7JavusEL8/bysItRz1SviWCAqCqDBo0iBtuuIFt27axdetWzpw5wxNPPPGHZVNSUqhRo4ZbJ43Zs2dTsWLFAo+3WbNmTJ06NeP7l19+SZMmTQp8O9mpW7cutWrV4ueff86YtnnzZhISEmjbtm2e60dFRfHaa69d8vazJoJff/31kssypjCkpinjZ26kRkgA93at75FtWCIoAPPnzycgIIA77rgDAD8/P15++WUmTpxIYmIikyZN4pZbbmHAgAH06dPnoivcxMREbr31Vpo3b87gwYNp165dxpV5+hXu7t27ady4MXfffTdNmzalT58+nDt3DoD33nuP6OhoWrRowU033URiYmKe8d5www188803AOzcuZOQkBCqVv29C5IpU6bQrFkzIiMjGTt2bMb0Dz/8kAYNGtC1a1d++eWXjOlHjx7lpptuIjo6mujo6IvmZWfo0KEXJaKpU6cydOhQdu/eTefOnWndujWtW7fO9iS9cOFCrrvuOgCOHz9Onz59aNWqFffccw+Ze9K94YYbaNOmDU2bNuXdd98FYNy4cZw7d46WLVsybJgzMFf58s647qrKmDFjiIyMpFmzZkybNi1je926dePmm2+mUaNGDBs2DHd77E1KSuKOO+6gWbNmtGrVigULFgCwceNG2rZtS8uWLWnevDnbtm3j7Nmz9O/fnxYtWhAZGZmxfWOmrdjHpoOneezaxgSW8fPINkrE46OZPTtrI5sOnC7QMpvUCOaZAU1znL9x40batGlz0bTg4GBq166dUQWzdOlS1q1bR+XKldm9e3fGcm+99RaVKlVi3bp1bNiwgZYtW2a7jW3btjFlyhTee+89br31Vr766ituv/12Bg0axN133w3Ak08+yQcffMCDDz6Y6/4EBwdTq1YtNmzYwDfffMPgwYP58MMPAafaauzYscTExFCpUiX69OnDjBkzaNeuHc888wwxMTGEhITQvXt3WrVqBcDo0aN5+OGH6dSpE3v37qVv377ExsbmuP1bb72VVq1a8frrr1O6dGmmTZvGF198QbVq1Zg7dy4BAQFs27aNoUOH5lpd9eyzz9KpUyeefvppvvvuu4wTPsDEiROpXLky586dIzo6mptuuonnn3+eN954gzVr1vyhrK+//po1a9awdu1ajh07RnR0NF26dAFg9erVbNy4kRo1atCxY0d++eUXOnXqlOsxBnjzzTcBWL9+PZs3b6ZPnz5s3bqVCRMmMHr0aIYNG8aFCxdITU1l9uzZ1KhRg++++w6A+Pj4PMs3JV/8uWRemrOFtnUrc13z6h7bTolLBN6gqtk+1ZF5eu/evalcufIfllmyZAmjR48GIDIykubNm2e7jXr16mUkiTZt2mQkkw0bNvDkk09y6tQpzpw5Q9++fd2KeciQIUydOpUff/yRn376KSMRrFixgm7dumXcIQwbNozFixcDXDR98ODBbN26FYB58+axadOmjLJPnz5NQkJCjtu+4ooraNq0KT/99BNhYWH4+/sTGRlJfHw8DzzwAGvWrMHPzy+j/JwsXryYr7/+GoD+/ftTqVKljHmvvfYa06dPB2Dfvn1s27aN0NDQHMtasmQJQ4cOxc/Pj7CwMLp27cqKFSsIDg6mbdu2hIeHA05bxu7du91KBEuWLMlIyo0aNaJOnTps3bqVDh068NxzzxEXF8egQYOIiIigWbNmPPLII4wdO5brrruOzp0751m+KflenbeNk4kXeHpAE48+OVbiEkFuV+6e0rRpU7766quLpp0+fZp9+/ZRv359YmJiKFeuXLbrulvNULZs2YzPfn5+GVVDI0eOZMaMGbRo0YJJkyaxcOFCt8obMGAAY8aMISoqiuDgYLfiyekPMS0tjaVLlxIYGOjWtuH36qGwsDCGDh0KOI3WYWFhrF27lrS0NAICAvIsJ7uYFi5cyLx581i6dClBQUF069Ytz2fwc9vvrMc+JSUlz7hyK/O2226jXbt2fPfdd/Tt25f333+fHj16EBMTw+zZs3nsscfo06cPTz/9tFvbMSXT9iMJfLx0N0OiaxNZM8Sj27I2ggLQs2dPEhMT+fjjjwFITU3lb3/7GyNHjiQoKCjXdTt16sTnn38OwKZNm1i/fn2+tp2QkED16tVJTk5m8uTJbq8XGBjICy+88IcG7Xbt2rFo0SKOHTtGamoqU6ZMoWvXrrRr146FCxdy/PhxkpOT+eKLLzLW6dOnD2+88UbG9+yqXrK66aabmD17NtOmTWPIkCGAUx1SvXp1SpUqxSeffEJqamquZXTp0iVjn7///ntOnjyZUU6lSpUICgpi8+bN/Pbbbxnr+Pv7k5ycnG1Z06ZNIzU1laNHj7J48WK3Gq/djW/r1q3s3buXhg0bsnPnTq688koeeughBg4cyLp16zhw4ABBQUHcfvvtPPLII6xateqytm2KN1Xl2VmbCCzjxyN9Gnh8e5YICoCIMH36dL744gsiIiJo0KABAQEBf3hCJTv3338/R48epXnz5rzwwgs0b96ckBD3s/8//vEP2rVrR+/evWnUqFG+4h4yZAitW7e+aFr16tX597//Tffu3WnRogWtW7fm+uuvp3r16owfP54OHTrQq1evi9Z77bXXWLlyJc2bN6dJkyZMmDAhz21XrFiR9u3bExYWltFtwv33389HH31E+/bt2bp1a453UemeeeYZFi9eTOvWrZkzZw61a9cGoF+/fqSkpNC8eXOeeuop2rdvn7HOqFGjaN68eUZjcbobb7yR5s2b06JFC3r06MGLL77IFVdcked+ZHbPPfcQHh5OeHg4HTp04P777yc1NZVmzZoxePBgJk2aRNmyZZk2bRqRkZG0bNmSzZs3M2LECNavX5/RgPzcc8/x5JNP5mvbpmT5KfYIP287xl96NSC0fNm8V7hMxW7M4qioKM3agBgbG0vjxo29FNHlSU1NJTk5mYCAAHbs2EHPnj3ZunUrZcqU8XZopogpzn/nxn3nU1Lp+/Ji/EoJP/ylC/5+BXO9LiIxqhqV3bwS10ZQ3CQmJtK9e3eSk5NRVd5++21LAsb4sA9/2c3u44l8dGfbAksCebFE4GUVKlSwoTeNMQAcOZ3E6z9to1fjanRtkO3wwh5hbQTGGFNEvPjjFi6kpvFk/8J50z+dJQJjjCkC1uw7xZcxcdzZqR51q+T+oERBs0RgjDFelubqT6hqhbI82COi0LdvicAYY7xsxpr9rNl3irH9GlG+bOE33VoiKCD56Yb6UowZM4amTZsyZswYJkyYkPHy2qRJkzhw4EC266S/0Ja5u4fRo0cjInl215xZTl1a57XMwoUL6dChw0XTUlJSCAsL4+DBg9mWk7lTuZkzZ/L8889nu1x6Z3E5OXXqFG+99VbGd3e7/nZHt27drIHfFJgz51N4/vvNtKhVkUGtanolBksEBSA/3VDnV3p3Bu+88w6rVq3iP//5D/feey8jRowAck8EAFdddVVGT6NpaWksWLCAmjUL54+tS5cuxMXFXdTJ3rx584iMjKR69bw70Bo4cCDjxo27pG1nTQTudv1tTGF7c8F2jiScZ/yAJpQq5Z2R6DyaCESkn4hsEZHtIvKH/9EiMkZE1rj+bRCRVBH5Y89sRVxe3VC3a9eOjRs3ZizfrVs3YmJiOHv2LHfeeSfR0dG0atUq44SdtdvqgQMHcvbsWdq1a8e0adMyrr6//PJLVq5cybBhw2jZsmVG/0OZDR069KIulTt27Ejp0r/fev7vf/8jMjKSyMhIXnnllYzpzz33HA0bNqRXr15s2bIlY/qOHTvo168fbdq0oXPnzmzevDnH41KqVCluueWWi7pUTu9yevny5Vx99dW0atWKq6+++qJtpMs8sM2uXbvo0KED0dHRPPXUUxnLnDlzhp49e9K6dWuaNWuWcQzHjRvHjh07aNmyJWPGjLmo6++cuoeeNGkSgwYNol+/fkRERPDoo4/muG9ZnThxghtuuIHmzZvTvn171q1bB8CiRYto2bIlLVu2pFWrViQkJHDw4EG6dOlCy5YtiYyMvGhsBuNb9hw/ywc/72JQ65q0ql0p7xU8xGOVUSLiB7wJ9AbigBUiMlNVM7qpVNX/AP9xLT8AeFhVT1zWhr8fB4fy119Pnq5oBtdkX0UBeXdDPWTIED7//HOeffZZDh48yIEDB2jTpg2PP/44PXr0YOLEiZw6dYq2bdvSq1cv4OJuq8GpCknvw2f8+PEA3Hzzzbzxxhu89NJLREVl+8IgERERfPPNN5w8eZIpU6Zw++238/333wMQExPDhx9+yLJly1BV2rVrR9euXUlLS2Pq1KmsXr2alJQUWrdunbF/o0aNYsKECURERLBs2TLuv/9+5s+fn+OxGTp0KKNGjWLs2LGcP3+e2bNn8/LLL+Pn58fixYspXbo08+bN4/HHH/9Dx32ZjR49mvvuu48RI0ZkdO8MEBAQwPTp0wkODubYsWO0b9+egQMH8vzzz7Nhw4aMY5b5riSn7qHB6Sdp9erVlC1bloYNG/Lggw9Sq1atHONK98wzz9CqVStmzJjB/PnzGTFiBGvWrOGll17izTffpGPHjpw5c4aAgADeffdd+vbtyxNPPEFqaqpbY0iYkumf38VS2k8Y2y9/3cMUNE+2SrQFtqvqTgARmQpcD2zKYfmhwBQPxuMxeXVDfeutt9K7d2+effZZPv/8c2655RYA5syZw8yZMzPq1pOSkti7dy+Qc7fVl2LQoEFMnTqVZcuW8c4772RMX7JkCTfeeGNGnz6DBg3i559/Ji0tjRtvvDGjw7yBAwcCztX3r7/+mhE/wPnz53PddnR0NGfOnGHLli3ExsbSvn17KlWqxL59+/jTn/7Etm3bEJFsO4LL7JdffslIFMOHD88YMEdVefzxx1m8eDGlSpVi//79HD58ONeycuoeGpwOBNP7emrSpAl79uxxKxEsWbIkI74ePXpw/Phx4uPj6dixI3/9618ZNmwYgwYNIjw8nOjoaO68806Sk5O54YYbchyDwpRsP287ytxNh3m0X0PCgvPuadeTPJkIagL7Mn2PA9plt6CIBAH9gNwHwXVHLlfunpJXN9RBQUGEhoaybt06pk2blnEyVlW++uorGjZseNG6y5Yty7PDtfxI71zuT3/6E6VK/V4bmN8up9PS0qhYsaJbvYtm3f7UqVOJjY3N6HL6qaeeonv37kyfPp3du3fTrVu3PMvJLqbJkydz9OhRYmJi8Pf3p27dukWmy2kRYdy4cfTv35/Zs2fTvn175s2bR5cuXVi8eDHfffcdw4cPZ8yYMRltPsY3JKem8fdZm6gTGsRdnep5OxyPthFk1+qR0//AAcAvOVULicgoEVkpIiuPHvXM4M2Xw51uqIcMGcKLL75IfHw8zZo1A6Bv3768/vrrGSeR1atX53vbFSpUyHUQGIDatWvz3HPPcf/99180vUuXLsyYMYPExETOnj3L9OnT6dy5M126dGH69OmcO3eOhIQEZs2aBTjVXfXq1cvoglpVWbt2bZ4xDh06lE8//ZT58+dn3F3Ex8dnNFpPmjQpzzI6duyYMbxl5u624+PjqVatGv7+/ixYsIA9e/bkeVxy6h76cmQuc+HChVSpUoXg4GB27NhBs2bNGDt2LFFRUWzevJk9e/ZQrVo17r77bu666y7rctoHffrbHrYdOcMT1zambGnPDD+ZH55MBHFA5nvqcCCnx1uGkEu1kKq+q6pRqhqVeWzdosKdbqhvvvlmpk6dyq233pox7amnniI5OZnmzZsTGRl5USOou0aOHMm9996bY2NxunvuuYf69S8e+Lp169aMHDmStm3b0q5dO/785z/TqlUrWrduzeDBg2nZsiU33XTTRaNlTZ48mQ8++IAWLVrQtGnTjMbZ3DRp0oSgoCB69OiRcafz6KOP8thjj9GxY8c8xx0AePXVV3nzzTeJjo6+aBjHYcOGsXLlSqKiopg8eXJGV9yhoaF07NiRyMhIxowZc1FZOXUPnR/9+/fP6HL6lltuYfz48RldcY8bN46PPvoIgFdeeYXIyEhatGhBYGAg11xzDQsXLsxoPP7qq68yRqgzvuH4mfO8PHcrnSOq0LtJmLfDATzYDbWIlAa2Aj2B/cAK4DZV3ZhluRBgF1BLVc/mVW5J64baGHfZ33nJ8MT09UxdsY8fRncmIqxCoW3XK91Qq2qKiDwA/Aj4ARNVdaOI3Ouanz56yY3AHHeSgDHGFGebDpxmyvK9jOhQt1CTQF48+i6zqs4GZmeZNiHL90nAJE/GYYwx3uYMP7mRkEB/Hu7l+eEn86PEvFlc3EZaMyY/7O+7+Ju9/hDLdp3gb30aEhLk7+1wLlIiEkFAQADHjx+3/yymRFJVjh8/TkCAd581N5fu3IVU/jU7lsbVgxnatra3w/mDEjFCWXh4OHFxcRTFR0uNKQgBAQGEh4d7Owxzid5dvJP9p87x31tb4Oel/oRyUyISgb+/P/Xqef+lDGOMyWr/qXO8vWg7/ZtVp/2Vod4OJ1slomrIGGOKqn/PjkUVHrvWu/0J5cYSgTHGeMjyXSf4dt1B7u1an/BKQd4OJ0eWCIwxxgNSXcNP1ggJ4N6u9fNewYssERhjjAdMW7GPTQdP89i1jQks4/3+hHJjicAYYwpY/LlkXpqzhbZ1K3Nd87xH4/M2SwTGGFPAXp23jZOJF3h6QJNsu08vaiwRGGNMAdp+JIGPl+5mSHRtImuGeDsct1giMMaYAuL0J7SJwDJ+PNKnaPUnlBtLBMYYU0B+ij3Cz9uO8ZdeDQgtn78xLrzJEoExxhSA8ymp/OO7TVxVrTwjOtTxdjj5YonAGGMKwIe/7GbP8USeuq4J/n7F69RavKI1xpgi6MjpJF7/aRu9Gleja4OiN5xuXiwRGGPMZXrhhy1cSE3jyf5NvB3KJbFEYIwxl2HNvlN8tSqOOzvVo26Vct4O55JYIjDGmEuU5upPqGqFsjzYI8Lb4VwySwTGGHOJZqzZz5p9pxjbrxHlyxbf4V0sERhjzCU4cz6F57/fTItaFRnUqqa3w7kslgiMMeYSvLlgO0cSzjN+QBNKFcHhJ/PDEoExxuTTnuNn+eDnXQxqXZNWtSt5O5zLZonAGGPy6Z/fxVLaTxjbr+gOP5kflgiMMSYfft52lLmbDvNAj6sICw7wdjgFwhKBMca4KTk1jb/P2kSd0CDu6lTP2+EUGEsExhjjpk9/28O2I2d44trGlC1dtIefzA9LBMYY44bjZ87z8tytdI6oQu8mYd4Op0BZIjDGGDf8b+5Wzl5I5enrisfwk/lhicAYY/Kw6cBppizfy/D2dYgIq+DtcAqcJQJjjMmFqjJ+1kZCAv15uFfxGX4yPywRGGNMLmavP8TyXSd4pG9DQoL8vR2OR1giMMaYHMzZeIixX62jcfVghkTX9nY4HuPRRCAi/URki4hsF5FxOSzTTUTWiMhGEVnkyXiMMcYdaWnKy3O3MuqTGOpXLcfEkVH4FfP+hHLjsX5TRcQPeBPoDcQBK0RkpqpuyrRMReAtoJ+q7hWRap6Kxxhj3JGQlMzD09YyL/YwN7cJ5583RBLgX3LeGciOJzvQbgtsV9WdACIyFbge2JRpmduAr1V1L4CqHvFgPMYYk6sdR88w6uOV7D6eyLMDmzKiQ50S96hodjxZNVQT2Jfpe5xrWmYNgEoislBEYkRkRHYFicgoEVkpIiuPHj3qoXCNMb7sp9jD3PDGL5xMTObTu9rxp6vr+kQSgHzeEYhIKaC8qp52Z/Fspmk2228D9AQCgaUi8puqbr1oJdV3gXcBoqKispZhjDGXLC1NeXPBdv43bytNawTzzvAoalYM9HZYhSrPOwIR+UxEgkWkHE61zhYRGeNG2XFArUzfw4ED2Szzg6qeVdVjwGKghXuhG2PM5TlzPoX7J6/iv3O3ckPLmnx579U+lwTAvaqhJq47gBuA2UBtYLgb660AIkSknoiUAYYAM7Ms8w3QWURKi0gQ0A6IdTd4Y4y5VLuPneXGN39hbuxhnuzfmP/d2qLENwrnxJ2qIX8R8cdJBG+oarKI5Fk9o6opIvIA8CPgB0xU1Y0icq9r/gRVjRWRH4B1QBrwvqpuuNSdMcYYdyzccoSHpqzGr5Tw8Z1t6XhVFW+H5FXuJIJ3gN3AWmCxiNQB3GkjQFVn49xFZJ42Icv3/wD/cac8Y4y5HKrK24t28J8ft9DoimDeHd6GWpWDvB2W1+WZCFT1NeC1TJP2iEh3z4VkjDEFL/FCCmO+XMd36w4yoEUNXrypOYFlfLMqKCt3GotHuxqLRUQ+EJFVQI9CiM0YYwrE3uOJDHrrV75ff5DHrmnEa0NaWhLIxJ3G4jtdjcV9gKrAHcDzHo3KGGMKyM/bjjLgjSUcjE9i0h1tuadrfZ95P8Bd7rQRpB+xa4EPVXWt2FE0xhRxqsp7P+/k+e830yCsAu8Mb0Od0HLeDqtIcicRxIjIHKAe8JiIVMB5wscYY4qkcxdSGfvVOmauPcC1za7gPze3oFxZT/aoU7y5c2TuAloCO1U1UURCcaqHjDGmyNl3IpF7Pokh9tBpxvRtyP3drCooL+48NZQmIuHAba6DuUhVZ3k8MmOMyadftx/j/z5bRUqaMnFkNN0bWofG7sgzEYjI80A0MNk16SERuVpVH/NoZMYY4yZVZeIvu/nX7FjqVSnHeyOiqFfF2gPc5U7V0LVAS1VNAxCRj4DVgCUCY4zXJSWn8vjX6/l69X76NAnjf4NbUt7aA/LF3aNVETjh+hzimVCMMSZ/9p86x72fxLB+fzx/7d2AB7pfRakSPJKYp7iTCP4NrBaRBTiPknbB7gaMMV62bOdx7p+8igspabw/IopeTcK8HVKx5U5j8RQRWYjTTiDAWKCOh+MyxphsqSofL93DP77dRO3QIN4dHsVV1cp7O6xiza2qIVU9SKYupEVkOU531MYYU2iSklN5asYGvoiJo2ejarw8pCXBAf7eDqvYu9QWFauEM8YUqkPxSdzzaQxr953ioZ4R/KVnhLUHFJBLTQQ2XKQxptCs3H2Cez9dxbkLKbwzvA19m17h7ZBKlBwTgYjMIvsTvgChHovIGGMymbxsD+NnbqRmxUCm3N2OiLAK3g6pxMntjuClS5xnjDGX7XxKKuNnbmTK8n10a1iVV4e0IiTQ2gM8IcdEoKqLCjMQY4wBSElN45s1B3h9/jZ2H0/k/7rX56+9G+Jn7QEeY6/fGWOKhJTUNGasOcAbrgTQpHowH95h/QUVBksExhivSk8Ar8/fxh5XAnh3eBt6NwmzXkMLiSUCY4xXZE0ATWsE896IKHo1rmYJoJC50/toA2AMztvEGcurqo1bbIzJN0sARY87dwRfABOA94BUz4ZjjCmpUlLTmL56P28s2G4JoIhxJxGkqOrbHo/EGFMiZU0AkTWDeX9EFD0tARQZ7iSCWSJyPzAdOJ8+UVVP5LyKMcbXWQIoPtxJBH9y/RyTaZoCVxZ8OMaY4s4SQPHjTjfU9QojEGNM8ZacngDmb2fviUSa1Qzhgz9F0aORJYCizp2nhvyB+3AGpAFYCLyjqskejMsYU0xYAij+3KkaehvwB95yfR/umvZnTwVljCn6LAEUosObYNEL0HgANLu5wIt3JxFEq2qLTN/ni8jaAo/EGFMsWAIoRIc3Oglg0zdQpgLU7eSRzbiTCFJFpL6q7gAQkSux9wmM8TnJqWlMX7Wf1xdsY9+JczQPD2H8wCi6N7QEUOAObXASQOxMJwF0GQPt74egyh7ZnDuJYAywQER24oxFUAe4wyPRGGOKnOwSwLMDm1oC8IRD610JYBaUDYYuj0L7+zyWANK589TQTyISATTESQSbVfV8HqsBICL9gFcBP+B9VX0+y/xuwDfALtekr1X1725Hb4zxGEsAhejgOicBbP7WSQBdxzoJILBSoWw+txHKeqjqfBEZlGVWfRFBVb/OrWAR8QPeBHoDccAKEZmpqpuyLPqzql53KcEbYwpeSmoaX2dKAC3CQ/j7wEi6NaxqCaCgHVwLi150JYAQ6DoO2t9baAkgXW53BF2B+cCAbOYpkGsiANoC21V1J4CITAWuB7ImAmNMEZGSmsZDU1cze/0hSwCedHAtLHwBtnznJIBuj0G7eyGwolfCyW2EsmdcH/+uqrsyzxMRd14yqwnsy/Q9DmiXzXIdXE8hHQAeUdWNWRcQkVHAKIDatWu7sWljTH6lpSljvlzH7PWHePzaRtzd+UpLAAXtwBqnCmjLbAgIgW6PQ7t7vJYA0rnTWPwV0DrLtC+BNnmsl91fkGb5vgqoo6pnRORaYAYQ8YeVVN8F3gWIiorKWoYx5jKpKk/MWM/01ft5pE8DRnWp7+2QSpYDq507gK3fOwmg+xNOAggI8XZkQO5tBI2ApkBIlnaCYCDAjbLjgFqZvofjXPVnUNXTmT7PFpG3RKSKqh5zJ3hjzOVTVZ6dtYkpy/fxf93r80CPP1yLmUu1f5VzB7D1BwioCN2fhHajikwCSJfbHUFD4DqgIhe3EyQAd7tR9gogwlWNtB8YAtyWeQERuQI4rKoqIm2BUsBxt6M3xlwWVeXFH7cw6dfd3NmxHo/0aejtkEqG/THOHcC2H50E0ONJaHsPBAR7O7Js5dZG8A3wjYh0UNWl+S1YVVNE5AHgR5zHRyeq6kYRudc1fwJwM3CfiKQA54AhqmpVP8YUkjfmb+fthTu4rV1tnrqusbUJXK64GFj0PGyb4zz50+MpaDuqyCaAdJLXeVdEAoC7cKqJMqqEVPVOz4aWvaioKF25cqU3Nm1MifLe4p08NzuWQa1q8tItLShVypLAJYtbCQufh+1znQRw9YNOAihbwduRZRCRGFWNym6eO43FnwCbgb7A34FhQGzBhWeMKWyf/LaH52bH0r9ZdV68ubklgUu1b4VzB7B9HgRWhp7PQNu7i1QCcIc7ieAqVb1FRK5X1Y9E5DOc6h5jTDH0ZUwcT83YQK/G1Xh5cEtK+5XydkjFz77lzh3Ajp+cBNBrPET/udglgHTuJIL0cQdOiUgkcAio67GIjDEeM2vtAR79ci2dI6rwxm2tKVPakkC+7FsOC/8NO+ZDUCj0etaVAMp7O7LL4k4ieFdEKgFPATOB8sDTHo3KGFPg5mw8xMPT1hBVpzLvDo8iwN/P2yEVH3uXOQlg5wInAfT+O0TdVewTQDp3Op173/VxETZOsTHF0qKtR3ngs9U0rRnCByOjCCxjScAtFyWAKtD7HxB9F5Qp5+3IClRuL5T9NbcVVfV/BR+OMaagLd1xnFEfr+SqauX5+I62VAjw93ZIRd++FbDwX64qoJKbANLldkeQ3urREIjGqRYC5+WyxZ4MyhhTMGL2nOSuj1ZQq3IQn9zVlpAgSwK5ilvp3AFsn/d7FVD0n0tsAkiX2wtlzwKIyBygtaomuL6PB74olOiMMZdsw/54Rn64nGoVyvLZn9sRWr6st0MquuJiXAlgbolqBHaXO43FtYELmb5fwJ4aMqZI23IogeEfLCM4wJ/Jd7enWrA73YP5oP0xzmOg2+Zkeg9glM8kgHTuvlC2XESm4/QeeiPwsUejMsZcsp1HzzDs/WX4+5Xis7vbUbNioLdDKnoOrHYSwNYfnDeBez5d5N4ELkzuPDX0nIh8D3R2TbpDVVd7NixjzKXYdyKRYe8vQ1X5bFR76oSW7LrtfDuwxpUAvi8WncEVltyeGgpW1dMiUhnY7fqXPq+yqp7wfHjGGHcdjD/Hbe//RuKFVKbc3Z6rqvnm1W22Mo8IFhDi6g7aEkC63O4IPsPphjqGiweUEdd3e6fAmCLiaMJ5hr23jJNnk5n853Y0qWEnOAAOrXfuADZ/WyQHhCkqcntq6DrXT3eGpTTGeMnJsxe4/f1lHIxP4uO72tKiVkVvh+R9hzY4ncHFzioSYwIXdblVDWUdnvIiqrqq4MMxxuRH/Llkhk9cxq7jZ/lwZDTRdSt7OyTvOrzRuQOInQllg6HrOGh/nyWAPORWNfTfXOYp0KOAYzHG5MPZ8ync8eFythxK4N3hUXS8qoq3Q/Kew5ucISE3zYAyFaDLo9DhfueJIJOn3KqGuhdmIMYY9yUlp3LXRytYGxfPG0Nb0b1RNW+H5B1HYp0EsHEGlCkPXcZA+/shyMfvjPLJnfcIcHU/3YSLRyizdwmM8YLzKanc80kMy3ad4OVbW3JNs+reDqnwHdnsSgDTne4fOv8VOjxgCeAS5ZkIROQZoBtOIpgNXAMswV4qM6bQJaem8eBnq1m09Sgv3NSMG1rV9HZIhevoFlj0Imz4CvyDoNPDzrCQlgAuizt3BDcDLYDVqnqHiIQB7+exjjGmgKWmKX/9fC1zNh1m/IAmDI6u7e2QCs+xbc4dwPovnQTQcTRc/RCUC/V2ZCWCO4ngnKqmiUiKiAQDR7B3CIwpVGlpytiv1jFr7QHGXdOIkR195KnuY9th8Yuw/gsoHQAdH3IlAB9uGPcAdxLBShGpCLyH83LZGWC5J4MyxvxOVXlm5ka+jInjoZ4R3Nu1vrdD8rx9K2DZ204bgF9Z6PB/cPVoKF/V25GVSLm9R/AG8Jmq3u+aNEFEfgCCVXVdoURnjI9TVf41O5ZPftvDqC5X8nCvCG+H5DmpybDpG/jtbdi/0nkPoMP/OXcA5X30qahCktsdwTbgvyJSHZgGTFHVNYUSlTEGgJfnbeO9n3cxvH0dHrumESLi7ZAK3tnjEPMhrPgAEg5A5fpwzX+g5VCf7Q20sOX2HsGrwKsiUgcYAnwoIgHAFGCqqm4tpBiN8UlvLdzOaz9t45Y24Tw7sGnJSwKHNznVP+s+h5QkuLI7DHgFruoNpUp5Ozqf4k431HuAF4AXRKQVMBF4BrDRr43xkA9/2cWLP2xhQIsaPH9Tc0qVKiFJIC3NGQTmt7dg1yKnAbjFEKcfoGqNvR2dz3LnPQJ/oB/OXUFPYBHwrIfjMsZnTVm+l2dnbaJPkzD+d2sL/EpCEjifAGs+g2UT4MROqFDDGQ2szUh7B6AIyK2xuDcwFOiP85TQVGCUqp4tpNiM8TnTV8fx+PT1dG1Qlddva4W/XzGvIjm5G5a9C6s/gfOnITzaGQym8UDw8/d2dMYltzuCx3HGJHjEBqEx5vKpKqcSk9l/6hwHTp3L+HngVBJxrs9HE87T4cpQ3hnehrKli2ntqyrs+cV5+mfLbJBS0OQGpxfQ8ChvR2eyYZ3OGVNAklPTOBSflOkE75zs959KyvieeCH1onXKli5FzYqB1KgYSPeGValXpTwjOtQhwL8YJoHkJKfrh9/ehsPrncHgOz0M0X+G4Brejs7kwq1O54wxcDopmf0nfz/Jx7mu5g+cOsf+k+c4nJCE6sXrhJYrQ42KgVxVtTxdIqpSo2IANSsGUrOSc/IPLVem+D8NlHAYVn7gPP6ZeAyqNoYBr0HzW8E/0NvRGTdYIjAGpx+fw6eTMl3Fn8s4waef7BPOp1y0Thm/UlSvGECNkEA6XlWFmpUCqVkxgBquK/waIYEElimGV/buOrAafpvg3AWkpUCDvk71T72uUNyTm4+xRGB83q5jZxn8zlKOJJy/aHrFIH9qhARSOzSIDvVDXVfzQRlX9VXKly05j3W6KzXFGQD+t7dh71JnDICoO51xgEN9oOuLEsqjiUBE+gGv4rxz8L6qPp/DctHAb8BgVf3SkzEZk9mZ8ymM+nglyalp/POGSMIrBWbU2Zcra9dJGc6dhFWfwPJ3IX4fVKwDff8FrW63geBLAI/9pYuIH/Am0BuIA1aIyExV3ZTNci8AP3oqFmOyo6o88vladhw9wyd3tfPtoR5zcmyb8+z/ms8gORHqdoZ+z0PDa6BUCa728jGevORpC2xX1Z0AIjIVuB7YlGW5B4GvgGgPxmLMH7y5YDs/bDzEk/0bWxLITBV2zHeqf7bPBb8y0OxWp/qnenNvR2c8wJOJoCawL9P3OKBd5gVEpCZwI9CDXBKBiIwCRgHUru1Dg3EYj1mw+Qj/nbuVG1rW4K5OPtK3vzuO74BZo2H3z1A+DLo9DlF3WO+fJZwnE0F2rWhZHq7jFWCsqqbm9gidqr4LvAsQFRWVtQxj8mXXsbM8NHU1ja8I5t+Dmhf/xzcLQsoF+PVVWPQfp/+f/v+FVsOhdFlvR2YKgScTQRxQK9P3cOBAlmWigKmu/4hVgGtFJEVVZ3gwLuPD0huHS5cS3hnepmQ/3umufcudu4Ajm5w3gK95ASpc4e2oTCHyZCJYAUSISD1gP06ndbdlXkBVM+7JRWQS8K0lAeMpWRuHa1UO8nZI3pV0Gn76O6x4H4JrwtCpTiOw8TkeSwSqmiIiD+A8DeQHTFTVjSJyr2v+BE9t25jsWONwJrHfwuwxkHDQ6QK6xxM2CIwP8+iD0qo6G5idZVq2CUBVR3oyFuPbrHHY5fQBJwFs/hbCImHwpxDexttRGS+zN2ZMiWeNwzgDwsRMhHnPQuoF6DUeOjxgXUEbwBKBKeGscRg4Eus0Bu9bBld2g+tehspXejsqU4RYIjAlls83Dicnwc8vwZJXnPr/G9+B5oOtQzjzB5YITInl043Du36Gb/8Cx7dDi6HQ5zkoF+rtqEwRZYnAlEg+2ziceALmPu0MDVmpLgyfAfVtjCmTO0sEpsTxycZhVWdcgB/GOcmg41+g61go42PVYeaSWCIwJYpPNg6f3APf/c3pIK5Gaxg+Ha5o5u2oTDFiicCUGD7XOJyaAsvfgfn/BMTpHrrtKOse2uSbJQJTYvhU4/DBtTDzITi4BiL6Op3EVayV52rGZMcSgSkRfKZx+MJZWPhvWPoWBIXCLZOcjuJ8oR3EeIwlAlPs+Uzj8PZ58O3DcGovtBnpvB0cWMnbUZkSwBKBKdZ8onH4zFH48TFY/wVUaQB3fA91rvZ2VKYEsURgiq0S3zis6owVPOcJOH8Guo6Dzn+1wWJMgbNEYIqtEt04nHnIyNodYMCrULWht6MyJZQlAlMsldjG4dRk+OVVWPSiM2Tkda9A6z9BqVLejsyUYJYITLFTIhuHk07Dphnw29uuISOvh2tetCEjTaGwRGCKlRLVOJyWCrsWwZopEDsLUs5BaAQMmQKNrvV2dMaHWCIwxUaJaRw+ts1pBF43DU7vh4AQaDkUWtwG4VH2ToApdJYITLFRrBuHz510OoVbMwX2rwTxg6t6Qt/noME14B/g7QiND7NEYIqFYtk4nJoCO35yrv63zHaGiKzWBPr8E5rdChXCvB2hMYAlAlMMFLvG4cMbXVU/n8PZI05XEFF3OgPEVG9hVT+myLFEYIq0YtM4fPaY8+bvms/g0DooVRoa9HNO/hF9oHQZb0doTI4sEZgiq8g3DqdcgG0/OvX+236EtBTniv+aFyHyZhsa0hQblghMkVUkG4dVna6f10xx7gDOnYDyYdD+Puepn7Am3o7QmHyzRGCKpCLXOJxwyHncc80UOBoLfmWdZ/1b3Ab1e4Cf/VcyxZf99Zoip8g0DicnwZbvnJP/jp9A0yA8Gq57GZreaF1AmxLDEoEpUrzeOKwKcSucRt8NX8P5eAiuCZ0edhp+q0QUbjzGFAJLBKbI8Grj8Kl9TtXP2ilwfDuUDoQmA52Tf70uNg6wKdEsEZgio9Abh4/vgNiZEPut87YvQJ2O0PEvTqdvAcGej8GYIsBnEsHJsxeYvGwPwzvUJSTQ39vhmCwKpXFYFQ6th83fOp28HdnkTK/RCno8BZE3QeUi0DBtTCHzmUSwYMsRXpqzlXcW7+SOjvW4s2NdKgbZSz5FgUcbh9PSnDr/2JnOyf/UHpBSUPtq6PcCNOoPFWsV3PaMKYZ8JhEMah1Og7AKvDF/O6/9tI2JS3YxokMd/tz5SiqXs4TgLR5pHE5Ndkb2ip0Fm7+DM4ehlD9c2Q06/w0aXgvlq17+dowpITyaCESkH/Aq4Ae8r6rPZ5l/PfAPIA1IAf6iqks8EsypvURu+4wJvfuzuVcnXl+wg7cX7WDSr7u5vX0d7u58JVUr2FiwhSU+MZkFW47w6W97CqZx+EIi7JjvnPy3fg9J8eAfBBG9ofFA52dASMHtgDEliKiqZwoW8QO2Ar2BOGAFMFRVN2VapjxwVlVVRJoDn6tqo9zKjYqK0pUrV+Y/oHVfwNd3AwohtaFRf/Zf0Y3/bg5lxrojlClditva1uGerlcSFmxdAnvC3uOJzI09zLxNh1m++wSpaUqV8mUZ07cBg6Nr57/ApHjYOsep9tk+D5ITIaCic8Xf+DrnRS//wALfD2OKIxGJUdWobOd5MBF0AMaral/X98cAVPXfuSw/UVUb51buJScCgDNHYOsPTnXBjgWQeh4CK5FQuydfJbbgpR3hXCgVyJDoWtzbtT41KtpJ5HKkpSlr404xL/YwczcdZuvhMwA0CCtP7yZh9GocRovwipQqlY82gTNHnZe8YmfBzkWQlgzlr3Dq+hsPgLqdwM8eBjAmK28lgpuBfqr6Z9f34UA7VX0gy3I3Av8GqgH9VXVpNmWNAkYB1K5du82ePXsuP8DzZ5yqhM3fOckh6RTqV5bNQW345GRT5mtrekQ1476u9YteZ2dFWFJyKr9sP8bcTYeZF3uEY2fO41dKaFu3Mr2ahNGrcTXqhJbLX6Gn9jqPeMbOgr1LAYVKdZ0Tf+OBUDPKBnc3Jg/eSgS3AH2zJIK2qvpgDst3AZ5W1V65lXtZdwQ5SU2Bvb/C5tlOYojfSxrC6rQGzE1rQ6nG/Rncr3v+T2A+4tiZ88yPPcLc2MP8vO0oSclplC9bmq4Nq9K7cRjdG1YjJCifV+lHtzgn/thZTidvANWauk7+10FYpPXrb0w+FIuqIdcyu4BoVT2W0zIeSQSZqcLhDbB5NsmbZuF/ZD0A27Qme6p0o1G3IYQ37eTTV6Cqyo6jZ5i76QjzYg+zau9JVKFGSIBT5dMkjHb1QilTOh/HKL1Xz/ST/7GtzvTwaGh0nZMAQut7ZH+M8QXeSgSlcRqLewL7cRqLb1PVjZmWuQrY4Wosbg3MAsI1l6A8ngiyOrWXhLUzObbya2qdXk1pSSPeLxQaXkNIqxuhXmcoXfKfNkpJTSNmz0lXlc9hdh9PBKBZzRB6NQ6jV5NqNKkenL93AJLiYX+M0+C7+VuI3+eM5Vu3o1Pl06g/BNfw0B4Z41u8kghcG74WeAXn8dGJqvqciNwLoKoTRGQsMAJIBs4BY/J6fLTQE0Emx48e4pcfPqPs9h/oxBrKyXlS/cvj16A3NOzvPKIYWNErsXnCmfMpLN56lHmbDjN/yxFOJSZTxq8UHeqHZtT3Vw9xs0E95bxzp7V/lXPy3x/z+1W/X1nnCZ/GA6DhNRBU2XM7ZYyP8loi8ARvJoJ0J85e4KPFm9n623d0Tl1G/7JrCEk96QxPWLeTU5XR8BoICfdqnJfiYPw55sUeYd6mwyzdcZwLqWlUDPKnR8Nq9G4SRucGVSlfNo/XT9LS4MTO30/4+1c6XTukXnDml6vqNPDWbAM1W0OttlC2gud3zhgfZonAQ+ITk/nw1118uGQH9c9v5s9VY+khKwmI3+EsUL2lU73RqD9Ua1IkGzdVlU0HTzPPVd+/fn88AHVDgzIe8WxTpxKl/XKp7084nOmkHwMHVjnVPgD+5Zy+fGq2dp342zgJsggeC2NKMksEHnY6KZmPf93N+0t2cSoxmcH1zvFgja2EH17g9HODQsU6zp1Co2uhVnuvjGiVlJzKofgkDp1O4vDpJFbtOcm82CPsP3UOEWhduxK9GofRu0k16lctn319//kEOLAm04l/FZyOc+aJnzNUY802v1/xV21oXTgbUwRYIigkZ86n8Olve3hv8U6On73A1fVD+dvVFWmT9JvzWOrOhU71SOkAp3okKBTKVYGgKs7PzJ+DqjiDn5erCmXK53oFnZamnEi8wKF45wR/6HQSh10n/EOnz2d8jj+XfNF6Af6l6BxRld5NwujRqBpVymdp9E5NhsMbfz/h74+Bo5sB199Mpbq/X+XXbANXNIcy9s6FMUWRJYJClnghhc+W7eWdxTs5mnCetvUqM7pnBFeHl0F2zHfuEhKPw9ljkHgMzh6Hs0ch5Vy25alfWS6Urcw5/4ok+IVwkhCOplXgYEp54pKC2J0UyJHUCpygAsc1hAQCERGqli/LFSEBhAUHcEVwQJbPZQmvFESAv+tqXRVO7nJO+HErnZP+oXWQkuTMDwq9+KRfo7WTqIwxxYIlAi9JSk5l6vK9TFi0k0Onk2hduyIP9YygawOn58sTZy9kVNMcij/PsZMnSTxxiKT4I6SeOYokHiXgwkkqSwKhnHZ+ymmqyGlCJYFAkrLdrpbyh3JVkPS7iqAqzp1FxmfXHcf50xfX7Z876RRQOhBqtPy9MbdmG6dqy+r1jSm2LBF42fmUVL5YGcfbC3ew/9Q5qpQvy+mkZC6kpF20nAiElivLFSFluSL496v3sJCLr+iDA0o79ffJ5/54V5F4LIdpx+FCwh+Dk1JQtfHvJ/zwKOe7F9owjDGeY4mgiLiQksbXq+JYvusEVSuUdU706dU1IQFUq1AW/9yezrlcyUlOlVTiMSdBlA6E6i2gbHnPbdMYUyRYIjDGGB+XWyLw3Q5zjDHGAJYIjDHG51kiMMYYH2eJwBhjfJwlAmOM8XGWCIwxxsdZIjDGGB9nicAYY3xcsXuhTESOAnsucfUqQI7jIfsgOx4Xs+PxOzsWFysJx6OOqlbNbkaxSwSXQ0RW5vRmnS+y43ExOx6/s2NxsZJ+PKxqyBhjfJwlAmOM8XG+lgje9XYARYwdj4vZ8fidHYuLlejj4VNtBMYYY/7I1+4IjDHGZGGJwBhjfJzPJAIR6SciW0Rku4iM83Y83iQitURkgYjEishGERnt7Zi8TUT8RGS1iHzr7Vi8TUQqisiXIrLZ9TfSwdsxeYuIPOz6P7JBRKaISIC3Y/IEn0gEIuIHvAlcAzQBhopIE+9G5VUpwN9UtTHQHvg/Hz8eAKOBWG8HUUS8Cvygqo2AFvjocRGRmsBDQJSqRgJ+wBDvRuUZPpEIgLbAdlXdqaoXgKnA9V6OyWtU9aCqrnJ9TsD5j17Tu1F5j4iEA/2B970di7eJSDDQBfgAQFUvqOoprwblXaWBQBEpDQQBB7wcj0f4SiKoCezL9D0OHz7xZSYidYFWwDIvh+JNrwCPAmlejqMouBI4Cnzoqip7X0TKeTsob1DV/cBLwF7gIBCvqnO8G5Vn+EoikGym+fxzsyJSHvgK+IuqnvZ2PN4gItcBR1Q1xtuxFBGlgdbA26raCjgL+GSbmohUwqk5qAfUAMqJyO3ejcozfCURxAG1Mn0Pp4Te4rlLRPxxksBkVf3a2/F4UUdgoIjsxqky7CEin3o3JK+KA+JUNf0O8UucxOCLegG7VPWoqiYDXwNXezkmj/CVRLACiBCReiJSBqfBZ6aXY/IaERGcOuBYVf2ft+PxJlV9TFXDVbUuzt/FfFUtkVd97lDVQ8A+EWnomtQT2OTFkLxpL9BeRIJc/2d6UkIbzkt7O4DCoKopIvIA8CNOy/9EVd3o5bC8qSMwHFgvImtc0x5X1dneC8kUIQ8Ck10XTTuBO7wcj1eo6jIR+RJYhfOk3WpKaFcT1sWEMcb4OF+pGjLGGJMDSwTGGOPjLBEYY4yPs0RgjDE+zhKBMcb4OEsExriISKqIrMn0r8DeqBWRuiKyoaDKM6Yg+cR7BMa46ZyqtvR2EMYUNrsjMCYPIrJbRF4QkeWuf1e5ptcRkZ9EZJ3rZ23X9DARmS4ia13/0rsl8BOR91z9288RkUDX8g+JyCZXOVO9tJvGh1kiMOZ3gVmqhgZnmndaVdsCb+D0Vorr88eq2hyYDLzmmv4asEhVW+D005P+FnsE8KaqNgVOATe5po8DWrnKudczu2ZMzuzNYmNcROSMqpbPZvpuoIeq7nR11ndIVUNF5BhQXVWTXdMPqmoVETkKhKvq+Uxl1AXmqmqE6/tYwF9V/ykiPwBngBnADFU94+FdNeYidkdgjHs0h885LZOd85k+p/J7G11/nBH02gAxrkFQjCk0lgiMcc/gTD+Xuj7/yu9DFw4Dlrg+/wTcBxljIQfnVKiIlAJqqeoCnMFxKgJ/uCsxxpPsysOY3wVm6o0VnHF70x8hLSsiy3Aunoa6pj0ETBSRMTijeqX30jkaeFdE7sK58r8PZ4Sr7PgBn4pICM4ASi/7+NCQxgusjcCYPLjaCKJU9Zi3YzHGE6xqyBhjfJzdERhjjI+zOwJjjPFxlgiMMcbHWSIwxhgfZ4nAGGN8nCUCY4zxcf8PQuPyFU/OvZAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Original Code \n",
    "model_overfit = Sequential([\n",
    "    Dense(units=512, activation='relu', input_shape=(max_features,)),\n",
    "    Dropout(0.6),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.6),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model_overfit.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_overfit = model_overfit.fit(x_train_reshaped, y_train, epochs= 10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "plt.plot(history.history['val_loss'], label='Original Model Validation Loss')\n",
    "plt.plot(history_overfit.history['val_loss'], label='Overfit Model Validation Loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss Comparison')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25306161",
   "metadata": {},
   "source": [
    "- As seen the output generation, the training accuracy continues to increase over epochs, indicating that the model is fitting the training data very well. However, the validation accuracy decreases after a certain number of epochs. This difference between training and validation accuracies is a classic sign of overfitting.\n",
    "\n",
    "- The training loss decreases steadily over epochs, while the validation loss increases.\n",
    "\n",
    "<u>Plot Observation -</u>\n",
    "\n",
    "The plot compares the validation loss between the original model and the overfit model. The original model consists of a deep neural network with dense layers and dropout regularization, trained for 10 epochs with a batch size of 32. Meanwhile, the overfit model employs a similar architecture but with larger units (512) and more complex layers, which can lead to overfitting. The validation loss of the overfit model is consistently higher than that of the original model, indicating poorer generalization performance.\n",
    "\n",
    "The plot illustrates how the validation loss evolves over epochs for both models. In the initial epochs, both models experience a decrease in validation loss, indicating improved performance. However, as training progresses, the overfit model starts to exhibit higher validation loss compared to the original model. This increase in validation loss suggests that the overfit model is overfitting to the training data, failing to generalize well to unseen data.\n",
    "\n",
    "The comparison highlights the importance of model complexity and regularization techniques in preventing overfitting, emphasizing the need to strike a balance between model capacity and generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d099975",
   "metadata": {},
   "source": [
    "### <U>13. Interpretation of Results</U><a class=\"anchor\" id=\"Results\"></a>\n",
    "\n",
    "I have already mentioned the interpretation of results for each code. But this section of the project will be used as an overall interpretation of results between the baseline model and the developed model.\n",
    "\n",
    "Baseline model -\n",
    "\n",
    "- It has a simpler architecture with two hidden layers and fewer units.\n",
    "- Trained for more epochs (20) with a larger batch size (512).\n",
    "- Slightly lower validation accuracy but similar test accuracy compared to the developed model.\n",
    "- Baseline Model Test Accuracy: Approximately 88.37%\n",
    "- Developed Model Test Accuracy: Approximately 88.99%.\n",
    "- Test Accuracy: Achieved approximately 88.37%.\n",
    "\n",
    "Developed model -\n",
    "- Utilizes a more complex architecture with three hidden layers and more units (512).\n",
    "- Trained for fewer epochs (10) with a smaller batch size (32).\n",
    "- Achieved slightly higher validation accuracy compared to the baseline model with comparable test accuracy.\n",
    "- Validation Accuracy: Achieved approximately 89.20% after 10 epochs.\n",
    "- Test Accuracy: Achieved approximately 88.99%.\n",
    "\n",
    "The developed model demonstrates a slightly better performance compared to the baseline model.\n",
    "\n",
    "The developed model, despite having a more complex architecture, achieved slightly better performance in terms of validation accuracy and comparable performance in terms of test accuracy compared to the baseline model. The developed model's improved validation accuracy suggests better generalization and robustness, indicating its effectiveness in learning more intricate patterns in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e4c21",
   "metadata": {},
   "source": [
    "### <U>14. Wrapping up and Conclusion</U><a class=\"anchor\" id=\"Conclusion\"></a>\n",
    "Here are some things i learned about this dataset while doing this project and reading the book -\n",
    "\n",
    "- You usually need to do quite a bit of preprocessing on your raw data in order to be able to feed it as tensors into a neural network. Sequences of words can be encoded as binary vectors, but there are other encoding options, too.\n",
    "\n",
    "- Stacks of Dense layers with relu activations can solve a wide range of problems including sentiment classification, and youll likely use them frequently.\n",
    "\n",
    "- In a binary classification problem (two output classes), your network should end with a Dense layer with one unit and a sigmoid activation: the output of your network should be a scalar between 0 and 1, encoding a probability.\n",
    "\n",
    "- With such a scalar sigmoid output on a binary classification problem, the loss function you should use is binary_crossentropy.\n",
    "\n",
    "- The rmsprop optimizer is generally a good enough choice, whatever your problem. Thats one less thing for you to worry about. I found adam optimizer to be helpful as well.\n",
    "\n",
    "- As they get better on their training data, neural networks eventually start overfitting and end up obtaining increasingly worse results on data theyve never seen before. Be sure to always monitor performance on data that is outside of the training set.\n",
    "\n",
    "The book deep learning with python by Chollet helped me alot with understanding this universal workflow. I found the book very interesting. It had cleared any doubt i had in deep learning and i would be very interested to work on projects beyond part 1 chapter 1-4 sections of the book. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5afd61d",
   "metadata": {},
   "source": [
    "### <u>15. References</u><a class=\"anchor\" id=\"References\"></a>\n",
    "\n",
    "- Chollet, F. (2018). Deep Learning with Python. Shelter Island (New York, Estados Unidos): Manning, Cop. Part 1 Chapters 1-4 pg 3-116"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
